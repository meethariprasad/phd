{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Siamese TF Cosine Distance Fine Tune.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/meethariprasad/research_works/blob/master/Siamese_TF_Cosine_Distance_Fine_Tune.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "90S3uGNW9sMO",
        "colab_type": "text"
      },
      "source": [
        "###### Author: Hari Prasad, This work is as part of providing simple examples in the world of complicated examples. Have fun!\n",
        "###### Licence: Free to Distribute and Modify. You can quote this github reference for sure. :-)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "94h6dYJrrUtU",
        "colab_type": "code",
        "outputId": "d2c7f42e-db2d-4efc-bf56-dc9bfe595706",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "%tensorflow_version 2.x\n",
        "# !pip install tensorflow==2.1.0rc0\n",
        "import tensorflow as tf\n",
        "print (tf.__version__)\n",
        "import numpy as np"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TensorFlow 2.x selected.\n",
            "2.1.0-rc1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bmRkXOdiIrGg",
        "colab_type": "code",
        "outputId": "6a24f905-f178-4d3b-c7a4-5a128a635192",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "print (\"Following is simple self explatory code of how dot layer works as cosine similiarity between two vectors, if normalize = True set\")\n",
        "tf.keras.backend.set_floatx('float64')\n",
        "x=np.asarray([-1.,-1.,-1])\n",
        "x=np.reshape(x,(1,3))\n",
        "y=np.asarray([1.,1.,1.])\n",
        "y=np.reshape(y,(1,3))\n",
        "tf.keras.layers.Dot(axes=-1,normalize=True)([x,y]).numpy()[0][0]"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Following is simple self explatory code of how dot layer works as cosine similiarity between two vectors, if normalize = True set\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "-1.0000000000000002"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DNX6jbv40iyV",
        "colab_type": "code",
        "outputId": "89f710ba-055d-460d-b62e-14b9f2b8e806",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 726
        }
      },
      "source": [
        "%tensorflow_version 2.x\n",
        "# !pip install tensorflow==2.1.0rc0\n",
        "import tensorflow as tf\n",
        "print (tf.__version__)\n",
        "tf.keras.backend.clear_session()\n",
        "import gc\n",
        "try:\n",
        "  del model\n",
        "except:\n",
        "  pass\n",
        "gc.collect()\n",
        "import tensorflow_hub as hub\n",
        "from tensorflow import keras\n",
        "import os\n",
        "import logging\n",
        "tf.get_logger().setLevel(logging.ERROR)\n",
        "\n",
        "huburl = \"https://tfhub.dev/google/universal-sentence-encoder/4\" #@param [\"https://tfhub.dev/google/universal-sentence-encoder/4\", \"https://tfhub.dev/google/universal-sentence-encoder-large/5\"]\n",
        "loaded_module_obj = hub.load(huburl)\n",
        "shared_embedding_layer = hub.KerasLayer(loaded_module_obj,trainable=True)\n",
        "\n",
        "left_input = keras.Input(shape=(), dtype=tf.string)\n",
        "right_input = keras.Input(shape=(), dtype=tf.string)\n",
        "\n",
        "embedding_left_output= shared_embedding_layer(left_input)\n",
        "embedding_right_output= shared_embedding_layer(right_input)\n",
        "\n",
        "\n",
        "\n",
        "cosine_similiarity=tf.keras.layers.Dot(axes=-1,normalize=True)([embedding_left_output,embedding_right_output])\n",
        "cos_distance=1-cosine_similiarity\n",
        "\n",
        "model = tf.keras.Model([left_input,right_input], cos_distance)\n",
        "#Define Optimizer\n",
        "#https://machinelearningmastery.com/how-to-avoid-exploding-gradients-in-neural-networks-with-gradient-clipping/\n",
        "\n",
        "# optim = keras.optimizers.RMSprop(clipnorm=1.)\n",
        "optim =tf.compat.v1.train.ProximalAdagradOptimizer(learning_rate=0.0001\n",
        "                                                   ,l1_regularization_strength=0.0,\n",
        "                                                   l2_regularization_strength=0.01\n",
        "                                                   )\n",
        "model.compile(optimizer=optim, loss='mse')\n",
        "model.summary()\n",
        "tf.keras.utils.plot_model(model, to_file='my_model.png')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2.1.0-rc1\n",
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_1 (InputLayer)            [(None,)]            0                                            \n",
            "__________________________________________________________________________________________________\n",
            "input_2 (InputLayer)            [(None,)]            0                                            \n",
            "__________________________________________________________________________________________________\n",
            "keras_layer (KerasLayer)        (None, 512)          256797824   input_1[0][0]                    \n",
            "                                                                 input_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dot (Dot)                       (None, 1)            0           keras_layer[0][0]                \n",
            "                                                                 keras_layer[1][0]                \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_sub (TensorFlowOpLa [(None, 1)]          0           dot[0][0]                        \n",
            "==================================================================================================\n",
            "Total params: 256,797,824\n",
            "Trainable params: 256,797,824\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAFgCAIAAADbyPIuAAAABmJLR0QA/wD/AP+gvaeTAAAgAElE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\nCABogTwCAFogjwCAFsgjAKDF/wNCNZkf6Ei1swAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<IPython.core.display.Image object>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L9qHUv-I59_9",
        "colab_type": "text"
      },
      "source": [
        "### Cosine Distance Target Value Explanation\n",
        "\n",
        "Cosine similiarity varies between -1 to +1, with -1 being Orthogonally Opposite and +1 being in same direction.\n",
        "\n",
        "Cosine Distance is 1-(Cosine Similiarity)=>falls under [0,2]\n",
        "\n",
        "Distance is Highest(Far Apart vectors) at 2, => Cosine Similiarity =-1, 1-(-1)=2\n",
        "\n",
        "Distance is lowest(Similiar Vectors) at 0, => Cosine Similiarity = 1, 1-(1)=0\n",
        "\n",
        "Hence if two vectors are similiar, the assignment should be 0, and if they are opposite then it must be 2\n",
        "\n",
        "This gives us option to rank highly similiar sentences based on notion between 0 to 2.\n",
        "\n",
        "As gradient loss function minimizes the loss, the 0 loss means the actual cosine distance and calculated are same."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RLtF-Xrk75SR",
        "colab_type": "code",
        "outputId": "a4fc32a8-de04-476b-e003-cc5d24421567",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "#Inputs\n",
        "import numpy as np\n",
        "text_list=[[\"Man is going to Moon\",\"Education is greatest gift to humanity\"],\n",
        "           [\"Man achieved great feat in apollo mission\",\"Literacy is important for civilization\"]]\n",
        "\n",
        "#Model Input Preperation as Numpy array\n",
        "left_inputs=np.asarray(text_list[0])\n",
        "right_inputs=np.asarray(text_list[1])\n",
        "#0 if we inputs are semantically similiar, 2 if not. Anything inbetween 0=2 represent partial similiarity\n",
        "similiarity=np.asarray([0,0])\n",
        "\n",
        "left_inputs=left_inputs.reshape(left_inputs.shape[0],)\n",
        "right_inputs=right_inputs.reshape(right_inputs.shape[0],)\n",
        "\n",
        "left_inputs.shape,right_inputs.shape,similiarity.shape"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((2,), (2,), (2,))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aJNH5d4UE6Ca",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_similiarity(target_text_embed,text_to_compare_embed):\n",
        "    from sklearn.metrics.pairwise import cosine_similarity\n",
        "    similiarity=cosine_similarity(target_text_embed,text_to_compare_embed)\n",
        "    similiarity=pd.DataFrame(similiarity)\n",
        "    return similiarity"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "16tBBRBJNNtu",
        "colab_type": "code",
        "outputId": "7fcc3d19-9fa5-4eeb-fea9-edd4001f3c9c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "#Model Fit\n",
        "from keras.callbacks import Callback\n",
        "class stopAtLossValue(Callback):\n",
        "  import numpy as np\n",
        "  def on_batch_end(self, batch, logs={}):\n",
        "    THR = 0 #Assign THR with the value at which you want to stop training.\n",
        "    if np.around(logs.get('loss'),decimals=1) == THR:\n",
        "      self.model.stop_training = True\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "model.fit([left_inputs,right_inputs],similiarity,epochs=300,callbacks=[stopAtLossValue()])"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 2 samples\n",
            "Epoch 1/300\n",
            "2/2 [==============================] - 6s 3s/sample - loss: 0.3239\n",
            "Epoch 2/300\n",
            "2/2 [==============================] - 0s 11ms/sample - loss: 0.3237\n",
            "Epoch 3/300\n",
            "2/2 [==============================] - 0s 10ms/sample - loss: 0.3235\n",
            "Epoch 4/300\n",
            "2/2 [==============================] - 0s 10ms/sample - loss: 0.3233\n",
            "Epoch 5/300\n",
            "2/2 [==============================] - 0s 10ms/sample - loss: 0.3232\n",
            "Epoch 6/300\n",
            "2/2 [==============================] - 0s 10ms/sample - loss: 0.3230\n",
            "Epoch 7/300\n",
            "2/2 [==============================] - 0s 11ms/sample - loss: 0.3228\n",
            "Epoch 8/300\n",
            "2/2 [==============================] - 0s 10ms/sample - loss: 0.3226\n",
            "Epoch 9/300\n",
            "2/2 [==============================] - 0s 11ms/sample - loss: 0.3224\n",
            "Epoch 10/300\n",
            "2/2 [==============================] - 0s 10ms/sample - loss: 0.3222\n",
            "Epoch 11/300\n",
            "2/2 [==============================] - 0s 12ms/sample - loss: 0.3221\n",
            "Epoch 12/300\n",
            "2/2 [==============================] - 0s 11ms/sample - loss: 0.3219\n",
            "Epoch 13/300\n",
            "2/2 [==============================] - 0s 11ms/sample - loss: 0.3217\n",
            "Epoch 14/300\n",
            "2/2 [==============================] - 0s 12ms/sample - loss: 0.3215\n",
            "Epoch 15/300\n",
            "2/2 [==============================] - 0s 12ms/sample - loss: 0.3213\n",
            "Epoch 16/300\n",
            "2/2 [==============================] - 0s 11ms/sample - loss: 0.3212\n",
            "Epoch 17/300\n",
            "2/2 [==============================] - 0s 12ms/sample - loss: 0.3210\n",
            "Epoch 18/300\n",
            "2/2 [==============================] - 0s 12ms/sample - loss: 0.3208\n",
            "Epoch 19/300\n",
            "2/2 [==============================] - 0s 12ms/sample - loss: 0.3206\n",
            "Epoch 20/300\n",
            "2/2 [==============================] - 0s 12ms/sample - loss: 0.3204\n",
            "Epoch 21/300\n",
            "2/2 [==============================] - 0s 12ms/sample - loss: 0.3203\n",
            "Epoch 22/300\n",
            "2/2 [==============================] - 0s 14ms/sample - loss: 0.3201\n",
            "Epoch 23/300\n",
            "2/2 [==============================] - 0s 11ms/sample - loss: 0.3199\n",
            "Epoch 24/300\n",
            "2/2 [==============================] - 0s 14ms/sample - loss: 0.3197\n",
            "Epoch 25/300\n",
            "2/2 [==============================] - 0s 13ms/sample - loss: 0.3195\n",
            "Epoch 26/300\n",
            "2/2 [==============================] - 0s 12ms/sample - loss: 0.3194\n",
            "Epoch 27/300\n",
            "2/2 [==============================] - 0s 12ms/sample - loss: 0.3192\n",
            "Epoch 28/300\n",
            "2/2 [==============================] - 0s 12ms/sample - loss: 0.3190\n",
            "Epoch 29/300\n",
            "2/2 [==============================] - 0s 11ms/sample - loss: 0.3188\n",
            "Epoch 30/300\n",
            "2/2 [==============================] - 0s 10ms/sample - loss: 0.3186\n",
            "Epoch 31/300\n",
            "2/2 [==============================] - 0s 12ms/sample - loss: 0.3185\n",
            "Epoch 32/300\n",
            "2/2 [==============================] - 0s 12ms/sample - loss: 0.3183\n",
            "Epoch 33/300\n",
            "2/2 [==============================] - 0s 11ms/sample - loss: 0.3181\n",
            "Epoch 34/300\n",
            "2/2 [==============================] - 0s 12ms/sample - loss: 0.3179\n",
            "Epoch 35/300\n",
            "2/2 [==============================] - 0s 12ms/sample - loss: 0.3178\n",
            "Epoch 36/300\n",
            "2/2 [==============================] - 0s 12ms/sample - loss: 0.3176\n",
            "Epoch 37/300\n",
            "2/2 [==============================] - 0s 13ms/sample - loss: 0.3174\n",
            "Epoch 38/300\n",
            "2/2 [==============================] - 0s 13ms/sample - loss: 0.3172\n",
            "Epoch 39/300\n",
            "2/2 [==============================] - 0s 10ms/sample - loss: 0.3171\n",
            "Epoch 40/300\n",
            "2/2 [==============================] - 0s 12ms/sample - loss: 0.3169\n",
            "Epoch 41/300\n",
            "2/2 [==============================] - 0s 13ms/sample - loss: 0.3167\n",
            "Epoch 42/300\n",
            "2/2 [==============================] - 0s 11ms/sample - loss: 0.3165\n",
            "Epoch 43/300\n",
            "2/2 [==============================] - 0s 13ms/sample - loss: 0.3164\n",
            "Epoch 44/300\n",
            "2/2 [==============================] - 0s 12ms/sample - loss: 0.3162\n",
            "Epoch 45/300\n",
            "2/2 [==============================] - 0s 12ms/sample - loss: 0.3160\n",
            "Epoch 46/300\n",
            "2/2 [==============================] - 0s 10ms/sample - loss: 0.3158\n",
            "Epoch 47/300\n",
            "2/2 [==============================] - 0s 9ms/sample - loss: 0.3157\n",
            "Epoch 48/300\n",
            "2/2 [==============================] - 0s 13ms/sample - loss: 0.3155\n",
            "Epoch 49/300\n",
            "2/2 [==============================] - 0s 10ms/sample - loss: 0.3153\n",
            "Epoch 50/300\n",
            "2/2 [==============================] - 0s 11ms/sample - loss: 0.3151\n",
            "Epoch 51/300\n",
            "2/2 [==============================] - 0s 11ms/sample - loss: 0.3150\n",
            "Epoch 52/300\n",
            "2/2 [==============================] - 0s 13ms/sample - loss: 0.3148\n",
            "Epoch 53/300\n",
            "2/2 [==============================] - 0s 12ms/sample - loss: 0.3146\n",
            "Epoch 54/300\n",
            "2/2 [==============================] - 0s 12ms/sample - loss: 0.3144\n",
            "Epoch 55/300\n",
            "2/2 [==============================] - 0s 11ms/sample - loss: 0.3143\n",
            "Epoch 56/300\n",
            "2/2 [==============================] - 0s 12ms/sample - loss: 0.3141\n",
            "Epoch 57/300\n",
            "2/2 [==============================] - 0s 11ms/sample - loss: 0.3139\n",
            "Epoch 58/300\n",
            "2/2 [==============================] - 0s 11ms/sample - loss: 0.3137\n",
            "Epoch 59/300\n",
            "2/2 [==============================] - 0s 12ms/sample - loss: 0.3136\n",
            "Epoch 60/300\n",
            "2/2 [==============================] - 0s 11ms/sample - loss: 0.3134\n",
            "Epoch 61/300\n",
            "2/2 [==============================] - 0s 12ms/sample - loss: 0.3132\n",
            "Epoch 62/300\n",
            "2/2 [==============================] - 0s 13ms/sample - loss: 0.3131\n",
            "Epoch 63/300\n",
            "2/2 [==============================] - 0s 13ms/sample - loss: 0.3129\n",
            "Epoch 64/300\n",
            "2/2 [==============================] - 0s 10ms/sample - loss: 0.3127\n",
            "Epoch 65/300\n",
            "2/2 [==============================] - 0s 14ms/sample - loss: 0.3125\n",
            "Epoch 66/300\n",
            "2/2 [==============================] - 0s 11ms/sample - loss: 0.3124\n",
            "Epoch 67/300\n",
            "2/2 [==============================] - 0s 11ms/sample - loss: 0.3122\n",
            "Epoch 68/300\n",
            "2/2 [==============================] - 0s 12ms/sample - loss: 0.3120\n",
            "Epoch 69/300\n",
            "2/2 [==============================] - 0s 11ms/sample - loss: 0.3119\n",
            "Epoch 70/300\n",
            "2/2 [==============================] - 0s 13ms/sample - loss: 0.3117\n",
            "Epoch 71/300\n",
            "2/2 [==============================] - 0s 12ms/sample - loss: 0.3115\n",
            "Epoch 72/300\n",
            "2/2 [==============================] - 0s 10ms/sample - loss: 0.3113\n",
            "Epoch 73/300\n",
            "2/2 [==============================] - 0s 11ms/sample - loss: 0.3112\n",
            "Epoch 74/300\n",
            "2/2 [==============================] - 0s 11ms/sample - loss: 0.3110\n",
            "Epoch 75/300\n",
            "2/2 [==============================] - 0s 12ms/sample - loss: 0.3108\n",
            "Epoch 76/300\n",
            "2/2 [==============================] - 0s 13ms/sample - loss: 0.3107\n",
            "Epoch 77/300\n",
            "2/2 [==============================] - 0s 11ms/sample - loss: 0.3105\n",
            "Epoch 78/300\n",
            "2/2 [==============================] - 0s 11ms/sample - loss: 0.3103\n",
            "Epoch 79/300\n",
            "2/2 [==============================] - 0s 14ms/sample - loss: 0.3102\n",
            "Epoch 80/300\n",
            "2/2 [==============================] - 0s 13ms/sample - loss: 0.3100\n",
            "Epoch 81/300\n",
            "2/2 [==============================] - 0s 14ms/sample - loss: 0.3098\n",
            "Epoch 82/300\n",
            "2/2 [==============================] - 0s 12ms/sample - loss: 0.3097\n",
            "Epoch 83/300\n",
            "2/2 [==============================] - 0s 11ms/sample - loss: 0.3095\n",
            "Epoch 84/300\n",
            "2/2 [==============================] - 0s 12ms/sample - loss: 0.3093\n",
            "Epoch 85/300\n",
            "2/2 [==============================] - 0s 11ms/sample - loss: 0.3091\n",
            "Epoch 86/300\n",
            "2/2 [==============================] - 0s 12ms/sample - loss: 0.3090\n",
            "Epoch 87/300\n",
            "2/2 [==============================] - 0s 10ms/sample - loss: 0.3088\n",
            "Epoch 88/300\n",
            "2/2 [==============================] - 0s 12ms/sample - loss: 0.3086\n",
            "Epoch 89/300\n",
            "2/2 [==============================] - 0s 13ms/sample - loss: 0.3085\n",
            "Epoch 90/300\n",
            "2/2 [==============================] - 0s 15ms/sample - loss: 0.3083\n",
            "Epoch 91/300\n",
            "2/2 [==============================] - 0s 11ms/sample - loss: 0.3081\n",
            "Epoch 92/300\n",
            "2/2 [==============================] - 0s 11ms/sample - loss: 0.3080\n",
            "Epoch 93/300\n",
            "2/2 [==============================] - 0s 12ms/sample - loss: 0.3078\n",
            "Epoch 94/300\n",
            "2/2 [==============================] - 0s 13ms/sample - loss: 0.3076\n",
            "Epoch 95/300\n",
            "2/2 [==============================] - 0s 12ms/sample - loss: 0.3075\n",
            "Epoch 96/300\n",
            "2/2 [==============================] - 0s 13ms/sample - loss: 0.3073\n",
            "Epoch 97/300\n",
            "2/2 [==============================] - 0s 11ms/sample - loss: 0.3071\n",
            "Epoch 98/300\n",
            "2/2 [==============================] - 0s 11ms/sample - loss: 0.3070\n",
            "Epoch 99/300\n",
            "2/2 [==============================] - 0s 12ms/sample - loss: 0.3068\n",
            "Epoch 100/300\n",
            "2/2 [==============================] - 0s 14ms/sample - loss: 0.3066\n",
            "Epoch 101/300\n",
            "2/2 [==============================] - 0s 13ms/sample - loss: 0.3065\n",
            "Epoch 102/300\n",
            "2/2 [==============================] - 0s 12ms/sample - loss: 0.3063\n",
            "Epoch 103/300\n",
            "2/2 [==============================] - 0s 13ms/sample - loss: 0.3061\n",
            "Epoch 104/300\n",
            "2/2 [==============================] - 0s 12ms/sample - loss: 0.3060\n",
            "Epoch 105/300\n",
            "2/2 [==============================] - 0s 13ms/sample - loss: 0.3058\n",
            "Epoch 106/300\n",
            "2/2 [==============================] - 0s 12ms/sample - loss: 0.3056\n",
            "Epoch 107/300\n",
            "2/2 [==============================] - 0s 11ms/sample - loss: 0.3055\n",
            "Epoch 108/300\n",
            "2/2 [==============================] - 0s 10ms/sample - loss: 0.3053\n",
            "Epoch 109/300\n",
            "2/2 [==============================] - 0s 11ms/sample - loss: 0.3051\n",
            "Epoch 110/300\n",
            "2/2 [==============================] - 0s 12ms/sample - loss: 0.3050\n",
            "Epoch 111/300\n",
            "2/2 [==============================] - 0s 10ms/sample - loss: 0.3048\n",
            "Epoch 112/300\n",
            "2/2 [==============================] - 0s 11ms/sample - loss: 0.3047\n",
            "Epoch 113/300\n",
            "2/2 [==============================] - 0s 12ms/sample - loss: 0.3045\n",
            "Epoch 114/300\n",
            "2/2 [==============================] - 0s 11ms/sample - loss: 0.3043\n",
            "Epoch 115/300\n",
            "2/2 [==============================] - 0s 12ms/sample - loss: 0.3042\n",
            "Epoch 116/300\n",
            "2/2 [==============================] - 0s 11ms/sample - loss: 0.3040\n",
            "Epoch 117/300\n",
            "2/2 [==============================] - 0s 10ms/sample - loss: 0.3038\n",
            "Epoch 118/300\n",
            "2/2 [==============================] - 0s 11ms/sample - loss: 0.3037\n",
            "Epoch 119/300\n",
            "2/2 [==============================] - 0s 11ms/sample - loss: 0.3035\n",
            "Epoch 120/300\n",
            "2/2 [==============================] - 0s 11ms/sample - loss: 0.3033\n",
            "Epoch 121/300\n",
            "2/2 [==============================] - 0s 11ms/sample - loss: 0.3032\n",
            "Epoch 122/300\n",
            "2/2 [==============================] - 0s 11ms/sample - loss: 0.3030\n",
            "Epoch 123/300\n",
            "2/2 [==============================] - 0s 11ms/sample - loss: 0.3028\n",
            "Epoch 124/300\n",
            "2/2 [==============================] - 0s 11ms/sample - loss: 0.3027\n",
            "Epoch 125/300\n",
            "2/2 [==============================] - 0s 13ms/sample - loss: 0.3025\n",
            "Epoch 126/300\n",
            "2/2 [==============================] - 0s 13ms/sample - loss: 0.3024\n",
            "Epoch 127/300\n",
            "2/2 [==============================] - 0s 12ms/sample - loss: 0.3022\n",
            "Epoch 128/300\n",
            "2/2 [==============================] - 0s 11ms/sample - loss: 0.3020\n",
            "Epoch 129/300\n",
            "2/2 [==============================] - 0s 13ms/sample - loss: 0.3019\n",
            "Epoch 130/300\n",
            "2/2 [==============================] - 0s 10ms/sample - loss: 0.3017\n",
            "Epoch 131/300\n",
            "2/2 [==============================] - 0s 14ms/sample - loss: 0.3015\n",
            "Epoch 132/300\n",
            "2/2 [==============================] - 0s 12ms/sample - loss: 0.3014\n",
            "Epoch 133/300\n",
            "2/2 [==============================] - 0s 12ms/sample - loss: 0.3012\n",
            "Epoch 134/300\n",
            "2/2 [==============================] - 0s 10ms/sample - loss: 0.3011\n",
            "Epoch 135/300\n",
            "2/2 [==============================] - 0s 11ms/sample - loss: 0.3009\n",
            "Epoch 136/300\n",
            "2/2 [==============================] - 0s 13ms/sample - loss: 0.3007\n",
            "Epoch 137/300\n",
            "2/2 [==============================] - 0s 12ms/sample - loss: 0.3006\n",
            "Epoch 138/300\n",
            "2/2 [==============================] - 0s 13ms/sample - loss: 0.3004\n",
            "Epoch 139/300\n",
            "2/2 [==============================] - 0s 11ms/sample - loss: 0.3002\n",
            "Epoch 140/300\n",
            "2/2 [==============================] - 0s 12ms/sample - loss: 0.3001\n",
            "Epoch 141/300\n",
            "2/2 [==============================] - 0s 11ms/sample - loss: 0.2999\n",
            "Epoch 142/300\n",
            "2/2 [==============================] - 0s 12ms/sample - loss: 0.2998\n",
            "Epoch 143/300\n",
            "2/2 [==============================] - 0s 12ms/sample - loss: 0.2996\n",
            "Epoch 144/300\n",
            "2/2 [==============================] - 0s 12ms/sample - loss: 0.2994\n",
            "Epoch 145/300\n",
            "2/2 [==============================] - 0s 12ms/sample - loss: 0.2993\n",
            "Epoch 146/300\n",
            "2/2 [==============================] - 0s 11ms/sample - loss: 0.2991\n",
            "Epoch 147/300\n",
            "2/2 [==============================] - 0s 12ms/sample - loss: 0.2990\n",
            "Epoch 148/300\n",
            "2/2 [==============================] - 0s 11ms/sample - loss: 0.2988\n",
            "Epoch 149/300\n",
            "2/2 [==============================] - 0s 12ms/sample - loss: 0.2986\n",
            "Epoch 150/300\n",
            "2/2 [==============================] - 0s 12ms/sample - loss: 0.2985\n",
            "Epoch 151/300\n",
            "2/2 [==============================] - 0s 11ms/sample - loss: 0.2983\n",
            "Epoch 152/300\n",
            "2/2 [==============================] - 0s 11ms/sample - loss: 0.2982\n",
            "Epoch 153/300\n",
            "2/2 [==============================] - 0s 12ms/sample - loss: 0.2980\n",
            "Epoch 154/300\n",
            "2/2 [==============================] - 0s 11ms/sample - loss: 0.2978\n",
            "Epoch 155/300\n",
            "2/2 [==============================] - 0s 12ms/sample - loss: 0.2977\n",
            "Epoch 156/300\n",
            "2/2 [==============================] - 0s 12ms/sample - loss: 0.2975\n",
            "Epoch 157/300\n",
            "2/2 [==============================] - 0s 12ms/sample - loss: 0.2974\n",
            "Epoch 158/300\n",
            "2/2 [==============================] - 0s 11ms/sample - loss: 0.2972\n",
            "Epoch 159/300\n",
            "2/2 [==============================] - 0s 11ms/sample - loss: 0.2970\n",
            "Epoch 160/300\n",
            "2/2 [==============================] - 0s 11ms/sample - loss: 0.2969\n",
            "Epoch 161/300\n",
            "2/2 [==============================] - 0s 11ms/sample - loss: 0.2967\n",
            "Epoch 162/300\n",
            "2/2 [==============================] - 0s 11ms/sample - loss: 0.2966\n",
            "Epoch 163/300\n",
            "2/2 [==============================] - 0s 11ms/sample - loss: 0.2964\n",
            "Epoch 164/300\n",
            "2/2 [==============================] - 0s 11ms/sample - loss: 0.2963\n",
            "Epoch 165/300\n",
            "2/2 [==============================] - 0s 12ms/sample - loss: 0.2961\n",
            "Epoch 166/300\n",
            "2/2 [==============================] - 0s 11ms/sample - loss: 0.2959\n",
            "Epoch 167/300\n",
            "2/2 [==============================] - 0s 11ms/sample - loss: 0.2958\n",
            "Epoch 168/300\n",
            "2/2 [==============================] - 0s 11ms/sample - loss: 0.2956\n",
            "Epoch 169/300\n",
            "2/2 [==============================] - 0s 11ms/sample - loss: 0.2955\n",
            "Epoch 170/300\n",
            "2/2 [==============================] - 0s 11ms/sample - loss: 0.2953\n",
            "Epoch 171/300\n",
            "2/2 [==============================] - 0s 11ms/sample - loss: 0.2951\n",
            "Epoch 172/300\n",
            "2/2 [==============================] - 0s 11ms/sample - loss: 0.2950\n",
            "Epoch 173/300\n",
            "2/2 [==============================] - 0s 11ms/sample - loss: 0.2948\n",
            "Epoch 174/300\n",
            "2/2 [==============================] - 0s 11ms/sample - loss: 0.2947\n",
            "Epoch 175/300\n",
            "2/2 [==============================] - 0s 10ms/sample - loss: 0.2945\n",
            "Epoch 176/300\n",
            "2/2 [==============================] - 0s 11ms/sample - loss: 0.2944\n",
            "Epoch 177/300\n",
            "2/2 [==============================] - 0s 10ms/sample - loss: 0.2942\n",
            "Epoch 178/300\n",
            "2/2 [==============================] - 0s 11ms/sample - loss: 0.2940\n",
            "Epoch 179/300\n",
            "2/2 [==============================] - 0s 13ms/sample - loss: 0.2939\n",
            "Epoch 180/300\n",
            "2/2 [==============================] - 0s 12ms/sample - loss: 0.2937\n",
            "Epoch 181/300\n",
            "2/2 [==============================] - 0s 13ms/sample - loss: 0.2936\n",
            "Epoch 182/300\n",
            "2/2 [==============================] - 0s 12ms/sample - loss: 0.2934\n",
            "Epoch 183/300\n",
            "2/2 [==============================] - 0s 12ms/sample - loss: 0.2933\n",
            "Epoch 184/300\n",
            "2/2 [==============================] - 0s 14ms/sample - loss: 0.2931\n",
            "Epoch 185/300\n",
            "2/2 [==============================] - 0s 14ms/sample - loss: 0.2930\n",
            "Epoch 186/300\n",
            "2/2 [==============================] - 0s 13ms/sample - loss: 0.2928\n",
            "Epoch 187/300\n",
            "2/2 [==============================] - 0s 13ms/sample - loss: 0.2926\n",
            "Epoch 188/300\n",
            "2/2 [==============================] - 0s 12ms/sample - loss: 0.2925\n",
            "Epoch 189/300\n",
            "2/2 [==============================] - 0s 11ms/sample - loss: 0.2923\n",
            "Epoch 190/300\n",
            "2/2 [==============================] - 0s 12ms/sample - loss: 0.2922\n",
            "Epoch 191/300\n",
            "2/2 [==============================] - 0s 12ms/sample - loss: 0.2920\n",
            "Epoch 192/300\n",
            "2/2 [==============================] - 0s 11ms/sample - loss: 0.2919\n",
            "Epoch 193/300\n",
            "2/2 [==============================] - 0s 9ms/sample - loss: 0.2917\n",
            "Epoch 194/300\n",
            "2/2 [==============================] - 0s 12ms/sample - loss: 0.2916\n",
            "Epoch 195/300\n",
            "2/2 [==============================] - 0s 12ms/sample - loss: 0.2914\n",
            "Epoch 196/300\n",
            "2/2 [==============================] - 0s 11ms/sample - loss: 0.2912\n",
            "Epoch 197/300\n",
            "2/2 [==============================] - 0s 12ms/sample - loss: 0.2911\n",
            "Epoch 198/300\n",
            "2/2 [==============================] - 0s 11ms/sample - loss: 0.2909\n",
            "Epoch 199/300\n",
            "2/2 [==============================] - 0s 11ms/sample - loss: 0.2908\n",
            "Epoch 200/300\n",
            "2/2 [==============================] - 0s 11ms/sample - loss: 0.2906\n",
            "Epoch 201/300\n",
            "2/2 [==============================] - 0s 11ms/sample - loss: 0.2905\n",
            "Epoch 202/300\n",
            "2/2 [==============================] - 0s 12ms/sample - loss: 0.2903\n",
            "Epoch 203/300\n",
            "2/2 [==============================] - 0s 12ms/sample - loss: 0.2901\n",
            "Epoch 204/300\n",
            "2/2 [==============================] - 0s 12ms/sample - loss: 0.2900\n",
            "Epoch 205/300\n",
            "2/2 [==============================] - 0s 11ms/sample - loss: 0.2898\n",
            "Epoch 206/300\n",
            "2/2 [==============================] - 0s 12ms/sample - loss: 0.2897\n",
            "Epoch 207/300\n",
            "2/2 [==============================] - 0s 12ms/sample - loss: 0.2895\n",
            "Epoch 208/300\n",
            "2/2 [==============================] - 0s 13ms/sample - loss: 0.2894\n",
            "Epoch 209/300\n",
            "2/2 [==============================] - 0s 12ms/sample - loss: 0.2892\n",
            "Epoch 210/300\n",
            "2/2 [==============================] - 0s 11ms/sample - loss: 0.2890\n",
            "Epoch 211/300\n",
            "2/2 [==============================] - 0s 16ms/sample - loss: 0.2889\n",
            "Epoch 212/300\n",
            "2/2 [==============================] - 0s 12ms/sample - loss: 0.2887\n",
            "Epoch 213/300\n",
            "2/2 [==============================] - 0s 12ms/sample - loss: 0.2886\n",
            "Epoch 214/300\n",
            "2/2 [==============================] - 0s 12ms/sample - loss: 0.2884\n",
            "Epoch 215/300\n",
            "2/2 [==============================] - 0s 11ms/sample - loss: 0.2883\n",
            "Epoch 216/300\n",
            "2/2 [==============================] - 0s 12ms/sample - loss: 0.2881\n",
            "Epoch 217/300\n",
            "2/2 [==============================] - 0s 11ms/sample - loss: 0.2880\n",
            "Epoch 218/300\n",
            "2/2 [==============================] - 0s 10ms/sample - loss: 0.2878\n",
            "Epoch 219/300\n",
            "2/2 [==============================] - 0s 10ms/sample - loss: 0.2876\n",
            "Epoch 220/300\n",
            "2/2 [==============================] - 0s 12ms/sample - loss: 0.2875\n",
            "Epoch 221/300\n",
            "2/2 [==============================] - 0s 11ms/sample - loss: 0.2873\n",
            "Epoch 222/300\n",
            "2/2 [==============================] - 0s 11ms/sample - loss: 0.2872\n",
            "Epoch 223/300\n",
            "2/2 [==============================] - 0s 12ms/sample - loss: 0.2870\n",
            "Epoch 224/300\n",
            "2/2 [==============================] - 0s 12ms/sample - loss: 0.2869\n",
            "Epoch 225/300\n",
            "2/2 [==============================] - 0s 13ms/sample - loss: 0.2867\n",
            "Epoch 226/300\n",
            "2/2 [==============================] - 0s 12ms/sample - loss: 0.2866\n",
            "Epoch 227/300\n",
            "2/2 [==============================] - 0s 12ms/sample - loss: 0.2864\n",
            "Epoch 228/300\n",
            "2/2 [==============================] - 0s 10ms/sample - loss: 0.2863\n",
            "Epoch 229/300\n",
            "2/2 [==============================] - 0s 11ms/sample - loss: 0.2861\n",
            "Epoch 230/300\n",
            "2/2 [==============================] - 0s 13ms/sample - loss: 0.2859\n",
            "Epoch 231/300\n",
            "2/2 [==============================] - 0s 9ms/sample - loss: 0.2858\n",
            "Epoch 232/300\n",
            "2/2 [==============================] - 0s 11ms/sample - loss: 0.2856\n",
            "Epoch 233/300\n",
            "2/2 [==============================] - 0s 12ms/sample - loss: 0.2855\n",
            "Epoch 234/300\n",
            "2/2 [==============================] - 0s 11ms/sample - loss: 0.2853\n",
            "Epoch 235/300\n",
            "2/2 [==============================] - 0s 12ms/sample - loss: 0.2852\n",
            "Epoch 236/300\n",
            "2/2 [==============================] - 0s 11ms/sample - loss: 0.2850\n",
            "Epoch 237/300\n",
            "2/2 [==============================] - 0s 11ms/sample - loss: 0.2849\n",
            "Epoch 238/300\n",
            "2/2 [==============================] - 0s 12ms/sample - loss: 0.2847\n",
            "Epoch 239/300\n",
            "2/2 [==============================] - 0s 11ms/sample - loss: 0.2846\n",
            "Epoch 240/300\n",
            "2/2 [==============================] - 0s 11ms/sample - loss: 0.2844\n",
            "Epoch 241/300\n",
            "2/2 [==============================] - 0s 12ms/sample - loss: 0.2843\n",
            "Epoch 242/300\n",
            "2/2 [==============================] - 0s 12ms/sample - loss: 0.2841\n",
            "Epoch 243/300\n",
            "2/2 [==============================] - 0s 11ms/sample - loss: 0.2840\n",
            "Epoch 244/300\n",
            "2/2 [==============================] - 0s 11ms/sample - loss: 0.2838\n",
            "Epoch 245/300\n",
            "2/2 [==============================] - 0s 12ms/sample - loss: 0.2837\n",
            "Epoch 246/300\n",
            "2/2 [==============================] - 0s 11ms/sample - loss: 0.2835\n",
            "Epoch 247/300\n",
            "2/2 [==============================] - 0s 11ms/sample - loss: 0.2834\n",
            "Epoch 248/300\n",
            "2/2 [==============================] - 0s 12ms/sample - loss: 0.2832\n",
            "Epoch 249/300\n",
            "2/2 [==============================] - 0s 12ms/sample - loss: 0.2831\n",
            "Epoch 250/300\n",
            "2/2 [==============================] - 0s 12ms/sample - loss: 0.2829\n",
            "Epoch 251/300\n",
            "2/2 [==============================] - 0s 12ms/sample - loss: 0.2827\n",
            "Epoch 252/300\n",
            "2/2 [==============================] - 0s 12ms/sample - loss: 0.2826\n",
            "Epoch 253/300\n",
            "2/2 [==============================] - 0s 11ms/sample - loss: 0.2824\n",
            "Epoch 254/300\n",
            "2/2 [==============================] - 0s 12ms/sample - loss: 0.2823\n",
            "Epoch 255/300\n",
            "2/2 [==============================] - 0s 11ms/sample - loss: 0.2821\n",
            "Epoch 256/300\n",
            "2/2 [==============================] - 0s 11ms/sample - loss: 0.2820\n",
            "Epoch 257/300\n",
            "2/2 [==============================] - 0s 12ms/sample - loss: 0.2818\n",
            "Epoch 258/300\n",
            "2/2 [==============================] - 0s 12ms/sample - loss: 0.2817\n",
            "Epoch 259/300\n",
            "2/2 [==============================] - 0s 12ms/sample - loss: 0.2815\n",
            "Epoch 260/300\n",
            "2/2 [==============================] - 0s 12ms/sample - loss: 0.2814\n",
            "Epoch 261/300\n",
            "2/2 [==============================] - 0s 12ms/sample - loss: 0.2812\n",
            "Epoch 262/300\n",
            "2/2 [==============================] - 0s 12ms/sample - loss: 0.2811\n",
            "Epoch 263/300\n",
            "2/2 [==============================] - 0s 13ms/sample - loss: 0.2809\n",
            "Epoch 264/300\n",
            "2/2 [==============================] - 0s 11ms/sample - loss: 0.2808\n",
            "Epoch 265/300\n",
            "2/2 [==============================] - 0s 12ms/sample - loss: 0.2806\n",
            "Epoch 266/300\n",
            "2/2 [==============================] - 0s 12ms/sample - loss: 0.2805\n",
            "Epoch 267/300\n",
            "2/2 [==============================] - 0s 15ms/sample - loss: 0.2803\n",
            "Epoch 268/300\n",
            "2/2 [==============================] - 0s 13ms/sample - loss: 0.2802\n",
            "Epoch 269/300\n",
            "2/2 [==============================] - 0s 12ms/sample - loss: 0.2800\n",
            "Epoch 270/300\n",
            "2/2 [==============================] - 0s 12ms/sample - loss: 0.2799\n",
            "Epoch 271/300\n",
            "2/2 [==============================] - 0s 12ms/sample - loss: 0.2797\n",
            "Epoch 272/300\n",
            "2/2 [==============================] - 0s 12ms/sample - loss: 0.2796\n",
            "Epoch 273/300\n",
            "2/2 [==============================] - 0s 11ms/sample - loss: 0.2794\n",
            "Epoch 274/300\n",
            "2/2 [==============================] - 0s 12ms/sample - loss: 0.2793\n",
            "Epoch 275/300\n",
            "2/2 [==============================] - 0s 10ms/sample - loss: 0.2791\n",
            "Epoch 276/300\n",
            "2/2 [==============================] - 0s 12ms/sample - loss: 0.2790\n",
            "Epoch 277/300\n",
            "2/2 [==============================] - 0s 13ms/sample - loss: 0.2788\n",
            "Epoch 278/300\n",
            "2/2 [==============================] - 0s 11ms/sample - loss: 0.2787\n",
            "Epoch 279/300\n",
            "2/2 [==============================] - 0s 12ms/sample - loss: 0.2785\n",
            "Epoch 280/300\n",
            "2/2 [==============================] - 0s 13ms/sample - loss: 0.2784\n",
            "Epoch 281/300\n",
            "2/2 [==============================] - 0s 13ms/sample - loss: 0.2782\n",
            "Epoch 282/300\n",
            "2/2 [==============================] - 0s 11ms/sample - loss: 0.2781\n",
            "Epoch 283/300\n",
            "2/2 [==============================] - 0s 14ms/sample - loss: 0.2780\n",
            "Epoch 284/300\n",
            "2/2 [==============================] - 0s 12ms/sample - loss: 0.2778\n",
            "Epoch 285/300\n",
            "2/2 [==============================] - 0s 11ms/sample - loss: 0.2777\n",
            "Epoch 286/300\n",
            "2/2 [==============================] - 0s 12ms/sample - loss: 0.2775\n",
            "Epoch 287/300\n",
            "2/2 [==============================] - 0s 11ms/sample - loss: 0.2774\n",
            "Epoch 288/300\n",
            "2/2 [==============================] - 0s 10ms/sample - loss: 0.2772\n",
            "Epoch 289/300\n",
            "2/2 [==============================] - 0s 12ms/sample - loss: 0.2771\n",
            "Epoch 290/300\n",
            "2/2 [==============================] - 0s 11ms/sample - loss: 0.2769\n",
            "Epoch 291/300\n",
            "2/2 [==============================] - 0s 11ms/sample - loss: 0.2768\n",
            "Epoch 292/300\n",
            "2/2 [==============================] - 0s 11ms/sample - loss: 0.2766\n",
            "Epoch 293/300\n",
            "2/2 [==============================] - 0s 11ms/sample - loss: 0.2765\n",
            "Epoch 294/300\n",
            "2/2 [==============================] - 0s 12ms/sample - loss: 0.2763\n",
            "Epoch 295/300\n",
            "2/2 [==============================] - 0s 16ms/sample - loss: 0.2762\n",
            "Epoch 296/300\n",
            "2/2 [==============================] - 0s 11ms/sample - loss: 0.2760\n",
            "Epoch 297/300\n",
            "2/2 [==============================] - 0s 11ms/sample - loss: 0.2759\n",
            "Epoch 298/300\n",
            "2/2 [==============================] - 0s 19ms/sample - loss: 0.2757\n",
            "Epoch 299/300\n",
            "2/2 [==============================] - 0s 16ms/sample - loss: 0.2756\n",
            "Epoch 300/300\n",
            "2/2 [==============================] - 0s 11ms/sample - loss: 0.2754\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7fa93794a550>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GXtrye20SNrH",
        "colab_type": "code",
        "outputId": "a179d872-76a8-4962-d73d-f5f5430cda98",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 258
        }
      },
      "source": [
        "import tensorflow_hub as hub\n",
        "import tensorflow as tf\n",
        "print (text_list)\n",
        "try:\n",
        "  text_list=sum(text_list, [])\n",
        "except:\n",
        "  pass\n",
        "\n",
        "embeddings = loaded_module_obj(text_list)\n",
        "\n",
        "# print (embeddings.numpy())\n",
        "\n",
        "embed_target=embeddings.numpy()\n",
        "import pandas as pd\n",
        "doc_embed = pd.DataFrame(data=embed_target)\n",
        "doc_embed.index=text_list\n",
        "\n",
        "\n",
        "\n",
        "finetuned_similiarity=get_similiarity(doc_embed,doc_embed)\n",
        "# np.fill_diagonal(sim.values, 0)\n",
        "finetuned_similiarity.index=text_list\n",
        "finetuned_similiarity.columns=text_list\n",
        "print (\"With Fine Tune\")\n",
        "finetuned_similiarity"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[['Man is going to Moon', 'Education is greatest gift to humanity'], ['Man achieved great feat in apollo mission', 'Literacy is important for civilization']]\n",
            "With Fine Tune\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Man is going to Moon</th>\n",
              "      <th>Education is greatest gift to humanity</th>\n",
              "      <th>Man achieved great feat in apollo mission</th>\n",
              "      <th>Literacy is important for civilization</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>Man is going to Moon</th>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.142228</td>\n",
              "      <td>0.467676</td>\n",
              "      <td>0.135820</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Education is greatest gift to humanity</th>\n",
              "      <td>0.142228</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.197816</td>\n",
              "      <td>0.483066</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Man achieved great feat in apollo mission</th>\n",
              "      <td>0.467676</td>\n",
              "      <td>0.197816</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.087623</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Literacy is important for civilization</th>\n",
              "      <td>0.135820</td>\n",
              "      <td>0.483066</td>\n",
              "      <td>0.087623</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                           Man is going to Moon  ...  Literacy is important for civilization\n",
              "Man is going to Moon                                   1.000000  ...                                0.135820\n",
              "Education is greatest gift to humanity                 0.142228  ...                                0.483066\n",
              "Man achieved great feat in apollo mission              0.467676  ...                                0.087623\n",
              "Literacy is important for civilization                 0.135820  ...                                1.000000\n",
              "\n",
              "[4 rows x 4 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wp4KFVABTCMm",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 289
        },
        "outputId": "187aca6b-4936-44b6-ddbf-cd921c015a97"
      },
      "source": [
        "#Without fine tune: Similiarity\n",
        "########################################\n",
        "import tensorflow_hub as hub\n",
        "import tensorflow as tf\n",
        "embed = hub.load(huburl)\n",
        "try:\n",
        "  text_list=sum(text_list, [])\n",
        "except:\n",
        "  pass\n",
        "text_list.append('Moon was in astrology of Man')\n",
        "\n",
        "embeddings = embed(text_list)\n",
        "\n",
        "embed_target=embeddings.numpy()\n",
        "import pandas as pd\n",
        "doc_embed_global = pd.DataFrame(data=embed_target)\n",
        "doc_embed_global.index=text_list\n",
        "########################################\n",
        "global_similiarity=get_similiarity(doc_embed_global,doc_embed_global)\n",
        "# np.fill_diagonal(sim.values, 0)\n",
        "global_similiarity.index=text_list\n",
        "global_similiarity.columns=text_list\n",
        "\n",
        "print (\"Without Fine Tune\")\n",
        "global_similiarity"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Without Fine Tune\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Man is going to Moon</th>\n",
              "      <th>Education is greatest gift to humanity</th>\n",
              "      <th>Man achieved great feat in apollo mission</th>\n",
              "      <th>Literacy is important for civilization</th>\n",
              "      <th>Moon was in astrology of Man</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>Man is going to Moon</th>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.104139</td>\n",
              "      <td>0.421958</td>\n",
              "      <td>0.099204</td>\n",
              "      <td>0.511980</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Education is greatest gift to humanity</th>\n",
              "      <td>0.104139</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.163523</td>\n",
              "      <td>0.439952</td>\n",
              "      <td>0.103207</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Man achieved great feat in apollo mission</th>\n",
              "      <td>0.421958</td>\n",
              "      <td>0.163523</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.049431</td>\n",
              "      <td>0.272311</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Literacy is important for civilization</th>\n",
              "      <td>0.099204</td>\n",
              "      <td>0.439952</td>\n",
              "      <td>0.049431</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.196779</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Moon was in astrology of Man</th>\n",
              "      <td>0.511980</td>\n",
              "      <td>0.103207</td>\n",
              "      <td>0.272311</td>\n",
              "      <td>0.196779</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                           Man is going to Moon  ...  Moon was in astrology of Man\n",
              "Man is going to Moon                                   1.000000  ...                      0.511980\n",
              "Education is greatest gift to humanity                 0.104139  ...                      0.103207\n",
              "Man achieved great feat in apollo mission              0.421958  ...                      0.272311\n",
              "Literacy is important for civilization                 0.099204  ...                      0.196779\n",
              "Moon was in astrology of Man                           0.511980  ...                      1.000000\n",
              "\n",
              "[5 rows x 5 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4fchzX1wRTwh",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "e5516642-f96d-432e-bbc3-b22cec7e314a"
      },
      "source": [
        "print (\"Following code can be used to export and reuse the fine tuned module from local system.\")\n",
        "import os\n",
        "#Set export to true to export fine tuned model to module_export_dir_name\n",
        "export=False\n",
        "if (export):\n",
        "  module_export_dir_name='finetuned_model_export'\n",
        "  #Creating module export directory in current directory. You can change wherever you need.\n",
        "  os.makedirs(module_export_dir_name,exist_ok=True)\n",
        "  export_module_dir = os.path.join(os.getcwd(), module_export_dir_name)\n",
        "  tf.saved_model.save(loaded_module_obj, export_module_dir)\n",
        "  fine_tuned_module_dir=os.path.join(os.getcwd(),module_export_dir_name)\n",
        "\n",
        "  loaded_module_obj = hub.load(fine_tuned_module_dir)\n",
        "  embeddings = loaded_module_obj(text_list)\n",
        "\n",
        "  embed_target=embeddings.numpy()\n",
        "  import pandas as pd\n",
        "  doc_embed = pd.DataFrame(data=embed_target)\n",
        "  doc_embed.index=text_list\n",
        "\n",
        "\n",
        "  finetuned_similiarity_loaded=get_similiarity(doc_embed,doc_embed)\n",
        "  # np.fill_diagonal(sim.values, 0)\n",
        "  finetuned_similiarity_loaded.index=text_list\n",
        "  finetuned_similiarity_loaded.columns=text_list\n",
        "  print (\"With Fine Tune and Exported\")\n",
        "  finetuned_similiarity_loaded\n",
        "else:\n",
        "  print (\"No Export Selected\")"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Following code can be used to export and reuse the fine tuned module from local system.\n",
            "No Export Selected\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ejDq0GQO2bL0",
        "colab_type": "text"
      },
      "source": [
        "### *Following Sections are reused from Google Universal Sentence Embedding Evaluation. Thanks to TF Hub Authors.*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JgBfRfzRpZBh",
        "colab_type": "text"
      },
      "source": [
        "## Evaluation: STS (Semantic Textual Similarity) Benchmark\n",
        "\n",
        "The [**STS Benchmark**](http://ixa2.si.ehu.es/stswiki/index.php/STSbenchmark) provides an intristic evaluation of the degree to which similarity scores computed using sentence embeddings align with human judgements. The benchmark requires systems to return similarity scores for a diverse selection of sentence pairs. [Pearson correlation](https://en.wikipedia.org/wiki/Pearson_correlation_coefficient) is then used to evaluate the quality of the machine similarity scores against human judgements.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "q5nuBbI1iFQR"
      },
      "source": [
        "### Download data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q14v1nCnpYTy",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "8dec1d36-c45f-4d65-b1f8-9e2204888a17"
      },
      "source": [
        "import pandas\n",
        "import scipy\n",
        "import math\n",
        "import csv\n",
        "\n",
        "sts_dataset = tf.keras.utils.get_file(\n",
        "    fname=\"Stsbenchmark.tar.gz\",\n",
        "    origin=\"http://ixa2.si.ehu.es/stswiki/images/4/48/Stsbenchmark.tar.gz\",\n",
        "    extract=True)\n",
        "sts_dev = pandas.read_table(\n",
        "    os.path.join(os.path.dirname(sts_dataset), \"stsbenchmark\", \"sts-dev.csv\"),\n",
        "    error_bad_lines=False,\n",
        "    skip_blank_lines=True,\n",
        "    usecols=[4, 5, 6],\n",
        "    names=[\"sim\", \"sent_1\", \"sent_2\"])\n",
        "sts_test = pandas.read_table(\n",
        "    os.path.join(\n",
        "        os.path.dirname(sts_dataset), \"stsbenchmark\", \"sts-test.csv\"),\n",
        "    error_bad_lines=False,\n",
        "    quoting=csv.QUOTE_NONE,\n",
        "    skip_blank_lines=True,\n",
        "    usecols=[4, 5, 6],\n",
        "    names=[\"sim\", \"sent_1\", \"sent_2\"])\n",
        "# cleanup some NaN values in sts_dev\n",
        "sts_dev = sts_dev[[isinstance(s, str) for s in sts_dev['sent_2']]]"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from http://ixa2.si.ehu.es/stswiki/images/4/48/Stsbenchmark.tar.gz\n",
            "417792/409630 [==============================] - 3s 7us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "8OKy8WhnKRe_"
      },
      "source": [
        "### Evaluate Sentence Embeddings for Fine Tuned Module\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "W-q2r7jyZGb7",
        "cellView": "both",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "674c9edf-26f9-4c91-c7fe-d5487472030f"
      },
      "source": [
        "sts_data = sts_test #@param [\"sts_dev\", \"sts_test\"] {type:\"raw\"}\n",
        "embed = embed #@param [\"loaded_module_obj\", \"embed\"] {type:\"raw\"}\n",
        "# embed=embed\n",
        "def run_sts_benchmark(batch):\n",
        "  sts_encode1 = tf.nn.l2_normalize(embed(tf.constant(batch['sent_1'].tolist())), axis=1)\n",
        "  sts_encode2 = tf.nn.l2_normalize(embed(tf.constant(batch['sent_2'].tolist())), axis=1)\n",
        "  cosine_similarities = tf.reduce_sum(tf.multiply(sts_encode1, sts_encode2), axis=1)\n",
        "  clip_cosine_similarities = tf.clip_by_value(cosine_similarities, -1.0, 1.0)\n",
        "  scores = 1.0 - tf.acos(clip_cosine_similarities)\n",
        "  \"\"\"Returns the similarity scores\"\"\"\n",
        "  return scores\n",
        "\n",
        "dev_scores = sts_data['sim'].tolist()\n",
        "scores = []\n",
        "for batch in np.array_split(sts_data, 10):\n",
        "  scores.extend(run_sts_benchmark(batch))\n",
        "\n",
        "pearson_correlation = scipy.stats.pearsonr(scores, dev_scores)\n",
        "print('Pearson correlation coefficient = {0}\\np-value = {1}'.format(\n",
        "    pearson_correlation[0], pearson_correlation[1]))"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Pearson correlation coefficient = 0.7821121693112975\n",
            "p-value = 3.811544425526969e-285\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}