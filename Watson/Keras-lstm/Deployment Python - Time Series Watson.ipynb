{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Process Proposed by Hari Prasad for WML development & deployment. D-I-D. Development Notebook-Implementation Python-Deployment Notebook.\n",
    "\n",
    "* Create .py model file as per the coding standards.\n",
    "    * It means we need 3 notebooks. \n",
    "        * Development.pynb for initial development of models and runs.\n",
    "        * Implementation.py which is modified version of Development.pynb with changes as per Watson Coding Standards.\n",
    "            * Remember that we can seperate data cleansing in a seperate file  and call that in Implementation.Not mandatory.\n",
    "        * Deployment.pynb is to deploy the Implementation.py to Cloud & Scheck on Scoring.\n",
    "    * Implementation.py will have the code to connect to cloud storage\n",
    "        * Train Data Download from data bucket & Cleansing\n",
    "        * Model Definition\n",
    "        * Model Compile & Fit\n",
    "        * Model Save to results bucket"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Preperation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use LSTM to predict the signwave."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "angle = np.linspace(-np.pi, np.pi, 201)\n",
    "import matplotlib.pylab as plt\n",
    "y=np.sin(angle)\n",
    "print(type(y))\n",
    "plt.plot(angle,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "values=list()\n",
    "for looped in range(0,100):\n",
    "    for i in range(0,len(y)):\n",
    "        values.append(y[i]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_step=list()\n",
    "for i in range(0,len(values)):\n",
    "        time_step.append(i);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.DataFrame(\n",
    "    {'time_step': time_step,\n",
    "     'valuea': values,\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(data['time_step'][0:800],data['valuea'][0:800])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "series=np.reshape(data['valuea'].values,(np.shape(data['valuea'].values)[0],1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(\"series\",series)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have series repeating every once in 200 cycles.\n",
    "\n",
    "Preparing data for time series"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cloud Section to upload the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ibm_boto3\n",
    "from ibm_botocore.client import Config\n",
    "import os\n",
    "import json\n",
    "import warnings\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cos_credentials = {\n",
    "  \"apikey\": \"\",\n",
    "  \"cos_hmac_keys\": {\n",
    "    \"access_key_id\": \"\",\n",
    "    \"secret_access_key\": \"\"\n",
    "  },\n",
    "  \"endpoints\": \"https://cos-service.bluemix.net/endpoints\",\n",
    "  \"iam_apikey_description\": \"Auto generated apikey during resource-key operation for Instance - crn:v1:bluemix:public:cloud-object-storage:global:a/fa381d574e4a999f5fa9dca6b6b90fe7:94201900-eecd-43d2-ad0c-51692cc709b9::\",\n",
    "  \"iam_apikey_name\": \"auto-generated-apikey-14fc88d0-fb45-4b9d-bbf4-ea1eca5df527\",\n",
    "  \"iam_role_crn\": \"crn:v1:bluemix:public:iam::::serviceRole:Writer\",\n",
    "  \"iam_serviceid_crn\": \"crn:v1:bluemix:public:iam-identity::a/fa381d574e4a999f5fa9dca6b6b90fe7::serviceid:ServiceId-2eb92714-5e02-48e1-b8cf-d1b31cf204fc\",\n",
    "  \"resource_instance_id\": \"crn:v1:bluemix:public:cloud-object-storage:global:a/fa381d574e4a999f5fa9dca6b6b90fe7:94201900-eecd-43d2-ad0c-51692cc709b9::\"\n",
    "}\n",
    "\n",
    "api_key = cos_credentials['apikey']\n",
    "service_instance_id = cos_credentials['resource_instance_id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define endpoint information.\n",
    "service_endpoint = 'https://s3-api.us-geo.objectstorage.softlayer.net'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the authorization endpoint.\n",
    "auth_endpoint = 'https://iam.bluemix.net/oidc/token'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a COS resource.\n",
    "cos = ibm_boto3.resource ('s3',\n",
    "                         ibm_api_key_id=api_key,\n",
    "                         ibm_service_instance_id=service_instance_id,\n",
    "                         ibm_auth_endpoint=auth_endpoint,\n",
    "                         config=Config(signature_version='oauth'),\n",
    "                         endpoint_url=service_endpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create buckets.\n",
    "buckets = ['keras-lstm-data-v5', 'keras-lstm-result-v5']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create buckets.\n",
    "for bucket in buckets:\n",
    "    if not cos.Bucket(bucket) in cos.buckets.all():\n",
    "        print('Creating bucket \"{}\"...'.format(bucket))\n",
    "        try:\n",
    "            cos.create_bucket(Bucket=bucket)\n",
    "        except ibm_boto3.exceptions.ibm_botocore.client.ClientError as e:\n",
    "            print('Error: {}.'.format(e.response['Error']['Message']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display a list of created buckets.\n",
    "print(list(cos.buckets.all()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload files to the buckets.\n",
    "bucket_name = buckets[0]\n",
    "bucket_obj = cos.Bucket(bucket_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install wget\n",
    "link = 'https://github.com/meethariprasad/phd/raw/master/series.npy'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wget, os\n",
    "\n",
    "data_dir = 'keras_lstm'\n",
    "\n",
    "if not os.path.isdir(data_dir):\n",
    "    os.mkdir(data_dir)\n",
    "\n",
    "if not os.path.isfile(os.path.join(data_dir, os.path.join(link.split('/')[-1]))):\n",
    "    wget.download(link, out=data_dir)  \n",
    "        \n",
    "!ls keras_lstm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wget, os\n",
    "\n",
    "data_dir = 'keras_lstm'\n",
    "\n",
    "for filename in os.listdir(data_dir):\n",
    "    with open(os.path.join(data_dir, filename), 'rb') as data: \n",
    "        bucket_obj.upload_file(os.path.join(data_dir, filename), filename)\n",
    "        print('{} is uploaded.'.format(filename))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display a list of buckets.\n",
    "for obj in bucket_obj.objects.all():\n",
    "    print('Object key: {}'.format(obj.key))\n",
    "    print('Object size (kb): {}'.format(obj.size/1024))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Watson Machine Learning Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the libraries.\n",
    "import urllib3, requests, json, base64, time, os\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wml_credentials = {\n",
    "  \"apikey\": \"\",\n",
    "  \"iam_apikey_description\": \"Auto generated apikey during resource-key operation for Instance - crn:v1:bluemix:public:pm-20:us-south:a/fa381d574e4a999f5fa9dca6b6b90fe7:4277973c-258e-4701-a035-42d54566f2b3::\",\n",
    "  \"iam_apikey_name\": \"auto-generated-apikey-2247a1ae-1b9f-48de-8678-4cb3cc722453\",\n",
    "  \"iam_role_crn\": \"crn:v1:bluemix:public:iam::::serviceRole:Writer\",\n",
    "  \"iam_serviceid_crn\": \"crn:v1:bluemix:public:iam-identity::a/fa381d574e4a999f5fa9dca6b6b90fe7::serviceid:ServiceId-0e00fbf7-9a8d-4d4f-b931-7ecd4de1c921\",\n",
    "  \"instance_id\": \"\",\n",
    "  \"password\": \"\",\n",
    "  \"url\": \"https://us-south.ml.cloud.ibm.com\",\n",
    "  \"username\": \"\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from watson_machine_learning_client import WatsonMachineLearningAPIClient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = WatsonMachineLearningAPIClient(wml_credentials)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the client version number.\n",
    "print(client.version)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Training Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_definition_metadata = {\n",
    "            client.repository.DefinitionMetaNames.NAME: \"keras-lstm-v2\",\n",
    "            client.repository.DefinitionMetaNames.FRAMEWORK_NAME: \"tensorflow\",\n",
    "            client.repository.DefinitionMetaNames.FRAMEWORK_VERSION: \"1.5\",\n",
    "            client.repository.DefinitionMetaNames.RUNTIME_NAME: \"python\",\n",
    "            client.repository.DefinitionMetaNames.RUNTIME_VERSION: \"3.5\",\n",
    "            client.repository.DefinitionMetaNames.EXECUTION_COMMAND: \"python3 implementation_keras_lstm_v2.py\"\n",
    "            }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename_keras='implementation_keras_lstm_v2.zip'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -rf implementation_keras_lstm_v2.zip\n",
    "\n",
    "if os.path.isfile(filename_keras):\n",
    "    !ls 'implementation_keras_lstm_v2.zip'\n",
    "else:\n",
    "    !wget https://github.com/meethariprasad/phd/raw/master/Watson/Keras-lstm/implementation_keras_lstm_v2.zip\n",
    "    !ls 'implementation_keras_lstm_v2.zip'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Store Definitions in WML Repository\n",
    "definition_details = client.repository.store_definition(filename_keras, model_definition_metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "definition_url = client.repository.get_definition_url(definition_details)\n",
    "definition_uid = client.repository.get_definition_uid(definition_details)\n",
    "print(definition_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client.repository.list_definitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We need to define Required = Y mandatory to run experiment\n",
    "client.repository.ExperimentMetaNames.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAINING_DATA_REFERENCE = {\n",
    "                            \"connection\": {\n",
    "                                \"endpoint_url\": service_endpoint,\n",
    "                                \"access_key_id\": cos_credentials['cos_hmac_keys']['access_key_id'],\n",
    "                                \"secret_access_key\": cos_credentials['cos_hmac_keys']['secret_access_key']\n",
    "                            },\n",
    "                            \"source\": {\n",
    "                                \"bucket\": buckets[0],\n",
    "                            },\n",
    "                            \"type\": \"s3\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAINING_RESULTS_REFERENCE = {\n",
    "                                \"connection\": {\n",
    "                                    \"endpoint_url\": service_endpoint,\n",
    "                                    \"access_key_id\": cos_credentials['cos_hmac_keys']['access_key_id'],\n",
    "                                    \"secret_access_key\": cos_credentials['cos_hmac_keys']['secret_access_key']\n",
    "                                },\n",
    "                                \"target\": {\n",
    "                                    \"bucket\": buckets[1],\n",
    "                                },\n",
    "                                \"type\": \"s3\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We need to define Required = Y mandatory to run experiment\n",
    "client.repository.ExperimentMetaNames.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_metadata = {\n",
    "            client.repository.ExperimentMetaNames.NAME: \"Keras LSTM Experiment\",\n",
    "            #client.repository.ExperimentMetaNames.DESCRIPTION: \"KERAS LSTM MODEL\",\n",
    "            #client.repository.ExperimentMetaNames.EVALUATION_METHOD: \"multiclass\",\n",
    "            #client.repository.ExperimentMetaNames.EVALUATION_METRICS: [\"val_acc\"],\n",
    "            #client.repository.ExperimentMetaNames.EVALUATION_METRICS: [{\"name\": \"val_acc\", \"maximize\": True}],\n",
    "            client.repository.ExperimentMetaNames.TRAINING_DATA_REFERENCE: TRAINING_DATA_REFERENCE,\n",
    "            client.repository.ExperimentMetaNames.TRAINING_RESULTS_REFERENCE: TRAINING_RESULTS_REFERENCE,\n",
    "            client.repository.ExperimentMetaNames.TRAINING_REFERENCES: [\n",
    "                        {\n",
    "                            \"name\": \"keras_lstm\",\n",
    "                            \"training_definition_url\": definition_url,\n",
    "                            \"compute_configuration\": {\"name\": \"k80x2\"}\n",
    "                            \n",
    "                        }]\n",
    "            }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store the experiment, and display the experiments uid.\n",
    "experiment_details = client.repository.store_experiment(meta_props=experiment_metadata)\n",
    "\n",
    "experiment_uid = client.repository.get_experiment_uid(experiment_details)\n",
    "print(experiment_uid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client.repository.list_experiments()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#old_experiment_guid=\"9607bb06-7e89-41f1-b90c-b574541fb1c5\"\n",
    "#client.repository.delete(old_experiment_guid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "details = client.repository.get_experiment_details(experiment_uid)\n",
    "print(details)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start the experiment run.\n",
    "experiment_run_details = client.experiments.run(experiment_uid, asynchronous=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_run_uid = client.experiments.get_run_uid(experiment_run_details)\n",
    "print(experiment_run_uid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client.experiments.list_runs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_run_details = client.experiments.get_run_details(experiment_run_uid)\n",
    "print(experiment_run_details)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(json.dumps(experiment_details, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client.experiments.get_status(experiment_run_uid)['state']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "status = client.experiments.get_status(experiment_run_uid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Got the run ID from Watson Environment.\n",
    "client.training.get_details('training-WWmHAB5ig')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_uid='training-WWmHAB5ig'\n",
    "saved_model_details = client.repository.store_model(model_uid, {'name': 'Keras LSTM model'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_guid = client.repository.get_model_uid(saved_model_details)\n",
    "print(\"Saved model guid: \" + model_guid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client.repository.list_models()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deployment_details = client.deployments.create(model_guid, name=\"Keras LSTM deployment\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scoring_url = client.deployments.get_scoring_url(deployment_details)\n",
    "print(scoring_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "angle = np.linspace(-np.pi, np.pi, 201)\n",
    "#import matplotlib.pylab as plt\n",
    "y=np.sin(angle)\n",
    "#print(type(y))\n",
    "#plt.plot(angle,y)\n",
    "\n",
    "\n",
    "# In[4]:\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "values=list()\n",
    "for looped in range(0,100):\n",
    "    for i in range(0,len(y)):\n",
    "        values.append(y[i]);\n",
    "\n",
    "\n",
    "# In[5]:\n",
    "\n",
    "\n",
    "time_step=list()\n",
    "for i in range(0,len(values)):\n",
    "        time_step.append(i);\n",
    "\n",
    "\n",
    "# In[6]:\n",
    "\n",
    "\n",
    "data = pd.DataFrame(\n",
    "    {'time_step': time_step,\n",
    "     'valuea': values,\n",
    "    })\n",
    "\n",
    "\n",
    "# In[7]:\n",
    "\n",
    "\n",
    "#plt.plot(data['time_step'][0:800],data['valuea'][0:800])\n",
    "\n",
    "\n",
    "# In[8]:\n",
    "\n",
    "\n",
    "series=np.reshape(data['valuea'].values,(np.shape(data['valuea'].values)[0],1))\n",
    "\n",
    "\n",
    "# We have series repeating every once in 200 cycles.\n",
    "# \n",
    "# Preparing data for time series\n",
    "\n",
    "# In[9]:\n",
    "\n",
    "\n",
    "#Normalize Values\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "#Sin wave b/w -1 to 1\n",
    "scaler = MinMaxScaler(feature_range=(-1, 1))\n",
    "scaler_fit=scaler.fit(series)\n",
    "scaled = scaler_fit.transform(series)\n",
    "series = pd.DataFrame(scaled)\n",
    "\n",
    "\n",
    "# During new data incoming, remember to use scaler used above to scale the data.\n",
    "\n",
    "# In[10]:\n",
    "\n",
    "\n",
    "window_size = 201\n",
    "\n",
    "series_s = series.copy()\n",
    "for i in range(window_size):\n",
    "    series = pd.concat([series, series_s.shift(-(i+1))], axis = 1)\n",
    "    \n",
    "series.dropna(axis=0, inplace=True)\n",
    "\n",
    "\n",
    "# In[11]:\n",
    "\n",
    "\n",
    "#series.shape\n",
    "\n",
    "\n",
    "# In[12]:\n",
    "\n",
    "\n",
    "nrow = round(0.8*series.shape[0])\n",
    "\n",
    "\n",
    "# In[13]:\n",
    "\n",
    "\n",
    "train = series.iloc[:nrow, :]\n",
    "test = series.iloc[nrow:,:]\n",
    "\n",
    "\n",
    "# In[14]:\n",
    "\n",
    "\n",
    "#test.to_csv(\"time_series_train.csv\")\n",
    "#test.to_csv(\"time_series_test.csv\")\n",
    "\n",
    "\n",
    "# In[15]:\n",
    "\n",
    "\n",
    "from sklearn.utils import shuffle\n",
    "train = shuffle(train)\n",
    "\n",
    "\n",
    "# In[16]:\n",
    "\n",
    "\n",
    "train_X = train.iloc[:,:-1]\n",
    "train_y = train.iloc[:,-1]\n",
    "test_X = test.iloc[:,:-1]\n",
    "test_y = test.iloc[:,-1]\n",
    "\n",
    "\n",
    "# In[17]:\n",
    "\n",
    "\n",
    "train_X = train_X.values\n",
    "train_y = train_y.values\n",
    "test_X = test_X.values\n",
    "test_y = test_y.values\n",
    "\n",
    "\n",
    "# In[18]:\n",
    "\n",
    "\n",
    "#print(train_X.shape,train_y.shape,test_X.shape,test_y.shape)\n",
    "\n",
    "\n",
    "# Reshaping the data to match with LSTM\n",
    "\n",
    "# In[19]:\n",
    "\n",
    "\n",
    "train_X = train_X.reshape(train_X.shape[0],train_X.shape[1],1)\n",
    "test_X = test_X.reshape(test_X.shape[0],test_X.shape[1],1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(test_X[0:202].shape)\n",
    "payload_data=test_X[0:202]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: scoring_payload = {'fields': ['GENDER','AGE','MARITAL_STATUS','PROFESSION'], 'values': [['M',23,'Single','Student'],['M',55,'Single','Executive']]}\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scoring_data = {'values': payload_data.tolist()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = client.deployments.score(scoring_url, scoring_data)\n",
    "print(\"Scoring result: \" + str(predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_y[0:202].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_frame=pd.DataFrame(columns=(predictions['fields']))\n",
    "prediction_frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0,len(predictions['values'])):\n",
    "    if i==0:\n",
    "        prediction_data=np.transpose(np.array(predictions['values'][i]))\n",
    "    if i>0:\n",
    "        temp=np.transpose(np.array(predictions['values'][i]))\n",
    "        prediction_data=np.append(pred_data,temp,axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_dataframe=pd.DataFrame(prediction_data,columns=(predictions['fields']))\n",
    "prediction_dataframe.iloc[:,0:1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "actual=test_y[0:203]\n",
    "actuals_dataframe=pd.DataFrame(actual,columns=['actual'])\n",
    "actuals_dataframe.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_actual_df=pd.concat([prediction_dataframe.iloc[:,0:1], actuals_dataframe], axis=1)\n",
    "predict_actual_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We have scaled the data inside -1 to 1 to again -1 to 1. May not make much sense in this example, but for other real data we might want to transform and re-transform.\n",
    "scaler_fit.inverse_transform(predict_actual_df).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_actual_df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will plot and see the prediction and actual. We can see that it is doing well on test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly as pltly\n",
    "import plotly.tools as tls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "mat_plt_plt=plt.figure()\n",
    "plt.plot(predict_actual_df['prediction'][0:200])\n",
    "plt.plot(predict_actual_df['actual'][0:200])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How about zooming in and see how close the predicted and actual plots are?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from plotly import __version__\n",
    "from plotly.offline import download_plotlyjs, init_notebook_mode, plot, iplot\n",
    "init_notebook_mode(connected=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotly_fig = tls.mpl_to_plotly(mat_plt_plt)\n",
    "iplot(plotly_fig, filename='myplot')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Statistically Checking MSE,RMSE,R Squared on test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "from math import sqrt\n",
    "meanSquaredError=mean_squared_error(predict_actual_df['actual'], predict_actual_df['prediction'])\n",
    "print(\"MSE:\", meanSquaredError)\n",
    "rootMeanSquaredError = sqrt(meanSquaredError)\n",
    "print(\"RMSE:\", rootMeanSquaredError)\n",
    "from sklearn.metrics import r2_score\n",
    "print(\"R Squared:\",r2_score(predict_actual_df['actual'], predict_actual_df['prediction']) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given 201 time steps, the model is predicting 202 value.\n",
    "\n",
    "Some realtime steps are missing though.\n",
    "\n",
    "Model is trained once and used without updates. \n",
    "\n",
    "There is a possibility that as and when future data arrives, the accuracy of the model reduces beond acceptable level.\n",
    "\n",
    "One way to ensure model quality is within accceptable level levels is, predict for next value, then when the acctual next data arrives check the difference between actual/predicted and check the model quality. If the model tolerance below the acceptable limit, then retain the model based on new actual values and check if the model quality is increased. If it is not with tolerance limit, set up a alert to data scientist for further research in to model quality. Stakeholders also need to be informed about tolerance level dropped, ensuring they don't infer based on below threshold model. Best way is to put the model quality in dashboard accessible for both stakeholders and data scientists.\n",
    "\n",
    "The answer for above can be explored further in continuous learning.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
