{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Machine Translation from Hindi to English\n",
    "\n",
    "\n",
    "Assignment is to build a Neural Machine Translation (NMT) model to translate Hindi Sentences into machine English. \n",
    "\n",
    "We will do this using by creating attention model as in Neural Machine Translation by Jointly Learning to Align and Translate: Dzmitry Bahdanau, Kyunghyun Cho, Yoshua Bengio https://arxiv.org/pdf/1409.0473.pdf\n",
    "\n",
    "We will be using following small parallel corpus \"http://www.manythings.org/anki/hin-eng.zip\"\n",
    "\n",
    "Notes: In Appendix section at end we have given additional helper functions which can be used to improve model as future improvement effort."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Bidirectional, Concatenate, Permute, Dot, Input, LSTM, Multiply\n",
    "from keras.layers import RepeatVector, Dense, Activation, Lambda\n",
    "from keras.optimizers import Adam\n",
    "from keras.utils import to_categorical\n",
    "from keras.models import load_model, Model\n",
    "import keras.backend as K\n",
    "import numpy as np\n",
    "\n",
    "import random\n",
    "\n",
    "#nmt utils has functions which will be used for Softmax or Data Procesing.\n",
    "#from nmt_utils import *\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 - Translating human readable dates into machine readable dates\n",
    "\n",
    "The model we will build here could be used to translate from from English to Hindi or any other parallel corpus. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 - Dataset\n",
    "\n",
    "In this section we are going to download the dataset and prepare the dataset with Padding & Integer Encoding & One hot encoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hin.txt exists\n"
     ]
    }
   ],
   "source": [
    "import requests, zipfile, io,os\n",
    "r = requests.get(\"http://www.manythings.org/anki/hin-eng.zip\")\n",
    "z = zipfile.ZipFile(io.BytesIO(r.content))\n",
    "z.extractall()\n",
    "#Verifying if the file hin.txt are downloaded properly.\n",
    "if os.path.isfile(\"hin.txt\"):\n",
    "    print('hin.txt exists')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reading the file\n",
    "file=open(\"hin.txt\",'r',encoding='utf-8')\n",
    "content=file.read()\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of clean pairs 2867\n",
      "Saved: english-hindi.pkl\n",
      "[Help] => [बचाओ]\n",
      "[Jump] => [उछलो]\n",
      "[Jump] => [कूदो]\n",
      "[Jump] => [छलांग]\n",
      "[Hello] => [नमस्ते]\n",
      "[Hello] => [नमस्कार]\n",
      "[Cheers] => [वाहवाह]\n",
      "[Cheers] => [चियर्स]\n",
      "[Got it] => [समझे कि नहीं]\n",
      "[Im OK] => [मैं ठीक हूँ]\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "import re\n",
    "from pickle import dump\n",
    "from unicodedata import normalize\n",
    "from numpy import array\n",
    "\n",
    "# load doc into memory\n",
    "def load_doc(filename):\n",
    "\t# open the file as read only\n",
    "\tfile = open(filename, mode='rt', encoding='utf-8')\n",
    "\t# read all text\n",
    "\ttext = file.read()\n",
    "\t# close the file\n",
    "\tfile.close()\n",
    "\treturn text\n",
    "\n",
    "# split a loaded document into sentences\n",
    "def to_pairs(doc):\n",
    "\tlines = doc.strip().split('\\n')\n",
    "\tpairs = [line.split('\\t') for line in  lines]\n",
    "\treturn pairs\n",
    "\n",
    "# clean a list of lines\n",
    "def clean_pairs(lines):\n",
    "\tcleaned = list()\n",
    "\t# prepare regex for char filtering\n",
    "\tre_punc = re.compile('[।%s]' % re.escape(string.punctuation))\n",
    "\tre_print = re.compile('[^%s]' % re.escape(string.printable))\n",
    "\tfor pair in lines:\n",
    "\t\tclean_pair = list()\n",
    "\t\tfor line in pair:\n",
    "\t\t\t# tokenize on white space\n",
    "\t\t\tline = line.split()\n",
    "\t\t\t# remove punctuation from each token\n",
    "\t\t\tline = [re_punc.sub('', w) for w in line]\n",
    "\t\t\t# remove tokens with numbers in them\n",
    "\t\t\t#line = [word for word in line if word.isalpha()]\n",
    "\t\t\t#line=re.sub('[।]', '', line)\n",
    "\t\t\t# store as string\n",
    "\t\t\tclean_pair.append(' '.join(line))\n",
    "\t\tcleaned.append(clean_pair)\n",
    "\treturn array(cleaned)\n",
    "\n",
    "# save a list of clean sentences to file\n",
    "def save_clean_data(sentences, filename):\n",
    "\tdump(sentences, open(filename, 'wb'))\n",
    "\tprint('Saved: %s' % filename)\n",
    "\n",
    "# load dataset\n",
    "filename = 'hin.txt'\n",
    "doc = load_doc(filename)\n",
    "# split into english-hindi pairs\n",
    "pairs = to_pairs(doc)\n",
    "# clean sentences\n",
    "clean_pairs = clean_pairs(pairs)\n",
    "# save clean pairs to file\n",
    "print (\"Number of clean pairs\",clean_pairs.shape[0])\n",
    "save_clean_data(clean_pairs, 'english-hindi.pkl')\n",
    "# spot check\n",
    "for i in range(10):\n",
    "\tprint('[%s] => [%s]' % (clean_pairs[i,0], clean_pairs[i,1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2867\n",
      "Saved: english-hindi-both.pkl\n"
     ]
    }
   ],
   "source": [
    "from pickle import load\n",
    "from pickle import dump\n",
    "from numpy.random import shuffle\n",
    "\n",
    "# load a clean dataset\n",
    "def load_clean_sentences(filename):\n",
    "\treturn load(open(filename, 'rb'))\n",
    "\n",
    "# save a list of clean sentences to file\n",
    "def save_clean_data(sentences, filename):\n",
    "\tdump(sentences, open(filename, 'wb'))\n",
    "\tprint('Saved: %s' % filename)\n",
    "\n",
    "# load dataset\n",
    "raw_dataset = load_clean_sentences('english-hindi.pkl')\n",
    "\n",
    "# reduce dataset size\n",
    "n_sentences = raw_dataset.shape[0]\n",
    "print (n_sentences)\n",
    "dataset = raw_dataset[:n_sentences, :]\n",
    "# random shuffle\n",
    "shuffle(dataset)\n",
    "# split into train/test\n",
    "train, test = dataset[:2800], dataset[2800:]\n",
    "# save\n",
    "save_clean_data(dataset, 'english-hindi-both.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2867, 2)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Check the data sample\n",
    "dataset.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Converting it to tuples.\n",
    "dataset_list=(list(tuple(map(tuple, dataset))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#English Sentence List\n",
    "english_sentences_list=list(dataset[:,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Please make yourself at home'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "english_sentences_list[0]='Please make yourself at home'\n",
    "english_sentences_list[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#English Sentence Unique Word List and Length of Vocabulary\n",
    "english_unique_words=set((' '.join(english_sentences_list)).split())\n",
    "english_vocab_len=len(set((' '.join(english_sentences_list)).split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Hindi Sentence List\n",
    "hindi_sentences_list=list(dataset[:,1])\n",
    "hindi_sentences_list[0]='इसको अपना घर ही समझो'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Hindi Sentence Unique Word List and Length of Vocabulary\n",
    "hindi_unique_words=set((' '.join(hindi_sentences_list)).split())\n",
    "hindi_vocab_len=len(set((' '.join(hindi_sentences_list)).split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating Dictionary with Unknown and Pad elements\n",
    "english_dictionary=dict(zip(sorted(english_unique_words) + ['<unk>', '<pad>'], list(range(len(english_unique_words) + 2))))\n",
    "hindi_dictionary=dict(zip(sorted(hindi_unique_words) + ['<unk>', '<pad>'], list(range(len(hindi_unique_words) + 2))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reverse Dictionary for both languages\n",
    "revere_dictionary_hindi=dict((v,k) for k,v in hindi_dictionary.items())\n",
    "revere_dictionary_english=dict((v,k) for k,v in english_dictionary.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Storing the index of padding value in variables to add it going ahead.\n",
    "english_padding_value=english_dictionary['<pad>']\n",
    "hindi_padding_value=hindi_dictionary['<pad>']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22 25\n"
     ]
    }
   ],
   "source": [
    "#This going to be the global variable with maximum number of words found in a sentence\n",
    "max_english_words=max(len(line.split()) for line in english_sentences_list)\n",
    "max_hindi_words=max(len(line.split()) for line in hindi_sentences_list)\n",
    "print(max_english_words,max_hindi_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_padded_encoding(sentences_list,language_dictionary,max_language_words):\n",
    "    padding_value=language_dictionary['<pad>']\n",
    "    language_array=[]\n",
    "    #Iterate over List.\n",
    "    for sentence in sentences_list:\n",
    "        #Replaces English words with English Vocabulary Indexes and Hindi with Hindi Vocabulary Indexes.\n",
    "        #logic: if a word not in dictionary enters, it will be replaced by unk key value.\n",
    "        single_sentence_array=[]\n",
    "        for word in sentence.split(): \n",
    "            try:\n",
    "                #single_sentence_array=([language_dictionary[word] for word in sentence.split()])\n",
    "                single_sentence_array.append(language_dictionary[word])\n",
    "            except KeyError:\n",
    "                unk='<unk>'\n",
    "                single_sentence_array.append(language_dictionary[unk])\n",
    "        #Find the length of english_single_sentence_array\n",
    "        length_single_sentence=(len(single_sentence_array))\n",
    "        #So how many times padding dictionary key needs to be appended, if we say maximum length of sentences to be considered is eng_max_len.\n",
    "        if (max_language_words>length_single_sentence):\n",
    "            padding_count=(max_language_words-length_single_sentence)\n",
    "        else:\n",
    "            padding_count=0\n",
    "        if (padding_count>0):\n",
    "            for pad in range(0,padding_count):\n",
    "                single_sentence_array.append(padding_value)\n",
    "        else:\n",
    "            single_sentence_array=single_sentence_array[0:max_language_words]\n",
    "        #Append to main array\n",
    "        language_array.append(single_sentence_array)\n",
    "    #Convert to Numpy array at the end\n",
    "    language_array=np.array(language_array)\n",
    "    return(language_array)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of doing a padding over large sentence size, emperically it is found that it is better to do for a short sentences considering the limitation we are having with respect to corpus size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_length=6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2867, 6) (2867, 6)\n"
     ]
    }
   ],
   "source": [
    "#Get encoded sentences\n",
    "hindi_encoding=get_padded_encoding(hindi_sentences_list,hindi_dictionary,sentence_length)\n",
    "english_encoding=get_padded_encoding(english_sentences_list,english_dictionary,sentence_length)\n",
    "print(hindi_encoding.shape,english_encoding.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mumbai is the capital of the Indian state of Maharashtra मुम्बई भारतीय राज्य महाराष्ट्र की राजधानी है\n",
      "[ 240 1421 2335  716 1710 2335] [2157 2004 2274 2087  502 2269]\n",
      "Mumbai\n",
      "is\n",
      "the\n",
      "capital\n",
      "of\n",
      "the\n",
      "मुम्बई\n",
      "भारतीय\n",
      "राज्य\n",
      "महाराष्ट्र\n",
      "की\n",
      "राजधानी\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2870"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Verifying the encoding and decoding for a sample data.\n",
    "print(english_sentences_list[1],hindi_sentences_list[1])\n",
    "print(english_encoding[1],hindi_encoding[1])\n",
    "#Check if encoding gives back the same answer\n",
    "for key in english_encoding[1]:\n",
    "    print(revere_dictionary_english[key])\n",
    "for key in hindi_encoding[1]:\n",
    "    print(revere_dictionary_hindi[key])\n",
    "english_dictionary['<pad>']\n",
    "hindi_dictionary['<pad>']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X.shape: (2867, 6)\n",
      "Y.shape: (2867, 6)\n",
      "Xoh.shape: (2867, 6, 2871)\n",
      "Yoh.shape: (2867, 6, 2619)\n"
     ]
    }
   ],
   "source": [
    "#We will convert the english and hindi encodings to one hot encodings.\n",
    "#Please note Input is of the dimension (number of sentences,max_length_language(every column is a word))\n",
    "#Output is (number of sentences,Max_length_language(every row is a word),length of vocabulary)\n",
    "#Basically every row of the onehotcode matrix must be for one word.\n",
    "#How=1 => 1 0 0\n",
    "#Are=2 => 0 1 0\n",
    "#You=3 => 0 0 1\n",
    "#We are trying to translate hindi to english, so our X is Hindi and Y is English\n",
    "X=hindi_encoding\n",
    "Y=english_encoding\n",
    "#Note: Instead of one hot we can use word embeddings for Xoh\n",
    "Xoh=np.array(list(map(lambda x: to_categorical(x, num_classes=len(hindi_dictionary)), X)))\n",
    "Yoh=np.array(list(map(lambda x: to_categorical(x, num_classes=len(english_dictionary)), Y)))\n",
    "print(\"X.shape:\", X.shape)\n",
    "print(\"Y.shape:\", Y.shape)\n",
    "print(\"Xoh.shape:\", Xoh.shape)\n",
    "print(\"Yoh.shape:\", Yoh.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6, 6)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Tx = hindi_encoding.shape[1]\n",
    "Ty = english_encoding.shape[1]\n",
    "Tx,Ty"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You we have:\n",
    "- `X`: a processed version of the hindi in the data set, where each character is replaced by an index mapped to the character via `hindi_vocab`. Each date is further padded to $T_x$ values with a special character (< pad >). `X.shape = (m, Tx)`\n",
    "- `Y`: a processed version of the english sentences in the data set, where each character is replaced by the index it is mapped to in `english_vocab`. You should have `Y.shape = (m, Ty)`. \n",
    "- `Xoh`: one-hot version of `X`, the \"1\" entry's index is mapped to the character thanks to `hindi_vocab`. `Xoh.shape = (m, Tx, len(hindi_vocab))`\n",
    "- `Yoh`: one-hot version of `Y`, the \"1\" entry's index is mapped to the character thanks to `machine_vocab`. `Yoh.shape = (m, Tx, len(english_vocab))`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets also look at some examples of preprocessed training examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source: मैं घंटों तक उस चाबी को ढूँढता रहा जिसे मैंने गिरा दिया था\n",
      "Target: I spent hours looking for the key that I had dropped\n",
      "\n",
      "Source after preprocessing (indices): [ 218   59  751 2817 2577 2870]\n",
      "Target after preprocessing (indices): [ 273 1564 2614  550 1344 2618]\n",
      "\n",
      "Source after preprocessing (one-hot): [[ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  1.]]\n",
      "Target after preprocessing (one-hot): [[ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  0.]\n",
      " [ 0.  0.  0. ...,  0.  0.  1.]]\n"
     ]
    }
   ],
   "source": [
    "index = 0\n",
    "#The dataset is english -> Hindi\n",
    "#Our target is to generate English given Hindi\n",
    "print(\"Source:\", dataset_list[index][1])\n",
    "print(\"Target:\", dataset_list[index][0])\n",
    "print()\n",
    "print(\"Source after preprocessing (indices):\", X[index])\n",
    "print(\"Target after preprocessing (indices):\", Y[index])\n",
    "print()\n",
    "print(\"Source after preprocessing (one-hot):\", Xoh[index])\n",
    "print(\"Target after preprocessing (one-hot):\", Yoh[index])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 - Neural machine translation with attention\n",
    "\n",
    "The attention mechanism tells a Neural Machine Translation model where it should pay attention to at any step. \n",
    "\n",
    "\n",
    "### 2.1 - Attention mechanism\n",
    "\n",
    "In this part, we will implement the attention mechanism. The diagram on the left shows the attention model. The diagram on the right shows what one \"Attention\" step does to calculate the attention variables $\\alpha^{\\langle t, t' \\rangle}$, which are used to compute the context variable $context^{\\langle t \\rangle}$ for each timestep in the output ($t=1, \\ldots, T_y$). One change that we have in our implementation compared to diagram below here is the Post Attention LSTM will be feeding the previous predicted output also by utilizing return_sequences=True feature of Keras.\n",
    "\n",
    "<table>\n",
    "<td> \n",
    "<img src=\"attn_model.png\" style=\"width:500;height:500px;\"> <br>\n",
    "</td> \n",
    "<td> \n",
    "<img src=\"attn_mechanism.png\" style=\"width:500;height:500px;\"> <br>\n",
    "</td> \n",
    "</table>\n",
    "<caption><center> **Figure 1**: Neural machine translation with attention</center></caption>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Here is the summary of model: \n",
    "\n",
    "- There are two separate LSTMs in this model (see diagram on the left). Because the one at the bottom of the picture is a Bi-directional LSTM and comes *before* the attention mechanism, we will call it *pre-attention* Bi-LSTM. The LSTM at the top of the diagram comes *after* the attention mechanism, so we will call it the *post-attention* LSTM. The pre-attention Bi-LSTM goes through $T_x$ time steps; the post-attention LSTM goes through $T_y$ time steps. \n",
    "\n",
    "- The post-attention LSTM passes $s^{\\langle t \\rangle}, c^{\\langle t \\rangle}$ from one time step to the next. We are using an LSTM here, the LSTM has both the output activation $s^{\\langle t\\rangle}$ and the hidden cell state $c^{\\langle t\\rangle}$. \n",
    "\n",
    "- We use $a^{\\langle t \\rangle} = [\\overrightarrow{a}^{\\langle t \\rangle}; \\overleftarrow{a}^{\\langle t \\rangle}]$ to represent the concatenation of the activations of both the forward-direction and backward-directions of the pre-attention Bi-LSTM. \n",
    "\n",
    "- The diagram on the right uses a `RepeatVector` node to copy $s^{\\langle t-1 \\rangle}$'s value $T_x$ times, and then `Concatenation` to concatenate $s^{\\langle t-1 \\rangle}$ and $a^{\\langle t \\rangle}$ to compute $e^{\\langle t, t'}$, which is then passed through a softmax to compute $\\alpha^{\\langle t, t' \\rangle}$. We'll explain how to use `RepeatVector` and `Concatenation` in Keras below. \n",
    "\n",
    "Implementation detail of the model. \n",
    "\n",
    "We will start by implementing two functions: `one_step_attention()` and `model()`.\n",
    "\n",
    "**1) `one_step_attention()`**: At step $t$, given all the hidden states of the Bi-LSTM ($[a^{<1>},a^{<2>}, ..., a^{<T_x>}]$) and the previous hidden state of the second LSTM ($s^{<t-1>}$), `one_step_attention()` will compute the attention weights ($[\\alpha^{<t,1>},\\alpha^{<t,2>}, ..., \\alpha^{<t,T_x>}]$) and output the context vector (see Figure  1 (right) for details):\n",
    "$$context^{<t>} = \\sum_{t' = 0}^{T_x} \\alpha^{<t,t'>}a^{<t'>}\\tag{1}$$ \n",
    "\n",
    "Note that we are denoting the attention in this notebook $context^{\\langle t \\rangle}$. In the lecture videos, the context was denoted $c^{\\langle t \\rangle}$, but here we are calling it $context^{\\langle t \\rangle}$ to avoid confusion with the (post-attention) LSTM's internal memory cell variable, which is sometimes also denoted $c^{\\langle t \\rangle}$. \n",
    "  \n",
    "**2) `model()`**: Implements the entire model. It first runs the input through a Bi-LSTM to get back $[a^{<1>},a^{<2>}, ..., a^{<T_x>}]$. Then, it calls `one_step_attention()` $T_y$ times (`for` loop). At each iteration of this loop, it gives the computed context vector $c^{<t>}$ to the second LSTM, and runs the output of the LSTM through a dense layer with softmax activation to generate a prediction $\\hat{y}^{<t>}$. \n",
    "\n",
    "\n",
    "\n",
    "Implementation of `one_step_attention()`. The function `model()` will call the layers in `one_step_attention()` $T_y$ using a for-loop, and it is important that all $T_y$ copies have the same weights. I.e., it should not re-initiaiize the weights every time. In other words, all $T_y$ steps should have shared weights. Here's how we implement layers with shareable weights in Keras:\n",
    "1. Define the layer objects (as global variables for examples).\n",
    "2. Call these objects when propagating the input.\n",
    "\n",
    "We have defined the layers we need as global variables. Please run the following cells to create them. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x, axis=1):\n",
    "    \"\"\"Softmax activation function.\n",
    "    # Arguments\n",
    "        x : Tensor.\n",
    "        axis: Integer, axis along which the softmax normalization is applied.\n",
    "    # Returns\n",
    "        Tensor, output of softmax transformation.\n",
    "    # Raises\n",
    "        ValueError: In case `dim(x) == 1`.\n",
    "    \"\"\"\n",
    "    ndim = K.ndim(x)\n",
    "    if ndim == 2:\n",
    "        return K.softmax(x)\n",
    "    elif ndim > 2:\n",
    "        e = K.exp(x - K.max(x, axis=axis, keepdims=True))\n",
    "        s = K.sum(e, axis=axis, keepdims=True)\n",
    "        return e / s\n",
    "    else:\n",
    "        raise ValueError('Cannot apply softmax to a tensor that is 1D')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defined shared layers as global variables\n",
    "repeator = RepeatVector(Tx)\n",
    "concatenator = Concatenate(axis=-1)\n",
    "densor = Dense(1,activation = \"relu\")\n",
    "activator = Activation(softmax, name='attention_weights') \n",
    "dotor = Dot(axes = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you can use these layers to implement `one_step_attention()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_step_attention(a, s_prev):\n",
    "    \"\"\"\n",
    "    Performs one step of attention: Outputs a context vector computed as a dot product of the attention weights\n",
    "    \"alphas\" and the hidden states \"a\" of the Bi-LSTM.\n",
    "    \n",
    "    Arguments:\n",
    "    a -- hidden state output of the Bi-LSTM, numpy-array of shape (m, Tx, 2*n_a)\n",
    "    s_prev -- previous hidden state of the (post-attention) LSTM, numpy-array of shape (m, n_s)\n",
    "    \n",
    "    Returns:\n",
    "    context -- context vector, input of the next (post-attetion) LSTM cell\n",
    "    \"\"\"\n",
    "    \n",
    "    # Use repeator to repeat s_prev to be of shape (m, Tx, n_s) so that you can concatenate it with all hidden states \"a\" \n",
    "    print (\"s_prev.shape before repeator\",s_prev.shape)\n",
    "    s_prev = repeator(s_prev)\n",
    "    print (\"s_prev.shape after repeator\",s_prev.shape)\n",
    "    print (\"a.shape\",a.shape)\n",
    "    # Use concatenator to concatenate a and s_prev on the last axis \n",
    "    concat = concatenator([a, s_prev])\n",
    "    print (\"concat.shape\",concat.shape)\n",
    "    # Use densor to propagate concat through a small fully-connected neural network to compute the \"energies\" variable e.\n",
    "    e = densor(concat)\n",
    "    print (\"e.shape\",e.shape)\n",
    "    # Use activator and e to compute the attention weights \"alphas\" \n",
    "    alphas = activator(e)\n",
    "    print (\"alphas.shape\",alphas.shape)\n",
    "    # Use dotor together with \"alphas\" and \"a\" to compute the context vector to be given to the next (post-attention) LSTM-cell\n",
    "    context = dotor([alphas, a])\n",
    "    print (\"context.shape\",context.shape)\n",
    "    \n",
    "    return context"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have defined global layers that will share weights to be used in `model()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#n_a and n_s are the LSTM internal states. Can be selected arbitarily.\n",
    "n_a = 500\n",
    "n_s = 500\n",
    "#We have added dropout to avoid overfiting\n",
    "post_activation_LSTM_cell = (LSTM(n_s, activation='relu',return_sequences=True,return_state = True,dropout=0.4))\n",
    "output_layer = Dense(len(english_dictionary), activation=softmax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you can use these layers $T_y$ times in a `for` loop to generate the outputs, and their parameters will not be reinitialized. We will have to carry out the following steps: \n",
    "\n",
    "1. Propagate the input into a Bidirectional LSTM\n",
    "2. Iterate for $t = 0, \\dots, T_y-1$: \n",
    "    1. Call `one_step_attention()` on $[\\alpha^{<t,1>},\\alpha^{<t,2>}, ..., \\alpha^{<t,T_x>}]$ and $s^{<t-1>}$ to get the context vector $context^{<t>}$.\n",
    "    2. Give $context^{<t>}$ to the post-attention LSTM cell. We will pass in the previous hidden-state $s^{\\langle t-1\\rangle}$ and cell-states $c^{\\langle t-1\\rangle}$ of this LSTM using `initial_state= [previous hidden state, previous cell state]`. To predict the next LSTM cell, output are fed again by using return_sequences=True. Get back the new hidden state $s^{<t>}$ and the new cell state $c^{<t>}$.\n",
    "    3. Apply a softmax layer to $s^{<t>}$, get the output. \n",
    "    4. Save the output by adding it to the list of outputs.\n",
    "\n",
    "3. Create Keras model instance, it should have three inputs (\"inputs\", $s^{<0>}$ and $c^{<0>}$) and output the list of \"outputs\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 2871) 2870 (2867, 6, 2871)\n",
      "6 6 500 500 2871 2619\n",
      "<class 'numpy.ndarray'> (1, 2871)\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Masking\n",
    "print((to_categorical(hindi_padding_value, num_classes=len(hindi_dictionary))).shape,hindi_padding_value,Xoh.shape)\n",
    "print(Tx, Ty, n_a, n_s, len(hindi_dictionary), len(english_dictionary))\n",
    "hindi_padding_value_mask=(to_categorical(hindi_padding_value, num_classes=len(hindi_dictionary)))\n",
    "print(type(hindi_padding_value_mask),hindi_padding_value_mask.shape)\n",
    "#print(hindi_padding_value_mask.shape)\n",
    "#hindi_padding_value_mask = K.variable(hindi_padding_value_mask)\n",
    "#Xoh_tensor=K.variable(Xoh)\n",
    "#X_masked = Masking(mask_value=hindi_padding_value_mask)(Xoh_tensor)\n",
    "#Bidirectional(LSTM(n_a, activation='relu',return_sequences=True,dropout=0.4))(X_masked)\n",
    "#hindi_mask=keras.layers.Masking(mask_value=hindi_padding_value_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Masking\n",
    "def model(Tx, Ty, n_a, n_s, source_dictionary_size, target_dictionary_size):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    Tx -- length of the input sequence\n",
    "    Ty -- length of the output sequence\n",
    "    n_a -- hidden state size of the Bi-LSTM\n",
    "    n_s -- hidden state size of the post-attention LSTM\n",
    "    human_vocab_size -- size of the python dictionary \"human_vocab\"\n",
    "    machine_vocab_size -- size of the python dictionary \"machine_vocab\"\n",
    "\n",
    "    Returns:\n",
    "    model -- Keras model instance\n",
    "    \"\"\"\n",
    "    \n",
    "    # Define the inputs of your model with a shape (Tx,)\n",
    "    # Define s0 and c0, initial hidden state for the decoder LSTM of shape (n_s,)\n",
    "    X = Input(shape=(Tx, source_dictionary_size))\n",
    "    s0 = Input(shape=(n_s,), name='s0')\n",
    "    c0 = Input(shape=(n_s,), name='c0')\n",
    "    mask = Input(shape=(1,source_dictionary_size),name='mask')\n",
    "    print(\"mask type and shape\",type(mask),mask.shape)\n",
    "    print(\"X type and shape\",type(X),X.shape)\n",
    "    #print(\"In type and shape\",type(In),In.shape)\n",
    "    print(\"s0 type and shape\",type(s0),s0.shape)\n",
    "    print(\"c0 type and shape\",type(c0),c0.shape)\n",
    "    #print(\"hindi_padding_value_mask type and shape\",type(mask),mask.shape)\n",
    "    X_mask = Masking(mask_value=mask)(X)\n",
    "    print(\"X_mask type and shape\",type(X_mask),X_mask.shape)\n",
    "    s = s0\n",
    "    c = c0\n",
    "    \n",
    "    # Initialize empty list of outputs\n",
    "    outputs = []\n",
    "    \n",
    "    # Step 1: Define pre-attention Bi-LSTM.\n",
    "    a1 = Bidirectional(LSTM(n_a, activation='relu',return_sequences=True,dropout=0.4))(X_mask)\n",
    "    a = Bidirectional(LSTM(n_a, activation='relu',return_sequences=True,dropout=0.4))(a1)\n",
    "    print(a,a.shape)\n",
    "    # Step 2: Iterate for Ty steps\n",
    "    for t in range(Ty):\n",
    "    \n",
    "        # Step 2.A: Perform one step of the attention mechanism to get back the context vector at step t\n",
    "        print(\"Before getting Context: a.shape,s.shape\",a.shape,s.shape)\n",
    "        context = one_step_attention(a, s)\n",
    "        print(\"context.shape,s.shape,c.shape \",context.shape,s.shape,c.shape)\n",
    "        # Step 2.B: Apply the post-attention LSTM cell to the \"context\" vector.\n",
    "        _,s, c = post_activation_LSTM_cell(context, initial_state = [s,c])\n",
    "        # Step 2.C: Apply Dense layer to the hidden state output of the post-attention LSTM \n",
    "        out = output_layer(s)\n",
    "        \n",
    "        # Step 2.D: Append \"out\" to the \"outputs\" list\n",
    "        outputs.append(out)\n",
    "    \n",
    "    # Step 3: Create model instance taking three inputs and returning the list of outputs. \n",
    "    model = Model(inputs = [X, s0, c0,mask], outputs = outputs)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the following cell to create your model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mask type and shape <class 'tensorflow.python.framework.ops.Tensor'> (?, 1, 2871)\n",
      "X type and shape <class 'tensorflow.python.framework.ops.Tensor'> (?, 6, 2871)\n",
      "s0 type and shape <class 'tensorflow.python.framework.ops.Tensor'> (?, 500)\n",
      "c0 type and shape <class 'tensorflow.python.framework.ops.Tensor'> (?, 500)\n",
      "X_mask type and shape <class 'tensorflow.python.framework.ops.Tensor'> (?, 6, 2871)\n",
      "Tensor(\"bidirectional_2/concat_2:0\", shape=(?, ?, 1000), dtype=float32) (?, ?, 1000)\n",
      "Before getting Context: a.shape,s.shape (?, ?, 1000) (?, 500)\n",
      "s_prev.shape before repeator (?, 500)\n",
      "s_prev.shape after repeator (?, 6, 500)\n",
      "a.shape (?, ?, 1000)\n",
      "concat.shape (?, 6, 1500)\n",
      "e.shape (?, 6, 1)\n",
      "alphas.shape (?, 6, 1)\n",
      "context.shape (?, 1, 1000)\n",
      "context.shape,s.shape,c.shape  (?, 1, 1000) (?, 500) (?, 500)\n",
      "Before getting Context: a.shape,s.shape (?, ?, 1000) (?, 500)\n",
      "s_prev.shape before repeator (?, 500)\n",
      "s_prev.shape after repeator (?, 6, 500)\n",
      "a.shape (?, ?, 1000)\n",
      "concat.shape (?, 6, 1500)\n",
      "e.shape (?, 6, 1)\n",
      "alphas.shape (?, 6, 1)\n",
      "context.shape (?, 1, 1000)\n",
      "context.shape,s.shape,c.shape  (?, 1, 1000) (?, 500) (?, 500)\n",
      "Before getting Context: a.shape,s.shape (?, ?, 1000) (?, 500)\n",
      "s_prev.shape before repeator (?, 500)\n",
      "s_prev.shape after repeator (?, 6, 500)\n",
      "a.shape (?, ?, 1000)\n",
      "concat.shape (?, 6, 1500)\n",
      "e.shape (?, 6, 1)\n",
      "alphas.shape (?, 6, 1)\n",
      "context.shape (?, 1, 1000)\n",
      "context.shape,s.shape,c.shape  (?, 1, 1000) (?, 500) (?, 500)\n",
      "Before getting Context: a.shape,s.shape (?, ?, 1000) (?, 500)\n",
      "s_prev.shape before repeator (?, 500)\n",
      "s_prev.shape after repeator (?, 6, 500)\n",
      "a.shape (?, ?, 1000)\n",
      "concat.shape (?, 6, 1500)\n",
      "e.shape (?, 6, 1)\n",
      "alphas.shape (?, 6, 1)\n",
      "context.shape (?, 1, 1000)\n",
      "context.shape,s.shape,c.shape  (?, 1, 1000) (?, 500) (?, 500)\n",
      "Before getting Context: a.shape,s.shape (?, ?, 1000) (?, 500)\n",
      "s_prev.shape before repeator (?, 500)\n",
      "s_prev.shape after repeator (?, 6, 500)\n",
      "a.shape (?, ?, 1000)\n",
      "concat.shape (?, 6, 1500)\n",
      "e.shape (?, 6, 1)\n",
      "alphas.shape (?, 6, 1)\n",
      "context.shape (?, 1, 1000)\n",
      "context.shape,s.shape,c.shape  (?, 1, 1000) (?, 500) (?, 500)\n",
      "Before getting Context: a.shape,s.shape (?, ?, 1000) (?, 500)\n",
      "s_prev.shape before repeator (?, 500)\n",
      "s_prev.shape after repeator (?, 6, 500)\n",
      "a.shape (?, ?, 1000)\n",
      "concat.shape (?, 6, 1500)\n",
      "e.shape (?, 6, 1)\n",
      "alphas.shape (?, 6, 1)\n",
      "context.shape (?, 1, 1000)\n",
      "context.shape,s.shape,c.shape  (?, 1, 1000) (?, 500) (?, 500)\n"
     ]
    }
   ],
   "source": [
    "#We are also printing the shapes, just for the purpose of debug.\n",
    "model = model(Tx, Ty, n_a, n_s, len(hindi_dictionary), len(english_dictionary))\n",
    "\n",
    "#We will need copy of the model which will use the weights from model.fit.\n",
    "#This is done as it is observed there has been issues model.load_weights(weightFile)\n",
    "loaded_model = model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "input_1 (InputLayer)             (None, 6, 2871)       0                                            \n",
      "____________________________________________________________________________________________________\n",
      "masking_1 (Masking)              (None, 6, 2871)       0           input_1[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "bidirectional_1 (Bidirectional)  (None, 6, 1000)       13488000    masking_1[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "s0 (InputLayer)                  (None, 500)           0                                            \n",
      "____________________________________________________________________________________________________\n",
      "bidirectional_2 (Bidirectional)  (None, 6, 1000)       6004000     bidirectional_1[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "repeat_vector_1 (RepeatVector)   (None, 6, 500)        0           s0[0][0]                         \n",
      "                                                                   lstm_1[0][1]                     \n",
      "                                                                   lstm_1[1][1]                     \n",
      "                                                                   lstm_1[2][1]                     \n",
      "                                                                   lstm_1[3][1]                     \n",
      "                                                                   lstm_1[4][1]                     \n",
      "____________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)      (None, 6, 1500)       0           bidirectional_2[0][0]            \n",
      "                                                                   repeat_vector_1[0][0]            \n",
      "                                                                   bidirectional_2[0][0]            \n",
      "                                                                   repeat_vector_1[1][0]            \n",
      "                                                                   bidirectional_2[0][0]            \n",
      "                                                                   repeat_vector_1[2][0]            \n",
      "                                                                   bidirectional_2[0][0]            \n",
      "                                                                   repeat_vector_1[3][0]            \n",
      "                                                                   bidirectional_2[0][0]            \n",
      "                                                                   repeat_vector_1[4][0]            \n",
      "                                                                   bidirectional_2[0][0]            \n",
      "                                                                   repeat_vector_1[5][0]            \n",
      "____________________________________________________________________________________________________\n",
      "dense_1 (Dense)                  (None, 6, 1)          1501        concatenate_1[0][0]              \n",
      "                                                                   concatenate_1[1][0]              \n",
      "                                                                   concatenate_1[2][0]              \n",
      "                                                                   concatenate_1[3][0]              \n",
      "                                                                   concatenate_1[4][0]              \n",
      "                                                                   concatenate_1[5][0]              \n",
      "____________________________________________________________________________________________________\n",
      "attention_weights (Activation)   (None, 6, 1)          0           dense_1[0][0]                    \n",
      "                                                                   dense_1[1][0]                    \n",
      "                                                                   dense_1[2][0]                    \n",
      "                                                                   dense_1[3][0]                    \n",
      "                                                                   dense_1[4][0]                    \n",
      "                                                                   dense_1[5][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "dot_1 (Dot)                      (None, 1, 1000)       0           attention_weights[0][0]          \n",
      "                                                                   bidirectional_2[0][0]            \n",
      "                                                                   attention_weights[1][0]          \n",
      "                                                                   bidirectional_2[0][0]            \n",
      "                                                                   attention_weights[2][0]          \n",
      "                                                                   bidirectional_2[0][0]            \n",
      "                                                                   attention_weights[3][0]          \n",
      "                                                                   bidirectional_2[0][0]            \n",
      "                                                                   attention_weights[4][0]          \n",
      "                                                                   bidirectional_2[0][0]            \n",
      "                                                                   attention_weights[5][0]          \n",
      "                                                                   bidirectional_2[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "c0 (InputLayer)                  (None, 500)           0                                            \n",
      "____________________________________________________________________________________________________\n",
      "lstm_1 (LSTM)                    [(None, 1, 500), (Non 3002000     dot_1[0][0]                      \n",
      "                                                                   s0[0][0]                         \n",
      "                                                                   c0[0][0]                         \n",
      "                                                                   dot_1[1][0]                      \n",
      "                                                                   lstm_1[0][1]                     \n",
      "                                                                   lstm_1[0][2]                     \n",
      "                                                                   dot_1[2][0]                      \n",
      "                                                                   lstm_1[1][1]                     \n",
      "                                                                   lstm_1[1][2]                     \n",
      "                                                                   dot_1[3][0]                      \n",
      "                                                                   lstm_1[2][1]                     \n",
      "                                                                   lstm_1[2][2]                     \n",
      "                                                                   dot_1[4][0]                      \n",
      "                                                                   lstm_1[3][1]                     \n",
      "                                                                   lstm_1[3][2]                     \n",
      "                                                                   dot_1[5][0]                      \n",
      "                                                                   lstm_1[4][1]                     \n",
      "                                                                   lstm_1[4][2]                     \n",
      "____________________________________________________________________________________________________\n",
      "dense_2 (Dense)                  (None, 2619)          1312119     lstm_1[0][1]                     \n",
      "                                                                   lstm_1[1][1]                     \n",
      "                                                                   lstm_1[2][1]                     \n",
      "                                                                   lstm_1[3][1]                     \n",
      "                                                                   lstm_1[4][1]                     \n",
      "                                                                   lstm_1[5][1]                     \n",
      "====================================================================================================\n",
      "Total params: 23,807,620\n",
      "Trainable params: 23,807,620\n",
      "Non-trainable params: 0\n",
      "____________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After creating your model in Keras, we need to compile it and define what loss, optimizer and metrics your are want to use. Compile your model using `categorical_crossentropy` loss, and optimizer rmsprop or Adam."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.optimizers import RMSprop\n",
    "out = model.compile(optimizer='rmsprop'#(lr=0.001, beta_1=0.7, beta_2=0.8, decay=0.02)\n",
    "                    ,metrics=['accuracy'],\n",
    "                    loss='categorical_crossentropy')\n",
    "out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The last step is to define all your inputs and outputs to fit the model:\n",
    "- We already have X of shape $(m ,T_x )$ containing the training examples.\n",
    "- We need to create `s0` and `c0` to initialize your `post_activation_LSTM_cell` with 0s.\n",
    "- Given the `model()` you coded, you need the \"outputs\" to be a list of elements of shape (m, T_y). So that: `outputs[i][0], ..., outputs[i][Ty]` represent the true labels (characters) corresponding to the $i^{th}$ training example (`X[i]`). More generally, `outputs[i][j]` is the true label of the $j^{th}$ character in the $i^{th}$ training example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "s0 = np.zeros((len(dataset_list), n_s))\n",
    "c0 = np.zeros((len(dataset_list), n_s))\n",
    "outputs = list(Yoh.swapaxes(0,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Samples,Training,Testing 2867 2810 57\n",
      "Training X Shape and Y Shape (2810, 6, 2871) (2810, 6, 2619)\n",
      "Testing X Shape and Y Shape (57, 6, 2871) (57, 6, 2619)\n"
     ]
    }
   ],
   "source": [
    "#Divide data in to train & test\n",
    "#How much percentage of total data you need enter the number for training_sample_percentage\n",
    "training_sample_percentage=98\n",
    "training_sample_count=(round(X.shape[0]*training_sample_percentage/100))\n",
    "#training_sample_count=2\n",
    "#For to cover rest of data\n",
    "testing_sample_count=X.shape[0]-training_sample_count\n",
    "#testing_sample_count\n",
    "testing_sample_index=training_sample_count+testing_sample_count\n",
    "\n",
    "print(\"Total Samples,Training,Testing\",X.shape[0],training_sample_count,testing_sample_count)\n",
    "trainXoh=Xoh[0:training_sample_count]\n",
    "trainYoh=Yoh[0:training_sample_count]\n",
    "testXoh=Xoh[training_sample_count:testing_sample_index]\n",
    "testYoh=Yoh[training_sample_count:testing_sample_index]\n",
    "print(\"Training X Shape and Y Shape\",trainXoh.shape,trainYoh.shape)\n",
    "print(\"Testing X Shape and Y Shape\",testXoh.shape,testYoh.shape)\n",
    "train_outputs = list(trainYoh.swapaxes(0,1))\n",
    "test_outputs = list(testYoh.swapaxes(0,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2810, 6, 2871)"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainXoh.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 1, 2871)\n",
      "(2810, 1, 2871)\n",
      "(57, 1, 2871)\n",
      "(2867, 500) (2867, 500)\n"
     ]
    }
   ],
   "source": [
    "train_s0 = np.zeros((training_sample_count, n_s))\n",
    "train_c0 = np.zeros((training_sample_count, n_s))\n",
    "print(np.reshape(hindi_padding_value_mask,(1,hindi_padding_value_mask.shape[0],hindi_padding_value_mask.shape[1])).shape)\n",
    "h_mask=np.reshape(hindi_padding_value_mask,(1,hindi_padding_value_mask.shape[0],hindi_padding_value_mask.shape[1]))\n",
    "h_mask_train=h_mask\n",
    "for train_numbers in range(0,trainXoh.shape[0]-1):\n",
    "    h_mask_train=np.concatenate((h_mask_train,h_mask),axis=0)\n",
    "print(h_mask_train.shape)\n",
    "trainX=[trainXoh, train_s0, train_c0,h_mask_train]\n",
    "trainY=train_outputs\n",
    "test_s0 = np.zeros((testing_sample_count, n_s))\n",
    "test_c0 = np.zeros((testing_sample_count, n_s))\n",
    "h_mask_test=h_mask\n",
    "for train_numbers in range(0,testXoh.shape[0]-1):\n",
    "    h_mask_test=np.concatenate((h_mask_test,h_mask),axis=0)\n",
    "print(h_mask_test.shape)\n",
    "testX=[testXoh, test_s0, test_c0,h_mask_test]\n",
    "testY=test_outputs\n",
    "print(s0.shape,c0.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 2810 samples, validate on 57 samples\n",
      "Epoch 1/200\n",
      "2800/2810 [============================>.] - ETA: 0s - loss: 36.0878 - dense_2_loss_1: 5.6642 - dense_2_loss_2: 6.7177 - dense_2_loss_3: 6.4232 - dense_2_loss_4: 6.5010 - dense_2_loss_5: 5.8265 - dense_2_loss_6: 4.9552 - dense_2_acc_1: 0.1300 - dense_2_acc_2: 0.0482 - dense_2_acc_3: 0.0364 - dense_2_acc_4: 0.0918 - dense_2_acc_5: 0.2271 - dense_2_acc_6: 0.4014Epoch 00000: saving model to main_model_weights_new.h5\n",
      "2810/2810 [==============================] - 132s - loss: 36.0775 - dense_2_loss_1: 5.6623 - dense_2_loss_2: 6.7215 - dense_2_loss_3: 6.4186 - dense_2_loss_4: 6.5037 - dense_2_loss_5: 5.8206 - dense_2_loss_6: 4.9509 - dense_2_acc_1: 0.1299 - dense_2_acc_2: 0.0480 - dense_2_acc_3: 0.0367 - dense_2_acc_4: 0.0915 - dense_2_acc_5: 0.2274 - dense_2_acc_6: 0.4011 - val_loss: 33.4595 - val_dense_2_loss_1: 4.3919 - val_dense_2_loss_2: 6.4839 - val_dense_2_loss_3: 6.1500 - val_dense_2_loss_4: 6.2920 - val_dense_2_loss_5: 5.8988 - val_dense_2_loss_6: 4.2429 - val_dense_2_acc_1: 0.1930 - val_dense_2_acc_2: 0.0175 - val_dense_2_acc_3: 0.0351 - val_dense_2_acc_4: 0.1053 - val_dense_2_acc_5: 0.2105 - val_dense_2_acc_6: 0.4211\n",
      "Epoch 2/200\n",
      "2800/2810 [============================>.] - ETA: 0s - loss: 31.7617 - dense_2_loss_1: 4.2890 - dense_2_loss_2: 6.0123 - dense_2_loss_3: 5.8715 - dense_2_loss_4: 5.9899 - dense_2_loss_5: 5.2656 - dense_2_loss_6: 4.3334 - dense_2_acc_1: 0.1704 - dense_2_acc_2: 0.0861 - dense_2_acc_3: 0.0650 - dense_2_acc_4: 0.1043 - dense_2_acc_5: 0.2329 - dense_2_acc_6: 0.4154Epoch 00001: saving model to main_model_weights_new.h5\n",
      "2810/2810 [==============================] - 128s - loss: 31.7483 - dense_2_loss_1: 4.2864 - dense_2_loss_2: 6.0129 - dense_2_loss_3: 5.8750 - dense_2_loss_4: 5.9846 - dense_2_loss_5: 5.2620 - dense_2_loss_6: 4.3274 - dense_2_acc_1: 0.1705 - dense_2_acc_2: 0.0858 - dense_2_acc_3: 0.0648 - dense_2_acc_4: 0.1050 - dense_2_acc_5: 0.2335 - dense_2_acc_6: 0.4160 - val_loss: 34.5255 - val_dense_2_loss_1: 4.0798 - val_dense_2_loss_2: 6.4922 - val_dense_2_loss_3: 6.2081 - val_dense_2_loss_4: 6.4860 - val_dense_2_loss_5: 6.4659 - val_dense_2_loss_6: 4.7936 - val_dense_2_acc_1: 0.1930 - val_dense_2_acc_2: 0.0000e+00 - val_dense_2_acc_3: 0.0175 - val_dense_2_acc_4: 0.0877 - val_dense_2_acc_5: 0.2105 - val_dense_2_acc_6: 0.4211\n",
      "Epoch 3/200\n",
      "2800/2810 [============================>.] - ETA: 0s - loss: 30.7567 - dense_2_loss_1: 4.0787 - dense_2_loss_2: 5.7710 - dense_2_loss_3: 5.7185 - dense_2_loss_4: 5.8735 - dense_2_loss_5: 5.1340 - dense_2_loss_6: 4.1809 - dense_2_acc_1: 0.1714 - dense_2_acc_2: 0.0854 - dense_2_acc_3: 0.0782 - dense_2_acc_4: 0.1136 - dense_2_acc_5: 0.2300 - dense_2_acc_6: 0.4154Epoch 00002: saving model to main_model_weights_new.h5\n",
      "2810/2810 [==============================] - 128s - loss: 30.7620 - dense_2_loss_1: 4.0761 - dense_2_loss_2: 5.7740 - dense_2_loss_3: 5.7187 - dense_2_loss_4: 5.8751 - dense_2_loss_5: 5.1342 - dense_2_loss_6: 4.1840 - dense_2_acc_1: 0.1715 - dense_2_acc_2: 0.0854 - dense_2_acc_3: 0.0779 - dense_2_acc_4: 0.1132 - dense_2_acc_5: 0.2295 - dense_2_acc_6: 0.4157 - val_loss: 33.0816 - val_dense_2_loss_1: 3.9745 - val_dense_2_loss_2: 6.4235 - val_dense_2_loss_3: 6.0706 - val_dense_2_loss_4: 6.3668 - val_dense_2_loss_5: 5.8594 - val_dense_2_loss_6: 4.3868 - val_dense_2_acc_1: 0.1930 - val_dense_2_acc_2: 0.0000e+00 - val_dense_2_acc_3: 0.0526 - val_dense_2_acc_4: 0.1579 - val_dense_2_acc_5: 0.2105 - val_dense_2_acc_6: 0.4211\n",
      "Epoch 4/200\n",
      "2800/2810 [============================>.] - ETA: 0s - loss: 30.1078 - dense_2_loss_1: 3.9553 - dense_2_loss_2: 5.6459 - dense_2_loss_3: 5.6001 - dense_2_loss_4: 5.7673 - dense_2_loss_5: 5.0415 - dense_2_loss_6: 4.0977 - dense_2_acc_1: 0.1711 - dense_2_acc_2: 0.0850 - dense_2_acc_3: 0.0850 - dense_2_acc_4: 0.1214 - dense_2_acc_5: 0.2389 - dense_2_acc_6: 0.4179Epoch 00003: saving model to main_model_weights_new.h5\n",
      "2810/2810 [==============================] - 129s - loss: 30.1126 - dense_2_loss_1: 3.9537 - dense_2_loss_2: 5.6472 - dense_2_loss_3: 5.5992 - dense_2_loss_4: 5.7669 - dense_2_loss_5: 5.0450 - dense_2_loss_6: 4.1006 - dense_2_acc_1: 0.1712 - dense_2_acc_2: 0.0847 - dense_2_acc_3: 0.0847 - dense_2_acc_4: 0.1210 - dense_2_acc_5: 0.2384 - dense_2_acc_6: 0.4171 - val_loss: 33.0623 - val_dense_2_loss_1: 3.9224 - val_dense_2_loss_2: 6.5076 - val_dense_2_loss_3: 6.0497 - val_dense_2_loss_4: 6.4093 - val_dense_2_loss_5: 5.8730 - val_dense_2_loss_6: 4.3004 - val_dense_2_acc_1: 0.1930 - val_dense_2_acc_2: 0.0000e+00 - val_dense_2_acc_3: 0.0702 - val_dense_2_acc_4: 0.1404 - val_dense_2_acc_5: 0.2105 - val_dense_2_acc_6: 0.4211\n",
      "Epoch 5/200\n",
      "2800/2810 [============================>.] - ETA: 0s - loss: 29.5941 - dense_2_loss_1: 3.8541 - dense_2_loss_2: 5.5313 - dense_2_loss_3: 5.5055 - dense_2_loss_4: 5.6895 - dense_2_loss_5: 4.9630 - dense_2_loss_6: 4.0508 - dense_2_acc_1: 0.1757 - dense_2_acc_2: 0.0932 - dense_2_acc_3: 0.0868 - dense_2_acc_4: 0.1196 - dense_2_acc_5: 0.2375 - dense_2_acc_6: 0.4164Epoch 00004: saving model to main_model_weights_new.h5\n",
      "2810/2810 [==============================] - 129s - loss: 29.5898 - dense_2_loss_1: 3.8536 - dense_2_loss_2: 5.5281 - dense_2_loss_3: 5.5030 - dense_2_loss_4: 5.6882 - dense_2_loss_5: 4.9678 - dense_2_loss_6: 4.0492 - dense_2_acc_1: 0.1762 - dense_2_acc_2: 0.0932 - dense_2_acc_3: 0.0865 - dense_2_acc_4: 0.1196 - dense_2_acc_5: 0.2370 - dense_2_acc_6: 0.4164 - val_loss: 33.5914 - val_dense_2_loss_1: 3.8568 - val_dense_2_loss_2: 6.8080 - val_dense_2_loss_3: 6.1532 - val_dense_2_loss_4: 6.4607 - val_dense_2_loss_5: 5.9143 - val_dense_2_loss_6: 4.3983 - val_dense_2_acc_1: 0.3333 - val_dense_2_acc_2: 0.0000e+00 - val_dense_2_acc_3: 0.0702 - val_dense_2_acc_4: 0.1404 - val_dense_2_acc_5: 0.2105 - val_dense_2_acc_6: 0.4211\n",
      "Epoch 6/200\n",
      "2800/2810 [============================>.] - ETA: 0s - loss: 29.0694 - dense_2_loss_1: 3.7502 - dense_2_loss_2: 5.3932 - dense_2_loss_3: 5.4173 - dense_2_loss_4: 5.6026 - dense_2_loss_5: 4.9195 - dense_2_loss_6: 3.9866 - dense_2_acc_1: 0.2304 - dense_2_acc_2: 0.1025 - dense_2_acc_3: 0.0879 - dense_2_acc_4: 0.1225 - dense_2_acc_5: 0.2454 - dense_2_acc_6: 0.4161Epoch 00005: saving model to main_model_weights_new.h5\n",
      "2810/2810 [==============================] - 129s - loss: 29.0702 - dense_2_loss_1: 3.7508 - dense_2_loss_2: 5.3923 - dense_2_loss_3: 5.4212 - dense_2_loss_4: 5.6050 - dense_2_loss_5: 4.9154 - dense_2_loss_6: 3.9854 - dense_2_acc_1: 0.2306 - dense_2_acc_2: 0.1021 - dense_2_acc_3: 0.0875 - dense_2_acc_4: 0.1224 - dense_2_acc_5: 0.2463 - dense_2_acc_6: 0.4167 - val_loss: 32.8399 - val_dense_2_loss_1: 3.7693 - val_dense_2_loss_2: 6.4577 - val_dense_2_loss_3: 6.0327 - val_dense_2_loss_4: 6.4133 - val_dense_2_loss_5: 5.8591 - val_dense_2_loss_6: 4.3078 - val_dense_2_acc_1: 0.3509 - val_dense_2_acc_2: 0.0000e+00 - val_dense_2_acc_3: 0.0877 - val_dense_2_acc_4: 0.1404 - val_dense_2_acc_5: 0.2105 - val_dense_2_acc_6: 0.4211\n",
      "Epoch 7/200\n",
      "2800/2810 [============================>.] - ETA: 0s - loss: 28.5780 - dense_2_loss_1: 3.6511 - dense_2_loss_2: 5.2859 - dense_2_loss_3: 5.3439 - dense_2_loss_4: 5.5273 - dense_2_loss_5: 4.8541 - dense_2_loss_6: 3.9158 - dense_2_acc_1: 0.2561 - dense_2_acc_2: 0.1175 - dense_2_acc_3: 0.0961 - dense_2_acc_4: 0.1232 - dense_2_acc_5: 0.2486 - dense_2_acc_6: 0.4164Epoch 00006: saving model to main_model_weights_new.h5\n",
      "2810/2810 [==============================] - 128s - loss: 28.5752 - dense_2_loss_1: 3.6504 - dense_2_loss_2: 5.2837 - dense_2_loss_3: 5.3436 - dense_2_loss_4: 5.5266 - dense_2_loss_5: 4.8524 - dense_2_loss_6: 3.9186 - dense_2_acc_1: 0.2562 - dense_2_acc_2: 0.1174 - dense_2_acc_3: 0.0957 - dense_2_acc_4: 0.1235 - dense_2_acc_5: 0.2484 - dense_2_acc_6: 0.4157 - val_loss: 33.2795 - val_dense_2_loss_1: 3.7117 - val_dense_2_loss_2: 6.5766 - val_dense_2_loss_3: 6.1813 - val_dense_2_loss_4: 6.5643 - val_dense_2_loss_5: 5.8603 - val_dense_2_loss_6: 4.3853 - val_dense_2_acc_1: 0.3509 - val_dense_2_acc_2: 0.0175 - val_dense_2_acc_3: 0.1053 - val_dense_2_acc_4: 0.1228 - val_dense_2_acc_5: 0.2105 - val_dense_2_acc_6: 0.4211\n",
      "Epoch 8/200\n",
      "2800/2810 [============================>.] - ETA: 0s - loss: 28.1844 - dense_2_loss_1: 3.5288 - dense_2_loss_2: 5.2086 - dense_2_loss_3: 5.2759 - dense_2_loss_4: 5.4781 - dense_2_loss_5: 4.8106 - dense_2_loss_6: 3.8824 - dense_2_acc_1: 0.2786 - dense_2_acc_2: 0.1250 - dense_2_acc_3: 0.1068 - dense_2_acc_4: 0.1261 - dense_2_acc_5: 0.2489 - dense_2_acc_6: 0.4175Epoch 00007: saving model to main_model_weights_new.h5\n",
      "2810/2810 [==============================] - 127s - loss: 28.1902 - dense_2_loss_1: 3.5317 - dense_2_loss_2: 5.2086 - dense_2_loss_3: 5.2763 - dense_2_loss_4: 5.4793 - dense_2_loss_5: 4.8135 - dense_2_loss_6: 3.8808 - dense_2_acc_1: 0.2779 - dense_2_acc_2: 0.1253 - dense_2_acc_3: 0.1064 - dense_2_acc_4: 0.1260 - dense_2_acc_5: 0.2488 - dense_2_acc_6: 0.4181 - val_loss: 32.6484 - val_dense_2_loss_1: 3.6965 - val_dense_2_loss_2: 6.2607 - val_dense_2_loss_3: 5.9573 - val_dense_2_loss_4: 6.4289 - val_dense_2_loss_5: 5.8824 - val_dense_2_loss_6: 4.4226 - val_dense_2_acc_1: 0.3684 - val_dense_2_acc_2: 0.0000e+00 - val_dense_2_acc_3: 0.1228 - val_dense_2_acc_4: 0.1404 - val_dense_2_acc_5: 0.1930 - val_dense_2_acc_6: 0.4211\n",
      "Epoch 9/200\n",
      "2800/2810 [============================>.] - ETA: 0s - loss: 27.7945 - dense_2_loss_1: 3.4313 - dense_2_loss_2: 5.1047 - dense_2_loss_3: 5.2126 - dense_2_loss_4: 5.4301 - dense_2_loss_5: 4.7713 - dense_2_loss_6: 3.8445 - dense_2_acc_1: 0.3004 - dense_2_acc_2: 0.1371 - dense_2_acc_3: 0.1118 - dense_2_acc_4: 0.1286 - dense_2_acc_5: 0.2546 - dense_2_acc_6: 0.4179Epoch 00008: saving model to main_model_weights_new.h5\n",
      "2810/2810 [==============================] - 129s - loss: 27.7955 - dense_2_loss_1: 3.4330 - dense_2_loss_2: 5.1063 - dense_2_loss_3: 5.2111 - dense_2_loss_4: 5.4298 - dense_2_loss_5: 4.7728 - dense_2_loss_6: 3.8425 - dense_2_acc_1: 0.3007 - dense_2_acc_2: 0.1367 - dense_2_acc_3: 0.1114 - dense_2_acc_4: 0.1288 - dense_2_acc_5: 0.2544 - dense_2_acc_6: 0.4181 - val_loss: 32.8107 - val_dense_2_loss_1: 3.6264 - val_dense_2_loss_2: 6.4412 - val_dense_2_loss_3: 6.0444 - val_dense_2_loss_4: 6.4071 - val_dense_2_loss_5: 5.9454 - val_dense_2_loss_6: 4.3462 - val_dense_2_acc_1: 0.3509 - val_dense_2_acc_2: 0.0175 - val_dense_2_acc_3: 0.0877 - val_dense_2_acc_4: 0.1404 - val_dense_2_acc_5: 0.2281 - val_dense_2_acc_6: 0.4211\n",
      "Epoch 10/200\n",
      "2800/2810 [============================>.] - ETA: 0s - loss: 27.4267 - dense_2_loss_1: 3.3385 - dense_2_loss_2: 5.0530 - dense_2_loss_3: 5.1462 - dense_2_loss_4: 5.3675 - dense_2_loss_5: 4.7244 - dense_2_loss_6: 3.7971 - dense_2_acc_1: 0.3139 - dense_2_acc_2: 0.1354 - dense_2_acc_3: 0.1168 - dense_2_acc_4: 0.1289 - dense_2_acc_5: 0.2521 - dense_2_acc_6: 0.4221Epoch 00009: saving model to main_model_weights_new.h5\n",
      "2810/2810 [==============================] - 129s - loss: 27.4256 - dense_2_loss_1: 3.3357 - dense_2_loss_2: 5.0539 - dense_2_loss_3: 5.1431 - dense_2_loss_4: 5.3704 - dense_2_loss_5: 4.7244 - dense_2_loss_6: 3.7982 - dense_2_acc_1: 0.3139 - dense_2_acc_2: 0.1352 - dense_2_acc_3: 0.1167 - dense_2_acc_4: 0.1288 - dense_2_acc_5: 0.2523 - dense_2_acc_6: 0.4217 - val_loss: 32.7423 - val_dense_2_loss_1: 3.5139 - val_dense_2_loss_2: 6.4160 - val_dense_2_loss_3: 6.2020 - val_dense_2_loss_4: 6.4911 - val_dense_2_loss_5: 5.8146 - val_dense_2_loss_6: 4.3047 - val_dense_2_acc_1: 0.3509 - val_dense_2_acc_2: 0.0351 - val_dense_2_acc_3: 0.1228 - val_dense_2_acc_4: 0.1579 - val_dense_2_acc_5: 0.1930 - val_dense_2_acc_6: 0.4211\n",
      "Epoch 11/200\n",
      "2800/2810 [============================>.] - ETA: 0s - loss: 27.0525 - dense_2_loss_1: 3.2372 - dense_2_loss_2: 4.9900 - dense_2_loss_3: 5.0699 - dense_2_loss_4: 5.3154 - dense_2_loss_5: 4.6898 - dense_2_loss_6: 3.7503 - dense_2_acc_1: 0.3193 - dense_2_acc_2: 0.1389 - dense_2_acc_3: 0.1225 - dense_2_acc_4: 0.1321 - dense_2_acc_5: 0.2568 - dense_2_acc_6: 0.4211Epoch 00010: saving model to main_model_weights_new.h5\n",
      "2810/2810 [==============================] - 129s - loss: 27.0613 - dense_2_loss_1: 3.2373 - dense_2_loss_2: 4.9896 - dense_2_loss_3: 5.0701 - dense_2_loss_4: 5.3127 - dense_2_loss_5: 4.6940 - dense_2_loss_6: 3.7576 - dense_2_acc_1: 0.3192 - dense_2_acc_2: 0.1391 - dense_2_acc_3: 0.1224 - dense_2_acc_4: 0.1317 - dense_2_acc_5: 0.2559 - dense_2_acc_6: 0.4203 - val_loss: 32.4376 - val_dense_2_loss_1: 3.4831 - val_dense_2_loss_2: 6.4649 - val_dense_2_loss_3: 6.0154 - val_dense_2_loss_4: 6.3923 - val_dense_2_loss_5: 5.8258 - val_dense_2_loss_6: 4.2563 - val_dense_2_acc_1: 0.3860 - val_dense_2_acc_2: 0.0175 - val_dense_2_acc_3: 0.1404 - val_dense_2_acc_4: 0.1579 - val_dense_2_acc_5: 0.1930 - val_dense_2_acc_6: 0.4211\n",
      "Epoch 12/200\n",
      "2800/2810 [============================>.] - ETA: 0s - loss: 26.7698 - dense_2_loss_1: 3.1958 - dense_2_loss_2: 4.9245 - dense_2_loss_3: 5.0473 - dense_2_loss_4: 5.2535 - dense_2_loss_5: 4.6227 - dense_2_loss_6: 3.7260 - dense_2_acc_1: 0.3371 - dense_2_acc_2: 0.1386 - dense_2_acc_3: 0.1200 - dense_2_acc_4: 0.1311 - dense_2_acc_5: 0.2607 - dense_2_acc_6: 0.4254Epoch 00011: saving model to main_model_weights_new.h5\n",
      "2810/2810 [==============================] - 129s - loss: 26.7766 - dense_2_loss_1: 3.1985 - dense_2_loss_2: 4.9216 - dense_2_loss_3: 5.0463 - dense_2_loss_4: 5.2536 - dense_2_loss_5: 4.6213 - dense_2_loss_6: 3.7353 - dense_2_acc_1: 0.3367 - dense_2_acc_2: 0.1395 - dense_2_acc_3: 0.1199 - dense_2_acc_4: 0.1306 - dense_2_acc_5: 0.2598 - dense_2_acc_6: 0.4238 - val_loss: 32.5230 - val_dense_2_loss_1: 3.5186 - val_dense_2_loss_2: 6.3853 - val_dense_2_loss_3: 5.9838 - val_dense_2_loss_4: 6.4271 - val_dense_2_loss_5: 5.9268 - val_dense_2_loss_6: 4.2812 - val_dense_2_acc_1: 0.3509 - val_dense_2_acc_2: 0.0175 - val_dense_2_acc_3: 0.1053 - val_dense_2_acc_4: 0.1404 - val_dense_2_acc_5: 0.2281 - val_dense_2_acc_6: 0.4211\n",
      "Epoch 13/200\n",
      "2800/2810 [============================>.] - ETA: 0s - loss: 26.4923 - dense_2_loss_1: 3.1462 - dense_2_loss_2: 4.8619 - dense_2_loss_3: 4.9835 - dense_2_loss_4: 5.2119 - dense_2_loss_5: 4.5978 - dense_2_loss_6: 3.6911 - dense_2_acc_1: 0.3361 - dense_2_acc_2: 0.1432 - dense_2_acc_3: 0.1279 - dense_2_acc_4: 0.1357 - dense_2_acc_5: 0.2596 - dense_2_acc_6: 0.4257Epoch 00012: saving model to main_model_weights_new.h5\n",
      "2810/2810 [==============================] - 128s - loss: 26.5055 - dense_2_loss_1: 3.1486 - dense_2_loss_2: 4.8651 - dense_2_loss_3: 4.9846 - dense_2_loss_4: 5.2144 - dense_2_loss_5: 4.5979 - dense_2_loss_6: 3.6949 - dense_2_acc_1: 0.3356 - dense_2_acc_2: 0.1427 - dense_2_acc_3: 0.1285 - dense_2_acc_4: 0.1352 - dense_2_acc_5: 0.2598 - dense_2_acc_6: 0.4253 - val_loss: 32.8873 - val_dense_2_loss_1: 3.4434 - val_dense_2_loss_2: 6.4974 - val_dense_2_loss_3: 6.0653 - val_dense_2_loss_4: 6.5109 - val_dense_2_loss_5: 6.0289 - val_dense_2_loss_6: 4.3415 - val_dense_2_acc_1: 0.3684 - val_dense_2_acc_2: 0.0351 - val_dense_2_acc_3: 0.0877 - val_dense_2_acc_4: 0.1404 - val_dense_2_acc_5: 0.2281 - val_dense_2_acc_6: 0.4211\n",
      "Epoch 14/200\n",
      "2800/2810 [============================>.] - ETA: 0s - loss: 26.2932 - dense_2_loss_1: 3.1237 - dense_2_loss_2: 4.8167 - dense_2_loss_3: 4.9321 - dense_2_loss_4: 5.1751 - dense_2_loss_5: 4.5660 - dense_2_loss_6: 3.6796 - dense_2_acc_1: 0.3332 - dense_2_acc_2: 0.1432 - dense_2_acc_3: 0.1232 - dense_2_acc_4: 0.1368 - dense_2_acc_5: 0.2604 - dense_2_acc_6: 0.4246Epoch 00013: saving model to main_model_weights_new.h5\n",
      "2810/2810 [==============================] - 128s - loss: 26.2771 - dense_2_loss_1: 3.1207 - dense_2_loss_2: 4.8163 - dense_2_loss_3: 4.9294 - dense_2_loss_4: 5.1750 - dense_2_loss_5: 4.5611 - dense_2_loss_6: 3.6745 - dense_2_acc_1: 0.3335 - dense_2_acc_2: 0.1438 - dense_2_acc_3: 0.1231 - dense_2_acc_4: 0.1374 - dense_2_acc_5: 0.2612 - dense_2_acc_6: 0.4253 - val_loss: 32.9009 - val_dense_2_loss_1: 3.3512 - val_dense_2_loss_2: 6.4969 - val_dense_2_loss_3: 6.1418 - val_dense_2_loss_4: 6.4506 - val_dense_2_loss_5: 6.0247 - val_dense_2_loss_6: 4.4356 - val_dense_2_acc_1: 0.3860 - val_dense_2_acc_2: 0.0351 - val_dense_2_acc_3: 0.1053 - val_dense_2_acc_4: 0.1228 - val_dense_2_acc_5: 0.2281 - val_dense_2_acc_6: 0.4211\n",
      "Epoch 15/200\n",
      "2800/2810 [============================>.] - ETA: 0s - loss: 25.9277 - dense_2_loss_1: 3.0353 - dense_2_loss_2: 4.7623 - dense_2_loss_3: 4.8765 - dense_2_loss_4: 5.1336 - dense_2_loss_5: 4.5076 - dense_2_loss_6: 3.6123 - dense_2_acc_1: 0.3518 - dense_2_acc_2: 0.1479 - dense_2_acc_3: 0.1236 - dense_2_acc_4: 0.1350 - dense_2_acc_5: 0.2629 - dense_2_acc_6: 0.4261Epoch 00014: saving model to main_model_weights_new.h5\n",
      "2810/2810 [==============================] - 128s - loss: 25.9291 - dense_2_loss_1: 3.0360 - dense_2_loss_2: 4.7638 - dense_2_loss_3: 4.8763 - dense_2_loss_4: 5.1353 - dense_2_loss_5: 4.5063 - dense_2_loss_6: 3.6115 - dense_2_acc_1: 0.3520 - dense_2_acc_2: 0.1477 - dense_2_acc_3: 0.1235 - dense_2_acc_4: 0.1345 - dense_2_acc_5: 0.2633 - dense_2_acc_6: 0.4260 - val_loss: 32.7002 - val_dense_2_loss_1: 3.4309 - val_dense_2_loss_2: 6.4608 - val_dense_2_loss_3: 6.0528 - val_dense_2_loss_4: 6.4734 - val_dense_2_loss_5: 5.9984 - val_dense_2_loss_6: 4.2839 - val_dense_2_acc_1: 0.3509 - val_dense_2_acc_2: 0.0351 - val_dense_2_acc_3: 0.0877 - val_dense_2_acc_4: 0.1228 - val_dense_2_acc_5: 0.2281 - val_dense_2_acc_6: 0.4211\n",
      "Epoch 16/200\n",
      "2800/2810 [============================>.] - ETA: 0s - loss: 25.7013 - dense_2_loss_1: 2.9939 - dense_2_loss_2: 4.7070 - dense_2_loss_3: 4.8419 - dense_2_loss_4: 5.0739 - dense_2_loss_5: 4.4836 - dense_2_loss_6: 3.6010 - dense_2_acc_1: 0.3457 - dense_2_acc_2: 0.1479 - dense_2_acc_3: 0.1336 - dense_2_acc_4: 0.1396 - dense_2_acc_5: 0.2607 - dense_2_acc_6: 0.4268Epoch 00015: saving model to main_model_weights_new.h5\n",
      "2810/2810 [==============================] - 128s - loss: 25.7024 - dense_2_loss_1: 2.9947 - dense_2_loss_2: 4.7082 - dense_2_loss_3: 4.8400 - dense_2_loss_4: 5.0742 - dense_2_loss_5: 4.4863 - dense_2_loss_6: 3.5990 - dense_2_acc_1: 0.3452 - dense_2_acc_2: 0.1480 - dense_2_acc_3: 0.1335 - dense_2_acc_4: 0.1391 - dense_2_acc_5: 0.2598 - dense_2_acc_6: 0.4267 - val_loss: 33.2852 - val_dense_2_loss_1: 3.3106 - val_dense_2_loss_2: 6.6487 - val_dense_2_loss_3: 6.0584 - val_dense_2_loss_4: 6.6198 - val_dense_2_loss_5: 6.2104 - val_dense_2_loss_6: 4.4374 - val_dense_2_acc_1: 0.3860 - val_dense_2_acc_2: 0.0351 - val_dense_2_acc_3: 0.0877 - val_dense_2_acc_4: 0.1228 - val_dense_2_acc_5: 0.2281 - val_dense_2_acc_6: 0.4211\n",
      "Epoch 17/200\n",
      "2800/2810 [============================>.] - ETA: 0s - loss: 25.4025 - dense_2_loss_1: 2.9358 - dense_2_loss_2: 4.6465 - dense_2_loss_3: 4.7799 - dense_2_loss_4: 5.0393 - dense_2_loss_5: 4.4568 - dense_2_loss_6: 3.5442 - dense_2_acc_1: 0.3650 - dense_2_acc_2: 0.1546 - dense_2_acc_3: 0.1364 - dense_2_acc_4: 0.1382 - dense_2_acc_5: 0.2668 - dense_2_acc_6: 0.4282Epoch 00016: saving model to main_model_weights_new.h5\n",
      "2810/2810 [==============================] - 129s - loss: 25.4070 - dense_2_loss_1: 2.9320 - dense_2_loss_2: 4.6454 - dense_2_loss_3: 4.7810 - dense_2_loss_4: 5.0391 - dense_2_loss_5: 4.4582 - dense_2_loss_6: 3.5513 - dense_2_acc_1: 0.3655 - dense_2_acc_2: 0.1544 - dense_2_acc_3: 0.1359 - dense_2_acc_4: 0.1384 - dense_2_acc_5: 0.2669 - dense_2_acc_6: 0.4278 - val_loss: 32.6144 - val_dense_2_loss_1: 3.2912 - val_dense_2_loss_2: 6.6441 - val_dense_2_loss_3: 6.0329 - val_dense_2_loss_4: 6.4053 - val_dense_2_loss_5: 5.9419 - val_dense_2_loss_6: 4.2990 - val_dense_2_acc_1: 0.4035 - val_dense_2_acc_2: 0.0526 - val_dense_2_acc_3: 0.1053 - val_dense_2_acc_4: 0.1228 - val_dense_2_acc_5: 0.2456 - val_dense_2_acc_6: 0.4211\n",
      "Epoch 18/200\n",
      "2800/2810 [============================>.] - ETA: 0s - loss: 25.1831 - dense_2_loss_1: 2.8979 - dense_2_loss_2: 4.6023 - dense_2_loss_3: 4.7426 - dense_2_loss_4: 5.0038 - dense_2_loss_5: 4.4079 - dense_2_loss_6: 3.5286 - dense_2_acc_1: 0.3643 - dense_2_acc_2: 0.1621 - dense_2_acc_3: 0.1414 - dense_2_acc_4: 0.1414 - dense_2_acc_5: 0.2700 - dense_2_acc_6: 0.4268Epoch 00017: saving model to main_model_weights_new.h5\n",
      "2810/2810 [==============================] - 128s - loss: 25.1761 - dense_2_loss_1: 2.8967 - dense_2_loss_2: 4.5988 - dense_2_loss_3: 4.7380 - dense_2_loss_4: 5.0022 - dense_2_loss_5: 4.4089 - dense_2_loss_6: 3.5314 - dense_2_acc_1: 0.3648 - dense_2_acc_2: 0.1623 - dense_2_acc_3: 0.1420 - dense_2_acc_4: 0.1416 - dense_2_acc_5: 0.2694 - dense_2_acc_6: 0.4263 - val_loss: 33.0685 - val_dense_2_loss_1: 3.3583 - val_dense_2_loss_2: 6.7173 - val_dense_2_loss_3: 6.1179 - val_dense_2_loss_4: 6.5980 - val_dense_2_loss_5: 5.8982 - val_dense_2_loss_6: 4.3788 - val_dense_2_acc_1: 0.3684 - val_dense_2_acc_2: 0.0351 - val_dense_2_acc_3: 0.1053 - val_dense_2_acc_4: 0.1404 - val_dense_2_acc_5: 0.2281 - val_dense_2_acc_6: 0.4211\n",
      "Epoch 19/200\n",
      "2800/2810 [============================>.] - ETA: 0s - loss: 24.9379 - dense_2_loss_1: 2.8610 - dense_2_loss_2: 4.5577 - dense_2_loss_3: 4.6996 - dense_2_loss_4: 4.9525 - dense_2_loss_5: 4.3673 - dense_2_loss_6: 3.4999 - dense_2_acc_1: 0.3686 - dense_2_acc_2: 0.1568 - dense_2_acc_3: 0.1382 - dense_2_acc_4: 0.1425 - dense_2_acc_5: 0.2657 - dense_2_acc_6: 0.4289Epoch 00018: saving model to main_model_weights_new.h5\n",
      "2810/2810 [==============================] - 128s - loss: 24.9432 - dense_2_loss_1: 2.8628 - dense_2_loss_2: 4.5571 - dense_2_loss_3: 4.7043 - dense_2_loss_4: 4.9500 - dense_2_loss_5: 4.3660 - dense_2_loss_6: 3.5030 - dense_2_acc_1: 0.3680 - dense_2_acc_2: 0.1566 - dense_2_acc_3: 0.1381 - dense_2_acc_4: 0.1427 - dense_2_acc_5: 0.2658 - dense_2_acc_6: 0.4285 - val_loss: 32.7422 - val_dense_2_loss_1: 3.3479 - val_dense_2_loss_2: 6.4585 - val_dense_2_loss_3: 5.9899 - val_dense_2_loss_4: 6.5263 - val_dense_2_loss_5: 6.0568 - val_dense_2_loss_6: 4.3628 - val_dense_2_acc_1: 0.3860 - val_dense_2_acc_2: 0.0526 - val_dense_2_acc_3: 0.1053 - val_dense_2_acc_4: 0.1228 - val_dense_2_acc_5: 0.2281 - val_dense_2_acc_6: 0.4211\n",
      "Epoch 20/200\n",
      "2800/2810 [============================>.] - ETA: 0s - loss: 24.6349 - dense_2_loss_1: 2.7919 - dense_2_loss_2: 4.4899 - dense_2_loss_3: 4.6519 - dense_2_loss_4: 4.9158 - dense_2_loss_5: 4.3115 - dense_2_loss_6: 3.4740 - dense_2_acc_1: 0.3821 - dense_2_acc_2: 0.1671 - dense_2_acc_3: 0.1471 - dense_2_acc_4: 0.1500 - dense_2_acc_5: 0.2725 - dense_2_acc_6: 0.4293Epoch 00019: saving model to main_model_weights_new.h5\n",
      "2810/2810 [==============================] - 128s - loss: 24.6467 - dense_2_loss_1: 2.7943 - dense_2_loss_2: 4.4879 - dense_2_loss_3: 4.6553 - dense_2_loss_4: 4.9148 - dense_2_loss_5: 4.3160 - dense_2_loss_6: 3.4784 - dense_2_acc_1: 0.3815 - dense_2_acc_2: 0.1673 - dense_2_acc_3: 0.1473 - dense_2_acc_4: 0.1498 - dense_2_acc_5: 0.2722 - dense_2_acc_6: 0.4285 - val_loss: 32.4435 - val_dense_2_loss_1: 3.1253 - val_dense_2_loss_2: 6.6402 - val_dense_2_loss_3: 6.0317 - val_dense_2_loss_4: 6.5074 - val_dense_2_loss_5: 5.8883 - val_dense_2_loss_6: 4.2506 - val_dense_2_acc_1: 0.4211 - val_dense_2_acc_2: 0.0526 - val_dense_2_acc_3: 0.1053 - val_dense_2_acc_4: 0.1228 - val_dense_2_acc_5: 0.2281 - val_dense_2_acc_6: 0.4211\n",
      "Epoch 21/200\n",
      "2800/2810 [============================>.] - ETA: 0s - loss: 24.4217 - dense_2_loss_1: 2.7703 - dense_2_loss_2: 4.4405 - dense_2_loss_3: 4.6026 - dense_2_loss_4: 4.8802 - dense_2_loss_5: 4.2817 - dense_2_loss_6: 3.4464 - dense_2_acc_1: 0.3839 - dense_2_acc_2: 0.1629 - dense_2_acc_3: 0.1525 - dense_2_acc_4: 0.1479 - dense_2_acc_5: 0.2750 - dense_2_acc_6: 0.4343Epoch 00020: saving model to main_model_weights_new.h5\n",
      "2810/2810 [==============================] - 129s - loss: 24.4245 - dense_2_loss_1: 2.7669 - dense_2_loss_2: 4.4405 - dense_2_loss_3: 4.6043 - dense_2_loss_4: 4.8798 - dense_2_loss_5: 4.2830 - dense_2_loss_6: 3.4500 - dense_2_acc_1: 0.3843 - dense_2_acc_2: 0.1633 - dense_2_acc_3: 0.1520 - dense_2_acc_4: 0.1477 - dense_2_acc_5: 0.2747 - dense_2_acc_6: 0.4331 - val_loss: 33.0983 - val_dense_2_loss_1: 3.1624 - val_dense_2_loss_2: 6.8236 - val_dense_2_loss_3: 6.1019 - val_dense_2_loss_4: 6.6214 - val_dense_2_loss_5: 6.0162 - val_dense_2_loss_6: 4.3728 - val_dense_2_acc_1: 0.4035 - val_dense_2_acc_2: 0.0351 - val_dense_2_acc_3: 0.1404 - val_dense_2_acc_4: 0.1228 - val_dense_2_acc_5: 0.2281 - val_dense_2_acc_6: 0.4211\n",
      "Epoch 22/200\n",
      "2800/2810 [============================>.] - ETA: 0s - loss: 24.2423 - dense_2_loss_1: 2.7569 - dense_2_loss_2: 4.4179 - dense_2_loss_3: 4.5487 - dense_2_loss_4: 4.8360 - dense_2_loss_5: 4.2614 - dense_2_loss_6: 3.4213 - dense_2_acc_1: 0.3804 - dense_2_acc_2: 0.1664 - dense_2_acc_3: 0.1536 - dense_2_acc_4: 0.1532 - dense_2_acc_5: 0.2743 - dense_2_acc_6: 0.4286Epoch 00021: saving model to main_model_weights_new.h5\n",
      "2810/2810 [==============================] - 130s - loss: 24.2396 - dense_2_loss_1: 2.7528 - dense_2_loss_2: 4.4203 - dense_2_loss_3: 4.5496 - dense_2_loss_4: 4.8362 - dense_2_loss_5: 4.2608 - dense_2_loss_6: 3.4199 - dense_2_acc_1: 0.3811 - dense_2_acc_2: 0.1665 - dense_2_acc_3: 0.1534 - dense_2_acc_4: 0.1530 - dense_2_acc_5: 0.2744 - dense_2_acc_6: 0.4292 - val_loss: 33.9017 - val_dense_2_loss_1: 3.2112 - val_dense_2_loss_2: 6.7360 - val_dense_2_loss_3: 6.0851 - val_dense_2_loss_4: 6.7176 - val_dense_2_loss_5: 6.3865 - val_dense_2_loss_6: 4.7654 - val_dense_2_acc_1: 0.4035 - val_dense_2_acc_2: 0.0526 - val_dense_2_acc_3: 0.1228 - val_dense_2_acc_4: 0.1404 - val_dense_2_acc_5: 0.2281 - val_dense_2_acc_6: 0.4211\n",
      "Epoch 23/200\n",
      "2800/2810 [============================>.] - ETA: 0s - loss: 24.0605 - dense_2_loss_1: 2.7147 - dense_2_loss_2: 4.3698 - dense_2_loss_3: 4.5354 - dense_2_loss_4: 4.8104 - dense_2_loss_5: 4.2421 - dense_2_loss_6: 3.3882 - dense_2_acc_1: 0.3889 - dense_2_acc_2: 0.1750 - dense_2_acc_3: 0.1504 - dense_2_acc_4: 0.1518 - dense_2_acc_5: 0.2743 - dense_2_acc_6: 0.4318Epoch 00022: saving model to main_model_weights_new.h5\n",
      "2810/2810 [==============================] - 129s - loss: 24.0574 - dense_2_loss_1: 2.7096 - dense_2_loss_2: 4.3683 - dense_2_loss_3: 4.5350 - dense_2_loss_4: 4.8098 - dense_2_loss_5: 4.2431 - dense_2_loss_6: 3.3916 - dense_2_acc_1: 0.3890 - dense_2_acc_2: 0.1744 - dense_2_acc_3: 0.1502 - dense_2_acc_4: 0.1516 - dense_2_acc_5: 0.2740 - dense_2_acc_6: 0.4310 - val_loss: 33.7571 - val_dense_2_loss_1: 3.2474 - val_dense_2_loss_2: 6.8853 - val_dense_2_loss_3: 6.1736 - val_dense_2_loss_4: 6.8336 - val_dense_2_loss_5: 6.0739 - val_dense_2_loss_6: 4.5433 - val_dense_2_acc_1: 0.3860 - val_dense_2_acc_2: 0.0351 - val_dense_2_acc_3: 0.1053 - val_dense_2_acc_4: 0.1228 - val_dense_2_acc_5: 0.2281 - val_dense_2_acc_6: 0.4211\n",
      "Epoch 24/200\n",
      "2800/2810 [============================>.] - ETA: 0s - loss: 23.8366 - dense_2_loss_1: 2.6535 - dense_2_loss_2: 4.3028 - dense_2_loss_3: 4.4971 - dense_2_loss_4: 4.7741 - dense_2_loss_5: 4.2220 - dense_2_loss_6: 3.3871 - dense_2_acc_1: 0.4004 - dense_2_acc_2: 0.1793 - dense_2_acc_3: 0.1536 - dense_2_acc_4: 0.1604 - dense_2_acc_5: 0.2764 - dense_2_acc_6: 0.4279Epoch 00023: saving model to main_model_weights_new.h5\n",
      "2810/2810 [==============================] - 127s - loss: 23.8485 - dense_2_loss_1: 2.6553 - dense_2_loss_2: 4.3048 - dense_2_loss_3: 4.5005 - dense_2_loss_4: 4.7748 - dense_2_loss_5: 4.2246 - dense_2_loss_6: 3.3885 - dense_2_acc_1: 0.4000 - dense_2_acc_2: 0.1790 - dense_2_acc_3: 0.1537 - dense_2_acc_4: 0.1598 - dense_2_acc_5: 0.2762 - dense_2_acc_6: 0.4278 - val_loss: 33.1325 - val_dense_2_loss_1: 3.1858 - val_dense_2_loss_2: 6.6705 - val_dense_2_loss_3: 6.1204 - val_dense_2_loss_4: 6.6512 - val_dense_2_loss_5: 6.0783 - val_dense_2_loss_6: 4.4263 - val_dense_2_acc_1: 0.4035 - val_dense_2_acc_2: 0.0351 - val_dense_2_acc_3: 0.0877 - val_dense_2_acc_4: 0.1404 - val_dense_2_acc_5: 0.2281 - val_dense_2_acc_6: 0.4211\n",
      "Epoch 25/200\n",
      "2800/2810 [============================>.] - ETA: 0s - loss: 23.5879 - dense_2_loss_1: 2.6080 - dense_2_loss_2: 4.2743 - dense_2_loss_3: 4.4557 - dense_2_loss_4: 4.7226 - dense_2_loss_5: 4.1743 - dense_2_loss_6: 3.3529 - dense_2_acc_1: 0.4004 - dense_2_acc_2: 0.1804 - dense_2_acc_3: 0.1679 - dense_2_acc_4: 0.1571 - dense_2_acc_5: 0.2796 - dense_2_acc_6: 0.4329Epoch 00024: saving model to main_model_weights_new.h5\n",
      "2810/2810 [==============================] - 129s - loss: 23.5876 - dense_2_loss_1: 2.6054 - dense_2_loss_2: 4.2771 - dense_2_loss_3: 4.4543 - dense_2_loss_4: 4.7230 - dense_2_loss_5: 4.1743 - dense_2_loss_6: 3.3534 - dense_2_acc_1: 0.4007 - dense_2_acc_2: 0.1801 - dense_2_acc_3: 0.1680 - dense_2_acc_4: 0.1569 - dense_2_acc_5: 0.2794 - dense_2_acc_6: 0.4327 - val_loss: 33.1214 - val_dense_2_loss_1: 3.2263 - val_dense_2_loss_2: 6.5664 - val_dense_2_loss_3: 6.1820 - val_dense_2_loss_4: 6.7020 - val_dense_2_loss_5: 6.0682 - val_dense_2_loss_6: 4.3765 - val_dense_2_acc_1: 0.4211 - val_dense_2_acc_2: 0.0702 - val_dense_2_acc_3: 0.0877 - val_dense_2_acc_4: 0.1404 - val_dense_2_acc_5: 0.2105 - val_dense_2_acc_6: 0.4211\n",
      "Epoch 26/200\n",
      "2800/2810 [============================>.] - ETA: 0s - loss: 23.4831 - dense_2_loss_1: 2.6151 - dense_2_loss_2: 4.2418 - dense_2_loss_3: 4.4260 - dense_2_loss_4: 4.6996 - dense_2_loss_5: 4.1579 - dense_2_loss_6: 3.3426 - dense_2_acc_1: 0.4046 - dense_2_acc_2: 0.1846 - dense_2_acc_3: 0.1614 - dense_2_acc_4: 0.1543 - dense_2_acc_5: 0.2764 - dense_2_acc_6: 0.4339Epoch 00025: saving model to main_model_weights_new.h5\n",
      "2810/2810 [==============================] - 129s - loss: 23.4751 - dense_2_loss_1: 2.6138 - dense_2_loss_2: 4.2372 - dense_2_loss_3: 4.4248 - dense_2_loss_4: 4.7025 - dense_2_loss_5: 4.1554 - dense_2_loss_6: 3.3413 - dense_2_acc_1: 0.4053 - dense_2_acc_2: 0.1843 - dense_2_acc_3: 0.1616 - dense_2_acc_4: 0.1541 - dense_2_acc_5: 0.2765 - dense_2_acc_6: 0.4342 - val_loss: 33.9811 - val_dense_2_loss_1: 3.1656 - val_dense_2_loss_2: 6.9393 - val_dense_2_loss_3: 6.3500 - val_dense_2_loss_4: 6.6707 - val_dense_2_loss_5: 6.1879 - val_dense_2_loss_6: 4.6676 - val_dense_2_acc_1: 0.4211 - val_dense_2_acc_2: 0.0351 - val_dense_2_acc_3: 0.0702 - val_dense_2_acc_4: 0.1404 - val_dense_2_acc_5: 0.2281 - val_dense_2_acc_6: 0.4211\n",
      "Epoch 27/200\n",
      "2800/2810 [============================>.] - ETA: 0s - loss: 23.2449 - dense_2_loss_1: 2.5731 - dense_2_loss_2: 4.2033 - dense_2_loss_3: 4.3673 - dense_2_loss_4: 4.6657 - dense_2_loss_5: 4.1232 - dense_2_loss_6: 3.3123 - dense_2_acc_1: 0.4146 - dense_2_acc_2: 0.1850 - dense_2_acc_3: 0.1764 - dense_2_acc_4: 0.1579 - dense_2_acc_5: 0.2750 - dense_2_acc_6: 0.4336Epoch 00026: saving model to main_model_weights_new.h5\n",
      "2810/2810 [==============================] - 129s - loss: 23.2595 - dense_2_loss_1: 2.5723 - dense_2_loss_2: 4.2046 - dense_2_loss_3: 4.3718 - dense_2_loss_4: 4.6701 - dense_2_loss_5: 4.1305 - dense_2_loss_6: 3.3101 - dense_2_acc_1: 0.4142 - dense_2_acc_2: 0.1851 - dense_2_acc_3: 0.1762 - dense_2_acc_4: 0.1573 - dense_2_acc_5: 0.2740 - dense_2_acc_6: 0.4342 - val_loss: 33.1544 - val_dense_2_loss_1: 3.1327 - val_dense_2_loss_2: 6.6279 - val_dense_2_loss_3: 6.1077 - val_dense_2_loss_4: 6.6569 - val_dense_2_loss_5: 6.0554 - val_dense_2_loss_6: 4.5738 - val_dense_2_acc_1: 0.4211 - val_dense_2_acc_2: 0.0702 - val_dense_2_acc_3: 0.0877 - val_dense_2_acc_4: 0.1228 - val_dense_2_acc_5: 0.2281 - val_dense_2_acc_6: 0.4211\n",
      "Epoch 28/200\n",
      "2800/2810 [============================>.] - ETA: 0s - loss: 22.9958 - dense_2_loss_1: 2.5275 - dense_2_loss_2: 4.1236 - dense_2_loss_3: 4.3346 - dense_2_loss_4: 4.6136 - dense_2_loss_5: 4.0954 - dense_2_loss_6: 3.3010 - dense_2_acc_1: 0.4182 - dense_2_acc_2: 0.1925 - dense_2_acc_3: 0.1718 - dense_2_acc_4: 0.1561 - dense_2_acc_5: 0.2750 - dense_2_acc_6: 0.4336Epoch 00027: saving model to main_model_weights_new.h5\n",
      "2810/2810 [==============================] - 129s - loss: 23.0081 - dense_2_loss_1: 2.5316 - dense_2_loss_2: 4.1270 - dense_2_loss_3: 4.3338 - dense_2_loss_4: 4.6137 - dense_2_loss_5: 4.0976 - dense_2_loss_6: 3.3045 - dense_2_acc_1: 0.4178 - dense_2_acc_2: 0.1922 - dense_2_acc_3: 0.1712 - dense_2_acc_4: 0.1566 - dense_2_acc_5: 0.2747 - dense_2_acc_6: 0.4335 - val_loss: 33.5537 - val_dense_2_loss_1: 3.1375 - val_dense_2_loss_2: 6.6515 - val_dense_2_loss_3: 6.2931 - val_dense_2_loss_4: 6.6442 - val_dense_2_loss_5: 6.3178 - val_dense_2_loss_6: 4.5095 - val_dense_2_acc_1: 0.4386 - val_dense_2_acc_2: 0.0526 - val_dense_2_acc_3: 0.0877 - val_dense_2_acc_4: 0.1404 - val_dense_2_acc_5: 0.2281 - val_dense_2_acc_6: 0.4211\n",
      "Epoch 29/200\n",
      "2800/2810 [============================>.] - ETA: 0s - loss: 22.8189 - dense_2_loss_1: 2.5209 - dense_2_loss_2: 4.0925 - dense_2_loss_3: 4.2712 - dense_2_loss_4: 4.5994 - dense_2_loss_5: 4.0744 - dense_2_loss_6: 3.2605 - dense_2_acc_1: 0.4239 - dense_2_acc_2: 0.1918 - dense_2_acc_3: 0.1757 - dense_2_acc_4: 0.1589 - dense_2_acc_5: 0.2771 - dense_2_acc_6: 0.4311Epoch 00028: saving model to main_model_weights_new.h5\n",
      "2810/2810 [==============================] - 129s - loss: 22.8078 - dense_2_loss_1: 2.5215 - dense_2_loss_2: 4.0947 - dense_2_loss_3: 4.2718 - dense_2_loss_4: 4.5949 - dense_2_loss_5: 4.0669 - dense_2_loss_6: 3.2580 - dense_2_acc_1: 0.4238 - dense_2_acc_2: 0.1915 - dense_2_acc_3: 0.1758 - dense_2_acc_4: 0.1598 - dense_2_acc_5: 0.2783 - dense_2_acc_6: 0.4317 - val_loss: 33.2980 - val_dense_2_loss_1: 3.1202 - val_dense_2_loss_2: 6.7108 - val_dense_2_loss_3: 6.1286 - val_dense_2_loss_4: 6.6847 - val_dense_2_loss_5: 6.1300 - val_dense_2_loss_6: 4.5238 - val_dense_2_acc_1: 0.4737 - val_dense_2_acc_2: 0.0526 - val_dense_2_acc_3: 0.1404 - val_dense_2_acc_4: 0.1404 - val_dense_2_acc_5: 0.2456 - val_dense_2_acc_6: 0.4211\n",
      "Epoch 30/200\n",
      "2800/2810 [============================>.] - ETA: 0s - loss: 22.6431 - dense_2_loss_1: 2.5203 - dense_2_loss_2: 4.0861 - dense_2_loss_3: 4.2544 - dense_2_loss_4: 4.5426 - dense_2_loss_5: 4.0255 - dense_2_loss_6: 3.2142 - dense_2_acc_1: 0.4204 - dense_2_acc_2: 0.2054 - dense_2_acc_3: 0.1804 - dense_2_acc_4: 0.1646 - dense_2_acc_5: 0.2796 - dense_2_acc_6: 0.4336Epoch 00029: saving model to main_model_weights_new.h5\n",
      "2810/2810 [==============================] - 129s - loss: 22.6400 - dense_2_loss_1: 2.5205 - dense_2_loss_2: 4.0890 - dense_2_loss_3: 4.2521 - dense_2_loss_4: 4.5465 - dense_2_loss_5: 4.0198 - dense_2_loss_6: 3.2121 - dense_2_acc_1: 0.4203 - dense_2_acc_2: 0.2053 - dense_2_acc_3: 0.1808 - dense_2_acc_4: 0.1644 - dense_2_acc_5: 0.2808 - dense_2_acc_6: 0.4335 - val_loss: 33.8584 - val_dense_2_loss_1: 3.1547 - val_dense_2_loss_2: 6.8441 - val_dense_2_loss_3: 6.2554 - val_dense_2_loss_4: 6.6539 - val_dense_2_loss_5: 6.2675 - val_dense_2_loss_6: 4.6829 - val_dense_2_acc_1: 0.4561 - val_dense_2_acc_2: 0.0526 - val_dense_2_acc_3: 0.0702 - val_dense_2_acc_4: 0.1404 - val_dense_2_acc_5: 0.2281 - val_dense_2_acc_6: 0.4211\n",
      "Epoch 31/200\n",
      "2800/2810 [============================>.] - ETA: 0s - loss: 22.4790 - dense_2_loss_1: 2.4529 - dense_2_loss_2: 4.0259 - dense_2_loss_3: 4.2370 - dense_2_loss_4: 4.5239 - dense_2_loss_5: 4.0113 - dense_2_loss_6: 3.2280 - dense_2_acc_1: 0.4386 - dense_2_acc_2: 0.2061 - dense_2_acc_3: 0.1861 - dense_2_acc_4: 0.1629 - dense_2_acc_5: 0.2811 - dense_2_acc_6: 0.4332Epoch 00030: saving model to main_model_weights_new.h5\n",
      "2810/2810 [==============================] - 129s - loss: 22.4712 - dense_2_loss_1: 2.4510 - dense_2_loss_2: 4.0293 - dense_2_loss_3: 4.2333 - dense_2_loss_4: 4.5213 - dense_2_loss_5: 4.0076 - dense_2_loss_6: 3.2288 - dense_2_acc_1: 0.4384 - dense_2_acc_2: 0.2057 - dense_2_acc_3: 0.1865 - dense_2_acc_4: 0.1633 - dense_2_acc_5: 0.2811 - dense_2_acc_6: 0.4331 - val_loss: 33.7838 - val_dense_2_loss_1: 3.1004 - val_dense_2_loss_2: 6.7585 - val_dense_2_loss_3: 6.3251 - val_dense_2_loss_4: 6.8011 - val_dense_2_loss_5: 6.0923 - val_dense_2_loss_6: 4.7063 - val_dense_2_acc_1: 0.4737 - val_dense_2_acc_2: 0.0351 - val_dense_2_acc_3: 0.1404 - val_dense_2_acc_4: 0.1404 - val_dense_2_acc_5: 0.2456 - val_dense_2_acc_6: 0.4035\n",
      "Epoch 32/200\n",
      "2800/2810 [============================>.] - ETA: 0s - loss: 22.2007 - dense_2_loss_1: 2.4183 - dense_2_loss_2: 3.9785 - dense_2_loss_3: 4.1851 - dense_2_loss_4: 4.4925 - dense_2_loss_5: 3.9542 - dense_2_loss_6: 3.1721 - dense_2_acc_1: 0.4354 - dense_2_acc_2: 0.2107 - dense_2_acc_3: 0.1857 - dense_2_acc_4: 0.1700 - dense_2_acc_5: 0.2811 - dense_2_acc_6: 0.4396Epoch 00031: saving model to main_model_weights_new.h5\n",
      "2810/2810 [==============================] - 129s - loss: 22.2092 - dense_2_loss_1: 2.4195 - dense_2_loss_2: 3.9808 - dense_2_loss_3: 4.1847 - dense_2_loss_4: 4.4951 - dense_2_loss_5: 3.9574 - dense_2_loss_6: 3.1717 - dense_2_acc_1: 0.4359 - dense_2_acc_2: 0.2103 - dense_2_acc_3: 0.1858 - dense_2_acc_4: 0.1701 - dense_2_acc_5: 0.2804 - dense_2_acc_6: 0.4395 - val_loss: 33.0779 - val_dense_2_loss_1: 3.0568 - val_dense_2_loss_2: 6.5882 - val_dense_2_loss_3: 6.2506 - val_dense_2_loss_4: 6.6812 - val_dense_2_loss_5: 5.9729 - val_dense_2_loss_6: 4.5282 - val_dense_2_acc_1: 0.4386 - val_dense_2_acc_2: 0.0351 - val_dense_2_acc_3: 0.0877 - val_dense_2_acc_4: 0.1228 - val_dense_2_acc_5: 0.2105 - val_dense_2_acc_6: 0.4211\n",
      "Epoch 33/200\n",
      "2800/2810 [============================>.] - ETA: 0s - loss: 21.9524 - dense_2_loss_1: 2.3721 - dense_2_loss_2: 3.9463 - dense_2_loss_3: 4.1289 - dense_2_loss_4: 4.4236 - dense_2_loss_5: 3.9192 - dense_2_loss_6: 3.1622 - dense_2_acc_1: 0.4400 - dense_2_acc_2: 0.2036 - dense_2_acc_3: 0.1896 - dense_2_acc_4: 0.1707 - dense_2_acc_5: 0.2843 - dense_2_acc_6: 0.4364Epoch 00032: saving model to main_model_weights_new.h5\n",
      "2810/2810 [==============================] - 129s - loss: 21.9448 - dense_2_loss_1: 2.3744 - dense_2_loss_2: 3.9397 - dense_2_loss_3: 4.1253 - dense_2_loss_4: 4.4245 - dense_2_loss_5: 3.9209 - dense_2_loss_6: 3.1600 - dense_2_acc_1: 0.4388 - dense_2_acc_2: 0.2050 - dense_2_acc_3: 0.1897 - dense_2_acc_4: 0.1701 - dense_2_acc_5: 0.2840 - dense_2_acc_6: 0.4363 - val_loss: 33.2642 - val_dense_2_loss_1: 3.0294 - val_dense_2_loss_2: 6.5595 - val_dense_2_loss_3: 6.2646 - val_dense_2_loss_4: 6.7983 - val_dense_2_loss_5: 6.0482 - val_dense_2_loss_6: 4.5642 - val_dense_2_acc_1: 0.4386 - val_dense_2_acc_2: 0.0526 - val_dense_2_acc_3: 0.1404 - val_dense_2_acc_4: 0.1404 - val_dense_2_acc_5: 0.2281 - val_dense_2_acc_6: 0.4035\n",
      "Epoch 34/200\n",
      "2800/2810 [============================>.] - ETA: 0s - loss: 21.8330 - dense_2_loss_1: 2.3686 - dense_2_loss_2: 3.9058 - dense_2_loss_3: 4.0998 - dense_2_loss_4: 4.4076 - dense_2_loss_5: 3.9079 - dense_2_loss_6: 3.1433 - dense_2_acc_1: 0.4421 - dense_2_acc_2: 0.2146 - dense_2_acc_3: 0.1875 - dense_2_acc_4: 0.1686 - dense_2_acc_5: 0.2882 - dense_2_acc_6: 0.4407Epoch 00033: saving model to main_model_weights_new.h5\n",
      "2810/2810 [==============================] - 129s - loss: 21.8401 - dense_2_loss_1: 2.3693 - dense_2_loss_2: 3.9043 - dense_2_loss_3: 4.1001 - dense_2_loss_4: 4.4127 - dense_2_loss_5: 3.9076 - dense_2_loss_6: 3.1461 - dense_2_acc_1: 0.4423 - dense_2_acc_2: 0.2149 - dense_2_acc_3: 0.1875 - dense_2_acc_4: 0.1680 - dense_2_acc_5: 0.2883 - dense_2_acc_6: 0.4406 - val_loss: 34.0096 - val_dense_2_loss_1: 3.1459 - val_dense_2_loss_2: 6.6454 - val_dense_2_loss_3: 6.2150 - val_dense_2_loss_4: 6.7553 - val_dense_2_loss_5: 6.2776 - val_dense_2_loss_6: 4.9704 - val_dense_2_acc_1: 0.4386 - val_dense_2_acc_2: 0.0702 - val_dense_2_acc_3: 0.0877 - val_dense_2_acc_4: 0.1404 - val_dense_2_acc_5: 0.2281 - val_dense_2_acc_6: 0.4211\n",
      "Epoch 35/200\n",
      "2800/2810 [============================>.] - ETA: 0s - loss: 21.6498 - dense_2_loss_1: 2.3545 - dense_2_loss_2: 3.8717 - dense_2_loss_3: 4.0296 - dense_2_loss_4: 4.3808 - dense_2_loss_5: 3.8902 - dense_2_loss_6: 3.1230 - dense_2_acc_1: 0.4482 - dense_2_acc_2: 0.2175 - dense_2_acc_3: 0.1986 - dense_2_acc_4: 0.1761 - dense_2_acc_5: 0.2889 - dense_2_acc_6: 0.4425Epoch 00034: saving model to main_model_weights_new.h5\n",
      "2810/2810 [==============================] - 128s - loss: 21.6483 - dense_2_loss_1: 2.3520 - dense_2_loss_2: 3.8732 - dense_2_loss_3: 4.0317 - dense_2_loss_4: 4.3771 - dense_2_loss_5: 3.8919 - dense_2_loss_6: 3.1224 - dense_2_acc_1: 0.4495 - dense_2_acc_2: 0.2174 - dense_2_acc_3: 0.1986 - dense_2_acc_4: 0.1765 - dense_2_acc_5: 0.2890 - dense_2_acc_6: 0.4431 - val_loss: 33.4884 - val_dense_2_loss_1: 3.0921 - val_dense_2_loss_2: 6.5961 - val_dense_2_loss_3: 6.2067 - val_dense_2_loss_4: 6.9272 - val_dense_2_loss_5: 6.0805 - val_dense_2_loss_6: 4.5860 - val_dense_2_acc_1: 0.4386 - val_dense_2_acc_2: 0.0702 - val_dense_2_acc_3: 0.1053 - val_dense_2_acc_4: 0.1404 - val_dense_2_acc_5: 0.2281 - val_dense_2_acc_6: 0.4035\n",
      "Epoch 36/200\n",
      "2800/2810 [============================>.] - ETA: 0s - loss: 21.5238 - dense_2_loss_1: 2.3400 - dense_2_loss_2: 3.8499 - dense_2_loss_3: 4.0250 - dense_2_loss_4: 4.3350 - dense_2_loss_5: 3.8557 - dense_2_loss_6: 3.1182 - dense_2_acc_1: 0.4543 - dense_2_acc_2: 0.2204 - dense_2_acc_3: 0.1968 - dense_2_acc_4: 0.1754 - dense_2_acc_5: 0.2929 - dense_2_acc_6: 0.4454Epoch 00035: saving model to main_model_weights_new.h5\n",
      "2810/2810 [==============================] - 129s - loss: 21.5309 - dense_2_loss_1: 2.3403 - dense_2_loss_2: 3.8513 - dense_2_loss_3: 4.0253 - dense_2_loss_4: 4.3400 - dense_2_loss_5: 3.8549 - dense_2_loss_6: 3.1191 - dense_2_acc_1: 0.4548 - dense_2_acc_2: 0.2206 - dense_2_acc_3: 0.1968 - dense_2_acc_4: 0.1751 - dense_2_acc_5: 0.2929 - dense_2_acc_6: 0.4452 - val_loss: 33.2337 - val_dense_2_loss_1: 3.0927 - val_dense_2_loss_2: 6.3934 - val_dense_2_loss_3: 6.1375 - val_dense_2_loss_4: 6.7035 - val_dense_2_loss_5: 6.2691 - val_dense_2_loss_6: 4.6377 - val_dense_2_acc_1: 0.4211 - val_dense_2_acc_2: 0.0877 - val_dense_2_acc_3: 0.1053 - val_dense_2_acc_4: 0.1404 - val_dense_2_acc_5: 0.2632 - val_dense_2_acc_6: 0.4211\n",
      "Epoch 37/200\n",
      "2800/2810 [============================>.] - ETA: 0s - loss: 21.2854 - dense_2_loss_1: 2.2971 - dense_2_loss_2: 3.7912 - dense_2_loss_3: 3.9813 - dense_2_loss_4: 4.3161 - dense_2_loss_5: 3.8407 - dense_2_loss_6: 3.0590 - dense_2_acc_1: 0.4536 - dense_2_acc_2: 0.2279 - dense_2_acc_3: 0.2025 - dense_2_acc_4: 0.1800 - dense_2_acc_5: 0.2879 - dense_2_acc_6: 0.4429Epoch 00036: saving model to main_model_weights_new.h5\n",
      "2810/2810 [==============================] - 129s - loss: 21.2901 - dense_2_loss_1: 2.2982 - dense_2_loss_2: 3.7933 - dense_2_loss_3: 3.9884 - dense_2_loss_4: 4.3176 - dense_2_loss_5: 3.8361 - dense_2_loss_6: 3.0564 - dense_2_acc_1: 0.4534 - dense_2_acc_2: 0.2278 - dense_2_acc_3: 0.2018 - dense_2_acc_4: 0.1797 - dense_2_acc_5: 0.2886 - dense_2_acc_6: 0.4434 - val_loss: 33.9958 - val_dense_2_loss_1: 3.0051 - val_dense_2_loss_2: 6.4883 - val_dense_2_loss_3: 6.2530 - val_dense_2_loss_4: 6.8160 - val_dense_2_loss_5: 6.4935 - val_dense_2_loss_6: 4.9399 - val_dense_2_acc_1: 0.4386 - val_dense_2_acc_2: 0.0877 - val_dense_2_acc_3: 0.0526 - val_dense_2_acc_4: 0.1579 - val_dense_2_acc_5: 0.2281 - val_dense_2_acc_6: 0.4211\n",
      "Epoch 38/200\n",
      "2800/2810 [============================>.] - ETA: 0s - loss: 21.1872 - dense_2_loss_1: 2.2795 - dense_2_loss_2: 3.7881 - dense_2_loss_3: 3.9508 - dense_2_loss_4: 4.2929 - dense_2_loss_5: 3.8085 - dense_2_loss_6: 3.0674 - dense_2_acc_1: 0.4664 - dense_2_acc_2: 0.2314 - dense_2_acc_3: 0.2036 - dense_2_acc_4: 0.1843 - dense_2_acc_5: 0.2868 - dense_2_acc_6: 0.4436Epoch 00037: saving model to main_model_weights_new.h5\n",
      "2810/2810 [==============================] - 129s - loss: 21.1932 - dense_2_loss_1: 2.2838 - dense_2_loss_2: 3.7870 - dense_2_loss_3: 3.9512 - dense_2_loss_4: 4.2950 - dense_2_loss_5: 3.8112 - dense_2_loss_6: 3.0651 - dense_2_acc_1: 0.4658 - dense_2_acc_2: 0.2317 - dense_2_acc_3: 0.2036 - dense_2_acc_4: 0.1840 - dense_2_acc_5: 0.2872 - dense_2_acc_6: 0.4441 - val_loss: 34.1405 - val_dense_2_loss_1: 3.0485 - val_dense_2_loss_2: 6.8186 - val_dense_2_loss_3: 6.4787 - val_dense_2_loss_4: 6.8828 - val_dense_2_loss_5: 6.3360 - val_dense_2_loss_6: 4.5759 - val_dense_2_acc_1: 0.4561 - val_dense_2_acc_2: 0.0702 - val_dense_2_acc_3: 0.1053 - val_dense_2_acc_4: 0.1404 - val_dense_2_acc_5: 0.2281 - val_dense_2_acc_6: 0.4211\n",
      "Epoch 39/200\n",
      "2800/2810 [============================>.] - ETA: 0s - loss: 20.9382 - dense_2_loss_1: 2.2401 - dense_2_loss_2: 3.7231 - dense_2_loss_3: 3.9097 - dense_2_loss_4: 4.2457 - dense_2_loss_5: 3.7911 - dense_2_loss_6: 3.0285 - dense_2_acc_1: 0.4714 - dense_2_acc_2: 0.2268 - dense_2_acc_3: 0.2143 - dense_2_acc_4: 0.1854 - dense_2_acc_5: 0.2943 - dense_2_acc_6: 0.4436Epoch 00038: saving model to main_model_weights_new.h5\n",
      "2810/2810 [==============================] - 129s - loss: 20.9326 - dense_2_loss_1: 2.2377 - dense_2_loss_2: 3.7210 - dense_2_loss_3: 3.9080 - dense_2_loss_4: 4.2466 - dense_2_loss_5: 3.7916 - dense_2_loss_6: 3.0276 - dense_2_acc_1: 0.4715 - dense_2_acc_2: 0.2270 - dense_2_acc_3: 0.2146 - dense_2_acc_4: 0.1851 - dense_2_acc_5: 0.2943 - dense_2_acc_6: 0.4434 - val_loss: 34.2875 - val_dense_2_loss_1: 2.9934 - val_dense_2_loss_2: 6.6622 - val_dense_2_loss_3: 6.2998 - val_dense_2_loss_4: 7.0303 - val_dense_2_loss_5: 6.4613 - val_dense_2_loss_6: 4.8404 - val_dense_2_acc_1: 0.4561 - val_dense_2_acc_2: 0.0702 - val_dense_2_acc_3: 0.1053 - val_dense_2_acc_4: 0.1404 - val_dense_2_acc_5: 0.2281 - val_dense_2_acc_6: 0.4035\n",
      "Epoch 40/200\n",
      "2800/2810 [============================>.] - ETA: 0s - loss: 20.7266 - dense_2_loss_1: 2.2066 - dense_2_loss_2: 3.6905 - dense_2_loss_3: 3.8690 - dense_2_loss_4: 4.1961 - dense_2_loss_5: 3.7510 - dense_2_loss_6: 3.0134 - dense_2_acc_1: 0.4686 - dense_2_acc_2: 0.2457 - dense_2_acc_3: 0.2161 - dense_2_acc_4: 0.1929 - dense_2_acc_5: 0.2979 - dense_2_acc_6: 0.4486Epoch 00039: saving model to main_model_weights_new.h5\n",
      "2810/2810 [==============================] - 129s - loss: 20.7245 - dense_2_loss_1: 2.2053 - dense_2_loss_2: 3.6920 - dense_2_loss_3: 3.8684 - dense_2_loss_4: 4.1971 - dense_2_loss_5: 3.7476 - dense_2_loss_6: 3.0142 - dense_2_acc_1: 0.4690 - dense_2_acc_2: 0.2463 - dense_2_acc_3: 0.2160 - dense_2_acc_4: 0.1932 - dense_2_acc_5: 0.2986 - dense_2_acc_6: 0.4484 - val_loss: 33.7377 - val_dense_2_loss_1: 2.9667 - val_dense_2_loss_2: 6.5158 - val_dense_2_loss_3: 6.2891 - val_dense_2_loss_4: 6.8005 - val_dense_2_loss_5: 6.4168 - val_dense_2_loss_6: 4.7488 - val_dense_2_acc_1: 0.4737 - val_dense_2_acc_2: 0.0877 - val_dense_2_acc_3: 0.0702 - val_dense_2_acc_4: 0.1754 - val_dense_2_acc_5: 0.2632 - val_dense_2_acc_6: 0.4211\n",
      "Epoch 41/200\n",
      "2800/2810 [============================>.] - ETA: 0s - loss: 20.6677 - dense_2_loss_1: 2.1869 - dense_2_loss_2: 3.6795 - dense_2_loss_3: 3.8477 - dense_2_loss_4: 4.2058 - dense_2_loss_5: 3.7261 - dense_2_loss_6: 3.0217 - dense_2_acc_1: 0.4893 - dense_2_acc_2: 0.2432 - dense_2_acc_3: 0.2204 - dense_2_acc_4: 0.1871 - dense_2_acc_5: 0.2950 - dense_2_acc_6: 0.4475Epoch 00040: saving model to main_model_weights_new.h5\n",
      "2810/2810 [==============================] - 129s - loss: 20.6699 - dense_2_loss_1: 2.1873 - dense_2_loss_2: 3.6795 - dense_2_loss_3: 3.8496 - dense_2_loss_4: 4.2043 - dense_2_loss_5: 3.7254 - dense_2_loss_6: 3.0237 - dense_2_acc_1: 0.4890 - dense_2_acc_2: 0.2431 - dense_2_acc_3: 0.2199 - dense_2_acc_4: 0.1868 - dense_2_acc_5: 0.2954 - dense_2_acc_6: 0.4470 - val_loss: 33.8406 - val_dense_2_loss_1: 3.1213 - val_dense_2_loss_2: 6.5777 - val_dense_2_loss_3: 6.3455 - val_dense_2_loss_4: 6.9366 - val_dense_2_loss_5: 6.1829 - val_dense_2_loss_6: 4.6766 - val_dense_2_acc_1: 0.4386 - val_dense_2_acc_2: 0.0702 - val_dense_2_acc_3: 0.1228 - val_dense_2_acc_4: 0.1404 - val_dense_2_acc_5: 0.2456 - val_dense_2_acc_6: 0.4211\n",
      "Epoch 42/200\n",
      "2800/2810 [============================>.] - ETA: 0s - loss: 20.4136 - dense_2_loss_1: 2.1689 - dense_2_loss_2: 3.6259 - dense_2_loss_3: 3.7841 - dense_2_loss_4: 4.1453 - dense_2_loss_5: 3.6982 - dense_2_loss_6: 2.9913 - dense_2_acc_1: 0.4768 - dense_2_acc_2: 0.2500 - dense_2_acc_3: 0.2221 - dense_2_acc_4: 0.1921 - dense_2_acc_5: 0.2875 - dense_2_acc_6: 0.4446Epoch 00041: saving model to main_model_weights_new.h5\n",
      "2810/2810 [==============================] - 129s - loss: 20.4279 - dense_2_loss_1: 2.1743 - dense_2_loss_2: 3.6314 - dense_2_loss_3: 3.7849 - dense_2_loss_4: 4.1463 - dense_2_loss_5: 3.6982 - dense_2_loss_6: 2.9929 - dense_2_acc_1: 0.4758 - dense_2_acc_2: 0.2491 - dense_2_acc_3: 0.2221 - dense_2_acc_4: 0.1922 - dense_2_acc_5: 0.2875 - dense_2_acc_6: 0.4445 - val_loss: 33.2644 - val_dense_2_loss_1: 3.0086 - val_dense_2_loss_2: 6.5041 - val_dense_2_loss_3: 6.3779 - val_dense_2_loss_4: 6.6535 - val_dense_2_loss_5: 6.1798 - val_dense_2_loss_6: 4.5404 - val_dense_2_acc_1: 0.4561 - val_dense_2_acc_2: 0.0702 - val_dense_2_acc_3: 0.1404 - val_dense_2_acc_4: 0.1404 - val_dense_2_acc_5: 0.2456 - val_dense_2_acc_6: 0.4211\n",
      "Epoch 43/200\n",
      "2800/2810 [============================>.] - ETA: 0s - loss: 20.2464 - dense_2_loss_1: 2.1571 - dense_2_loss_2: 3.5786 - dense_2_loss_3: 3.7598 - dense_2_loss_4: 4.1037 - dense_2_loss_5: 3.6903 - dense_2_loss_6: 2.9570 - dense_2_acc_1: 0.4882 - dense_2_acc_2: 0.2514 - dense_2_acc_3: 0.2389 - dense_2_acc_4: 0.1971 - dense_2_acc_5: 0.2929 - dense_2_acc_6: 0.4507Epoch 00042: saving model to main_model_weights_new.h5\n",
      "2810/2810 [==============================] - 128s - loss: 20.2478 - dense_2_loss_1: 2.1599 - dense_2_loss_2: 3.5816 - dense_2_loss_3: 3.7581 - dense_2_loss_4: 4.1029 - dense_2_loss_5: 3.6883 - dense_2_loss_6: 2.9570 - dense_2_acc_1: 0.4879 - dense_2_acc_2: 0.2512 - dense_2_acc_3: 0.2391 - dense_2_acc_4: 0.1975 - dense_2_acc_5: 0.2929 - dense_2_acc_6: 0.4505 - val_loss: 34.2936 - val_dense_2_loss_1: 3.0674 - val_dense_2_loss_2: 6.5523 - val_dense_2_loss_3: 6.3709 - val_dense_2_loss_4: 6.8526 - val_dense_2_loss_5: 6.4424 - val_dense_2_loss_6: 5.0081 - val_dense_2_acc_1: 0.4737 - val_dense_2_acc_2: 0.1053 - val_dense_2_acc_3: 0.1404 - val_dense_2_acc_4: 0.1579 - val_dense_2_acc_5: 0.2281 - val_dense_2_acc_6: 0.4035\n",
      "Epoch 44/200\n",
      "2800/2810 [============================>.] - ETA: 0s - loss: 19.9930 - dense_2_loss_1: 2.1214 - dense_2_loss_2: 3.5303 - dense_2_loss_3: 3.7141 - dense_2_loss_4: 4.0467 - dense_2_loss_5: 3.6227 - dense_2_loss_6: 2.9579 - dense_2_acc_1: 0.4893 - dense_2_acc_2: 0.2621 - dense_2_acc_3: 0.2289 - dense_2_acc_4: 0.2014 - dense_2_acc_5: 0.2964 - dense_2_acc_6: 0.4461Epoch 00043: saving model to main_model_weights_new.h5\n",
      "2810/2810 [==============================] - 129s - loss: 19.9852 - dense_2_loss_1: 2.1190 - dense_2_loss_2: 3.5297 - dense_2_loss_3: 3.7113 - dense_2_loss_4: 4.0466 - dense_2_loss_5: 3.6227 - dense_2_loss_6: 2.9559 - dense_2_acc_1: 0.4890 - dense_2_acc_2: 0.2623 - dense_2_acc_3: 0.2292 - dense_2_acc_4: 0.2018 - dense_2_acc_5: 0.2964 - dense_2_acc_6: 0.4463 - val_loss: 34.7230 - val_dense_2_loss_1: 2.9701 - val_dense_2_loss_2: 6.5194 - val_dense_2_loss_3: 6.4305 - val_dense_2_loss_4: 6.8647 - val_dense_2_loss_5: 6.5591 - val_dense_2_loss_6: 5.3792 - val_dense_2_acc_1: 0.4737 - val_dense_2_acc_2: 0.1053 - val_dense_2_acc_3: 0.1404 - val_dense_2_acc_4: 0.1579 - val_dense_2_acc_5: 0.2632 - val_dense_2_acc_6: 0.4211\n",
      "Epoch 45/200\n",
      "2800/2810 [============================>.] - ETA: 0s - loss: 19.8154 - dense_2_loss_1: 2.1389 - dense_2_loss_2: 3.5063 - dense_2_loss_3: 3.6857 - dense_2_loss_4: 4.0150 - dense_2_loss_5: 3.5541 - dense_2_loss_6: 2.9153 - dense_2_acc_1: 0.4889 - dense_2_acc_2: 0.2579 - dense_2_acc_3: 0.2357 - dense_2_acc_4: 0.1932 - dense_2_acc_5: 0.3071 - dense_2_acc_6: 0.4471Epoch 00044: saving model to main_model_weights_new.h5\n",
      "2810/2810 [==============================] - 128s - loss: 19.8163 - dense_2_loss_1: 2.1360 - dense_2_loss_2: 3.5063 - dense_2_loss_3: 3.6873 - dense_2_loss_4: 4.0149 - dense_2_loss_5: 3.5552 - dense_2_loss_6: 2.9166 - dense_2_acc_1: 0.4890 - dense_2_acc_2: 0.2580 - dense_2_acc_3: 0.2359 - dense_2_acc_4: 0.1936 - dense_2_acc_5: 0.3071 - dense_2_acc_6: 0.4466 - val_loss: 34.8693 - val_dense_2_loss_1: 2.9777 - val_dense_2_loss_2: 6.7742 - val_dense_2_loss_3: 6.5414 - val_dense_2_loss_4: 7.0518 - val_dense_2_loss_5: 6.6382 - val_dense_2_loss_6: 4.8859 - val_dense_2_acc_1: 0.4737 - val_dense_2_acc_2: 0.0877 - val_dense_2_acc_3: 0.1228 - val_dense_2_acc_4: 0.1579 - val_dense_2_acc_5: 0.2281 - val_dense_2_acc_6: 0.3860\n",
      "Epoch 46/200\n",
      "2800/2810 [============================>.] - ETA: 0s - loss: 19.6756 - dense_2_loss_1: 2.1103 - dense_2_loss_2: 3.4877 - dense_2_loss_3: 3.6377 - dense_2_loss_4: 3.9757 - dense_2_loss_5: 3.5732 - dense_2_loss_6: 2.8910 - dense_2_acc_1: 0.4954 - dense_2_acc_2: 0.2711 - dense_2_acc_3: 0.2457 - dense_2_acc_4: 0.2043 - dense_2_acc_5: 0.3007 - dense_2_acc_6: 0.4504Epoch 00045: saving model to main_model_weights_new.h5\n",
      "2810/2810 [==============================] - 127s - loss: 19.6773 - dense_2_loss_1: 2.1100 - dense_2_loss_2: 3.4858 - dense_2_loss_3: 3.6421 - dense_2_loss_4: 3.9772 - dense_2_loss_5: 3.5742 - dense_2_loss_6: 2.8881 - dense_2_acc_1: 0.4961 - dense_2_acc_2: 0.2719 - dense_2_acc_3: 0.2456 - dense_2_acc_4: 0.2046 - dense_2_acc_5: 0.3004 - dense_2_acc_6: 0.4505 - val_loss: 33.5080 - val_dense_2_loss_1: 3.0078 - val_dense_2_loss_2: 6.6180 - val_dense_2_loss_3: 6.2254 - val_dense_2_loss_4: 6.7000 - val_dense_2_loss_5: 6.1939 - val_dense_2_loss_6: 4.7630 - val_dense_2_acc_1: 0.4561 - val_dense_2_acc_2: 0.1053 - val_dense_2_acc_3: 0.1053 - val_dense_2_acc_4: 0.2105 - val_dense_2_acc_5: 0.2456 - val_dense_2_acc_6: 0.4035\n",
      "Epoch 47/200\n",
      "2800/2810 [============================>.] - ETA: 0s - loss: 19.3664 - dense_2_loss_1: 2.0569 - dense_2_loss_2: 3.4353 - dense_2_loss_3: 3.5890 - dense_2_loss_4: 3.9274 - dense_2_loss_5: 3.5126 - dense_2_loss_6: 2.8450 - dense_2_acc_1: 0.5050 - dense_2_acc_2: 0.2643 - dense_2_acc_3: 0.2332 - dense_2_acc_4: 0.2121 - dense_2_acc_5: 0.3050 - dense_2_acc_6: 0.4550Epoch 00046: saving model to main_model_weights_new.h5\n",
      "2810/2810 [==============================] - 129s - loss: 19.3677 - dense_2_loss_1: 2.0550 - dense_2_loss_2: 3.4337 - dense_2_loss_3: 3.5890 - dense_2_loss_4: 3.9262 - dense_2_loss_5: 3.5138 - dense_2_loss_6: 2.8499 - dense_2_acc_1: 0.5046 - dense_2_acc_2: 0.2648 - dense_2_acc_3: 0.2338 - dense_2_acc_4: 0.2128 - dense_2_acc_5: 0.3050 - dense_2_acc_6: 0.4544 - val_loss: 35.1393 - val_dense_2_loss_1: 3.0984 - val_dense_2_loss_2: 7.0143 - val_dense_2_loss_3: 6.3914 - val_dense_2_loss_4: 7.1045 - val_dense_2_loss_5: 6.4037 - val_dense_2_loss_6: 5.1270 - val_dense_2_acc_1: 0.4737 - val_dense_2_acc_2: 0.0702 - val_dense_2_acc_3: 0.1053 - val_dense_2_acc_4: 0.1228 - val_dense_2_acc_5: 0.2281 - val_dense_2_acc_6: 0.3860\n",
      "Epoch 48/200\n",
      "2800/2810 [============================>.] - ETA: 0s - loss: 19.1766 - dense_2_loss_1: 2.0511 - dense_2_loss_2: 3.3693 - dense_2_loss_3: 3.5372 - dense_2_loss_4: 3.8691 - dense_2_loss_5: 3.5161 - dense_2_loss_6: 2.8337 - dense_2_acc_1: 0.5082 - dense_2_acc_2: 0.2764 - dense_2_acc_3: 0.2479 - dense_2_acc_4: 0.2254 - dense_2_acc_5: 0.3057 - dense_2_acc_6: 0.4575Epoch 00047: saving model to main_model_weights_new.h5\n",
      "2810/2810 [==============================] - 128s - loss: 19.1750 - dense_2_loss_1: 2.0496 - dense_2_loss_2: 3.3682 - dense_2_loss_3: 3.5372 - dense_2_loss_4: 3.8680 - dense_2_loss_5: 3.5173 - dense_2_loss_6: 2.8347 - dense_2_acc_1: 0.5078 - dense_2_acc_2: 0.2769 - dense_2_acc_3: 0.2484 - dense_2_acc_4: 0.2256 - dense_2_acc_5: 0.3060 - dense_2_acc_6: 0.4580 - val_loss: 34.5923 - val_dense_2_loss_1: 3.0684 - val_dense_2_loss_2: 6.7787 - val_dense_2_loss_3: 6.5022 - val_dense_2_loss_4: 6.9267 - val_dense_2_loss_5: 6.3721 - val_dense_2_loss_6: 4.9441 - val_dense_2_acc_1: 0.4737 - val_dense_2_acc_2: 0.1053 - val_dense_2_acc_3: 0.0702 - val_dense_2_acc_4: 0.1404 - val_dense_2_acc_5: 0.2105 - val_dense_2_acc_6: 0.4211\n",
      "Epoch 49/200\n",
      "2800/2810 [============================>.] - ETA: 0s - loss: 19.0691 - dense_2_loss_1: 2.0498 - dense_2_loss_2: 3.3654 - dense_2_loss_3: 3.5108 - dense_2_loss_4: 3.8715 - dense_2_loss_5: 3.4652 - dense_2_loss_6: 2.8065 - dense_2_acc_1: 0.5050 - dense_2_acc_2: 0.2886 - dense_2_acc_3: 0.2550 - dense_2_acc_4: 0.2196 - dense_2_acc_5: 0.3129 - dense_2_acc_6: 0.4511Epoch 00048: saving model to main_model_weights_new.h5\n",
      "2810/2810 [==============================] - 128s - loss: 19.0654 - dense_2_loss_1: 2.0571 - dense_2_loss_2: 3.3660 - dense_2_loss_3: 3.5094 - dense_2_loss_4: 3.8716 - dense_2_loss_5: 3.4617 - dense_2_loss_6: 2.7997 - dense_2_acc_1: 0.5046 - dense_2_acc_2: 0.2886 - dense_2_acc_3: 0.2548 - dense_2_acc_4: 0.2199 - dense_2_acc_5: 0.3135 - dense_2_acc_6: 0.4523 - val_loss: 34.4912 - val_dense_2_loss_1: 2.9712 - val_dense_2_loss_2: 6.5974 - val_dense_2_loss_3: 6.5305 - val_dense_2_loss_4: 6.8024 - val_dense_2_loss_5: 6.3943 - val_dense_2_loss_6: 5.1955 - val_dense_2_acc_1: 0.4737 - val_dense_2_acc_2: 0.1053 - val_dense_2_acc_3: 0.1053 - val_dense_2_acc_4: 0.1404 - val_dense_2_acc_5: 0.2105 - val_dense_2_acc_6: 0.4211\n",
      "Epoch 50/200\n",
      "2800/2810 [============================>.] - ETA: 0s - loss: 18.8655 - dense_2_loss_1: 1.9904 - dense_2_loss_2: 3.3290 - dense_2_loss_3: 3.4765 - dense_2_loss_4: 3.8190 - dense_2_loss_5: 3.4410 - dense_2_loss_6: 2.8097 - dense_2_acc_1: 0.5236 - dense_2_acc_2: 0.2793 - dense_2_acc_3: 0.2568 - dense_2_acc_4: 0.2182 - dense_2_acc_5: 0.3157 - dense_2_acc_6: 0.4607Epoch 00049: saving model to main_model_weights_new.h5\n",
      "2810/2810 [==============================] - 128s - loss: 18.8818 - dense_2_loss_1: 1.9899 - dense_2_loss_2: 3.3318 - dense_2_loss_3: 3.4788 - dense_2_loss_4: 3.8214 - dense_2_loss_5: 3.4424 - dense_2_loss_6: 2.8174 - dense_2_acc_1: 0.5238 - dense_2_acc_2: 0.2790 - dense_2_acc_3: 0.2566 - dense_2_acc_4: 0.2178 - dense_2_acc_5: 0.3149 - dense_2_acc_6: 0.4598 - val_loss: 33.9995 - val_dense_2_loss_1: 2.9774 - val_dense_2_loss_2: 6.6453 - val_dense_2_loss_3: 6.4483 - val_dense_2_loss_4: 6.8145 - val_dense_2_loss_5: 6.3050 - val_dense_2_loss_6: 4.8090 - val_dense_2_acc_1: 0.4737 - val_dense_2_acc_2: 0.1053 - val_dense_2_acc_3: 0.1228 - val_dense_2_acc_4: 0.1579 - val_dense_2_acc_5: 0.2632 - val_dense_2_acc_6: 0.4211\n",
      "Epoch 51/200\n",
      "2800/2810 [============================>.] - ETA: 0s - loss: 18.8452 - dense_2_loss_1: 2.0076 - dense_2_loss_2: 3.3182 - dense_2_loss_3: 3.4622 - dense_2_loss_4: 3.7995 - dense_2_loss_5: 3.4734 - dense_2_loss_6: 2.7843 - dense_2_acc_1: 0.5118 - dense_2_acc_2: 0.2882 - dense_2_acc_3: 0.2604 - dense_2_acc_4: 0.2268 - dense_2_acc_5: 0.3046 - dense_2_acc_6: 0.4571Epoch 00050: saving model to main_model_weights_new.h5\n",
      "2810/2810 [==============================] - 128s - loss: 18.8536 - dense_2_loss_1: 2.0100 - dense_2_loss_2: 3.3177 - dense_2_loss_3: 3.4619 - dense_2_loss_4: 3.8038 - dense_2_loss_5: 3.4740 - dense_2_loss_6: 2.7862 - dense_2_acc_1: 0.5114 - dense_2_acc_2: 0.2890 - dense_2_acc_3: 0.2598 - dense_2_acc_4: 0.2267 - dense_2_acc_5: 0.3046 - dense_2_acc_6: 0.4569 - val_loss: 34.2034 - val_dense_2_loss_1: 2.9491 - val_dense_2_loss_2: 6.5855 - val_dense_2_loss_3: 6.4746 - val_dense_2_loss_4: 6.7576 - val_dense_2_loss_5: 6.4700 - val_dense_2_loss_6: 4.9667 - val_dense_2_acc_1: 0.4561 - val_dense_2_acc_2: 0.1228 - val_dense_2_acc_3: 0.1404 - val_dense_2_acc_4: 0.1579 - val_dense_2_acc_5: 0.2456 - val_dense_2_acc_6: 0.4211\n",
      "Epoch 52/200\n",
      "2800/2810 [============================>.] - ETA: 0s - loss: 18.5656 - dense_2_loss_1: 1.9984 - dense_2_loss_2: 3.2620 - dense_2_loss_3: 3.4102 - dense_2_loss_4: 3.7434 - dense_2_loss_5: 3.3681 - dense_2_loss_6: 2.7836 - dense_2_acc_1: 0.5207 - dense_2_acc_2: 0.2957 - dense_2_acc_3: 0.2614 - dense_2_acc_4: 0.2293 - dense_2_acc_5: 0.3232 - dense_2_acc_6: 0.4607Epoch 00051: saving model to main_model_weights_new.h5\n",
      "2810/2810 [==============================] - 130s - loss: 18.5681 - dense_2_loss_1: 1.9998 - dense_2_loss_2: 3.2635 - dense_2_loss_3: 3.4084 - dense_2_loss_4: 3.7409 - dense_2_loss_5: 3.3729 - dense_2_loss_6: 2.7826 - dense_2_acc_1: 0.5206 - dense_2_acc_2: 0.2961 - dense_2_acc_3: 0.2616 - dense_2_acc_4: 0.2295 - dense_2_acc_5: 0.3228 - dense_2_acc_6: 0.4609 - val_loss: 34.3155 - val_dense_2_loss_1: 2.9024 - val_dense_2_loss_2: 6.4047 - val_dense_2_loss_3: 6.5197 - val_dense_2_loss_4: 6.9880 - val_dense_2_loss_5: 6.4353 - val_dense_2_loss_6: 5.0655 - val_dense_2_acc_1: 0.4737 - val_dense_2_acc_2: 0.1404 - val_dense_2_acc_3: 0.1053 - val_dense_2_acc_4: 0.1404 - val_dense_2_acc_5: 0.2632 - val_dense_2_acc_6: 0.4211\n",
      "Epoch 53/200\n",
      "2800/2810 [============================>.] - ETA: 0s - loss: 18.5487 - dense_2_loss_1: 2.0029 - dense_2_loss_2: 3.2889 - dense_2_loss_3: 3.4323 - dense_2_loss_4: 3.7431 - dense_2_loss_5: 3.3511 - dense_2_loss_6: 2.7304 - dense_2_acc_1: 0.5218 - dense_2_acc_2: 0.2932 - dense_2_acc_3: 0.2618 - dense_2_acc_4: 0.2346 - dense_2_acc_5: 0.3207 - dense_2_acc_6: 0.4621Epoch 00052: saving model to main_model_weights_new.h5\n",
      "2810/2810 [==============================] - 129s - loss: 18.5465 - dense_2_loss_1: 1.9997 - dense_2_loss_2: 3.2870 - dense_2_loss_3: 3.4345 - dense_2_loss_4: 3.7419 - dense_2_loss_5: 3.3539 - dense_2_loss_6: 2.7295 - dense_2_acc_1: 0.5221 - dense_2_acc_2: 0.2932 - dense_2_acc_3: 0.2612 - dense_2_acc_4: 0.2352 - dense_2_acc_5: 0.3196 - dense_2_acc_6: 0.4626 - val_loss: 34.4058 - val_dense_2_loss_1: 3.0781 - val_dense_2_loss_2: 6.5864 - val_dense_2_loss_3: 6.4127 - val_dense_2_loss_4: 6.9062 - val_dense_2_loss_5: 6.6010 - val_dense_2_loss_6: 4.8214 - val_dense_2_acc_1: 0.4386 - val_dense_2_acc_2: 0.1053 - val_dense_2_acc_3: 0.1228 - val_dense_2_acc_4: 0.1579 - val_dense_2_acc_5: 0.2632 - val_dense_2_acc_6: 0.4035\n",
      "Epoch 54/200\n",
      "2800/2810 [============================>.] - ETA: 0s - loss: 18.2971 - dense_2_loss_1: 1.9787 - dense_2_loss_2: 3.1988 - dense_2_loss_3: 3.3754 - dense_2_loss_4: 3.6969 - dense_2_loss_5: 3.3219 - dense_2_loss_6: 2.7255 - dense_2_acc_1: 0.5193 - dense_2_acc_2: 0.3043 - dense_2_acc_3: 0.2786 - dense_2_acc_4: 0.2354 - dense_2_acc_5: 0.3271 - dense_2_acc_6: 0.4589Epoch 00053: saving model to main_model_weights_new.h5\n",
      "2810/2810 [==============================] - 131s - loss: 18.2989 - dense_2_loss_1: 1.9785 - dense_2_loss_2: 3.2004 - dense_2_loss_3: 3.3785 - dense_2_loss_4: 3.6957 - dense_2_loss_5: 3.3199 - dense_2_loss_6: 2.7259 - dense_2_acc_1: 0.5196 - dense_2_acc_2: 0.3039 - dense_2_acc_3: 0.2779 - dense_2_acc_4: 0.2363 - dense_2_acc_5: 0.3278 - dense_2_acc_6: 0.4587 - val_loss: 34.4507 - val_dense_2_loss_1: 2.9740 - val_dense_2_loss_2: 6.6390 - val_dense_2_loss_3: 6.5155 - val_dense_2_loss_4: 6.7183 - val_dense_2_loss_5: 6.4841 - val_dense_2_loss_6: 5.1197 - val_dense_2_acc_1: 0.4737 - val_dense_2_acc_2: 0.1053 - val_dense_2_acc_3: 0.1404 - val_dense_2_acc_4: 0.1754 - val_dense_2_acc_5: 0.2105 - val_dense_2_acc_6: 0.4211\n",
      "Epoch 55/200\n",
      "2800/2810 [============================>.] - ETA: 0s - loss: 18.1290 - dense_2_loss_1: 1.9319 - dense_2_loss_2: 3.1791 - dense_2_loss_3: 3.3532 - dense_2_loss_4: 3.6785 - dense_2_loss_5: 3.2909 - dense_2_loss_6: 2.6955 - dense_2_acc_1: 0.5289 - dense_2_acc_2: 0.3096 - dense_2_acc_3: 0.2729 - dense_2_acc_4: 0.2296 - dense_2_acc_5: 0.3289 - dense_2_acc_6: 0.4675Epoch 00054: saving model to main_model_weights_new.h5\n",
      "2810/2810 [==============================] - 129s - loss: 18.1225 - dense_2_loss_1: 1.9293 - dense_2_loss_2: 3.1765 - dense_2_loss_3: 3.3552 - dense_2_loss_4: 3.6769 - dense_2_loss_5: 3.2902 - dense_2_loss_6: 2.6945 - dense_2_acc_1: 0.5295 - dense_2_acc_2: 0.3103 - dense_2_acc_3: 0.2730 - dense_2_acc_4: 0.2295 - dense_2_acc_5: 0.3288 - dense_2_acc_6: 0.4680 - val_loss: 34.1924 - val_dense_2_loss_1: 2.9488 - val_dense_2_loss_2: 6.5345 - val_dense_2_loss_3: 6.4667 - val_dense_2_loss_4: 6.8334 - val_dense_2_loss_5: 6.3663 - val_dense_2_loss_6: 5.0427 - val_dense_2_acc_1: 0.4561 - val_dense_2_acc_2: 0.1228 - val_dense_2_acc_3: 0.1404 - val_dense_2_acc_4: 0.1579 - val_dense_2_acc_5: 0.2456 - val_dense_2_acc_6: 0.4035\n",
      "Epoch 56/200\n",
      "2800/2810 [============================>.] - ETA: 0s - loss: 18.0190 - dense_2_loss_1: 1.9041 - dense_2_loss_2: 3.1593 - dense_2_loss_3: 3.3040 - dense_2_loss_4: 3.6928 - dense_2_loss_5: 3.2811 - dense_2_loss_6: 2.6776 - dense_2_acc_1: 0.5361 - dense_2_acc_2: 0.3089 - dense_2_acc_3: 0.2754 - dense_2_acc_4: 0.2293 - dense_2_acc_5: 0.3261 - dense_2_acc_6: 0.4650Epoch 00055: saving model to main_model_weights_new.h5\n",
      "2810/2810 [==============================] - 128s - loss: 17.9877 - dense_2_loss_1: 1.9060 - dense_2_loss_2: 3.1536 - dense_2_loss_3: 3.2976 - dense_2_loss_4: 3.6886 - dense_2_loss_5: 3.2736 - dense_2_loss_6: 2.6683 - dense_2_acc_1: 0.5359 - dense_2_acc_2: 0.3096 - dense_2_acc_3: 0.2762 - dense_2_acc_4: 0.2299 - dense_2_acc_5: 0.3274 - dense_2_acc_6: 0.4669 - val_loss: 36.6450 - val_dense_2_loss_1: 3.2130 - val_dense_2_loss_2: 6.8571 - val_dense_2_loss_3: 6.5933 - val_dense_2_loss_4: 7.0136 - val_dense_2_loss_5: 7.0705 - val_dense_2_loss_6: 5.8975 - val_dense_2_acc_1: 0.4211 - val_dense_2_acc_2: 0.1228 - val_dense_2_acc_3: 0.1053 - val_dense_2_acc_4: 0.1404 - val_dense_2_acc_5: 0.2281 - val_dense_2_acc_6: 0.4211\n",
      "Epoch 57/200\n",
      "2800/2810 [============================>.] - ETA: 0s - loss: 17.7821 - dense_2_loss_1: 1.8922 - dense_2_loss_2: 3.1120 - dense_2_loss_3: 3.2622 - dense_2_loss_4: 3.5697 - dense_2_loss_5: 3.2988 - dense_2_loss_6: 2.6472 - dense_2_acc_1: 0.5325 - dense_2_acc_2: 0.3239 - dense_2_acc_3: 0.2836 - dense_2_acc_4: 0.2482 - dense_2_acc_5: 0.3196 - dense_2_acc_6: 0.4625Epoch 00056: saving model to main_model_weights_new.h5\n",
      "2810/2810 [==============================] - 130s - loss: 17.7870 - dense_2_loss_1: 1.8911 - dense_2_loss_2: 3.1138 - dense_2_loss_3: 3.2614 - dense_2_loss_4: 3.5705 - dense_2_loss_5: 3.3011 - dense_2_loss_6: 2.6490 - dense_2_acc_1: 0.5331 - dense_2_acc_2: 0.3235 - dense_2_acc_3: 0.2840 - dense_2_acc_4: 0.2480 - dense_2_acc_5: 0.3199 - dense_2_acc_6: 0.4623 - val_loss: 34.8859 - val_dense_2_loss_1: 2.9297 - val_dense_2_loss_2: 6.3780 - val_dense_2_loss_3: 6.7492 - val_dense_2_loss_4: 7.0010 - val_dense_2_loss_5: 6.7282 - val_dense_2_loss_6: 5.0997 - val_dense_2_acc_1: 0.5263 - val_dense_2_acc_2: 0.1404 - val_dense_2_acc_3: 0.1053 - val_dense_2_acc_4: 0.1930 - val_dense_2_acc_5: 0.2105 - val_dense_2_acc_6: 0.4211\n",
      "Epoch 58/200\n",
      "2800/2810 [============================>.] - ETA: 0s - loss: 17.6742 - dense_2_loss_1: 1.8675 - dense_2_loss_2: 3.0744 - dense_2_loss_3: 3.2395 - dense_2_loss_4: 3.5780 - dense_2_loss_5: 3.2573 - dense_2_loss_6: 2.6574 - dense_2_acc_1: 0.5379 - dense_2_acc_2: 0.3175 - dense_2_acc_3: 0.2800 - dense_2_acc_4: 0.2493 - dense_2_acc_5: 0.3282 - dense_2_acc_6: 0.4721Epoch 00057: saving model to main_model_weights_new.h5\n",
      "2810/2810 [==============================] - 129s - loss: 17.6783 - dense_2_loss_1: 1.8658 - dense_2_loss_2: 3.0784 - dense_2_loss_3: 3.2369 - dense_2_loss_4: 3.5811 - dense_2_loss_5: 3.2587 - dense_2_loss_6: 2.6573 - dense_2_acc_1: 0.5384 - dense_2_acc_2: 0.3167 - dense_2_acc_3: 0.2804 - dense_2_acc_4: 0.2484 - dense_2_acc_5: 0.3274 - dense_2_acc_6: 0.4715 - val_loss: 34.7194 - val_dense_2_loss_1: 2.9462 - val_dense_2_loss_2: 6.5853 - val_dense_2_loss_3: 6.6173 - val_dense_2_loss_4: 6.9010 - val_dense_2_loss_5: 6.6849 - val_dense_2_loss_6: 4.9847 - val_dense_2_acc_1: 0.5088 - val_dense_2_acc_2: 0.1228 - val_dense_2_acc_3: 0.1404 - val_dense_2_acc_4: 0.2105 - val_dense_2_acc_5: 0.2281 - val_dense_2_acc_6: 0.3684\n",
      "Epoch 59/200\n",
      "2800/2810 [============================>.] - ETA: 0s - loss: 17.4681 - dense_2_loss_1: 1.8379 - dense_2_loss_2: 3.0426 - dense_2_loss_3: 3.1725 - dense_2_loss_4: 3.5274 - dense_2_loss_5: 3.2353 - dense_2_loss_6: 2.6524 - dense_2_acc_1: 0.5368 - dense_2_acc_2: 0.3204 - dense_2_acc_3: 0.2918 - dense_2_acc_4: 0.2500 - dense_2_acc_5: 0.3268 - dense_2_acc_6: 0.4625Epoch 00058: saving model to main_model_weights_new.h5\n",
      "2810/2810 [==============================] - 129s - loss: 17.4785 - dense_2_loss_1: 1.8391 - dense_2_loss_2: 3.0485 - dense_2_loss_3: 3.1754 - dense_2_loss_4: 3.5312 - dense_2_loss_5: 3.2349 - dense_2_loss_6: 2.6492 - dense_2_acc_1: 0.5359 - dense_2_acc_2: 0.3192 - dense_2_acc_3: 0.2911 - dense_2_acc_4: 0.2498 - dense_2_acc_5: 0.3278 - dense_2_acc_6: 0.4633 - val_loss: 35.2881 - val_dense_2_loss_1: 2.8988 - val_dense_2_loss_2: 6.3975 - val_dense_2_loss_3: 6.7660 - val_dense_2_loss_4: 6.7911 - val_dense_2_loss_5: 6.9138 - val_dense_2_loss_6: 5.5208 - val_dense_2_acc_1: 0.4912 - val_dense_2_acc_2: 0.1404 - val_dense_2_acc_3: 0.1579 - val_dense_2_acc_4: 0.1579 - val_dense_2_acc_5: 0.2105 - val_dense_2_acc_6: 0.4035\n",
      "Epoch 60/200\n",
      "2800/2810 [============================>.] - ETA: 0s - loss: 17.2726 - dense_2_loss_1: 1.8418 - dense_2_loss_2: 2.9972 - dense_2_loss_3: 3.1317 - dense_2_loss_4: 3.5118 - dense_2_loss_5: 3.1898 - dense_2_loss_6: 2.6003 - dense_2_acc_1: 0.5521 - dense_2_acc_2: 0.3421 - dense_2_acc_3: 0.2996 - dense_2_acc_4: 0.2539 - dense_2_acc_5: 0.3282 - dense_2_acc_6: 0.4739Epoch 00059: saving model to main_model_weights_new.h5\n",
      "2810/2810 [==============================] - 129s - loss: 17.2760 - dense_2_loss_1: 1.8414 - dense_2_loss_2: 2.9946 - dense_2_loss_3: 3.1351 - dense_2_loss_4: 3.5125 - dense_2_loss_5: 3.1912 - dense_2_loss_6: 2.6011 - dense_2_acc_1: 0.5523 - dense_2_acc_2: 0.3423 - dense_2_acc_3: 0.2996 - dense_2_acc_4: 0.2541 - dense_2_acc_5: 0.3281 - dense_2_acc_6: 0.4744 - val_loss: 34.7930 - val_dense_2_loss_1: 3.0518 - val_dense_2_loss_2: 6.4066 - val_dense_2_loss_3: 6.6533 - val_dense_2_loss_4: 6.7776 - val_dense_2_loss_5: 6.5236 - val_dense_2_loss_6: 5.3800 - val_dense_2_acc_1: 0.4561 - val_dense_2_acc_2: 0.1228 - val_dense_2_acc_3: 0.1228 - val_dense_2_acc_4: 0.1579 - val_dense_2_acc_5: 0.2281 - val_dense_2_acc_6: 0.4035\n",
      "Epoch 61/200\n",
      "2800/2810 [============================>.] - ETA: 0s - loss: 17.1165 - dense_2_loss_1: 1.8040 - dense_2_loss_2: 2.9646 - dense_2_loss_3: 3.1354 - dense_2_loss_4: 3.4540 - dense_2_loss_5: 3.1505 - dense_2_loss_6: 2.6080 - dense_2_acc_1: 0.5536 - dense_2_acc_2: 0.3350 - dense_2_acc_3: 0.3043 - dense_2_acc_4: 0.2596 - dense_2_acc_5: 0.3325 - dense_2_acc_6: 0.4661Epoch 00060: saving model to main_model_weights_new.h5\n",
      "2810/2810 [==============================] - 130s - loss: 17.1045 - dense_2_loss_1: 1.8016 - dense_2_loss_2: 2.9633 - dense_2_loss_3: 3.1305 - dense_2_loss_4: 3.4517 - dense_2_loss_5: 3.1511 - dense_2_loss_6: 2.6063 - dense_2_acc_1: 0.5537 - dense_2_acc_2: 0.3345 - dense_2_acc_3: 0.3046 - dense_2_acc_4: 0.2605 - dense_2_acc_5: 0.3324 - dense_2_acc_6: 0.4665 - val_loss: 34.8573 - val_dense_2_loss_1: 2.9034 - val_dense_2_loss_2: 6.6918 - val_dense_2_loss_3: 6.6966 - val_dense_2_loss_4: 6.8298 - val_dense_2_loss_5: 6.4621 - val_dense_2_loss_6: 5.2738 - val_dense_2_acc_1: 0.5263 - val_dense_2_acc_2: 0.1404 - val_dense_2_acc_3: 0.1228 - val_dense_2_acc_4: 0.1930 - val_dense_2_acc_5: 0.2456 - val_dense_2_acc_6: 0.4035\n",
      "Epoch 62/200\n",
      "2800/2810 [============================>.] - ETA: 0s - loss: 16.8789 - dense_2_loss_1: 1.8138 - dense_2_loss_2: 2.9226 - dense_2_loss_3: 3.0852 - dense_2_loss_4: 3.3968 - dense_2_loss_5: 3.0998 - dense_2_loss_6: 2.5608 - dense_2_acc_1: 0.5539 - dense_2_acc_2: 0.3271 - dense_2_acc_3: 0.2975 - dense_2_acc_4: 0.2686 - dense_2_acc_5: 0.3429 - dense_2_acc_6: 0.4686Epoch 00061: saving model to main_model_weights_new.h5\n",
      "2810/2810 [==============================] - 129s - loss: 16.8798 - dense_2_loss_1: 1.8153 - dense_2_loss_2: 2.9198 - dense_2_loss_3: 3.0851 - dense_2_loss_4: 3.3962 - dense_2_loss_5: 3.1002 - dense_2_loss_6: 2.5633 - dense_2_acc_1: 0.5537 - dense_2_acc_2: 0.3274 - dense_2_acc_3: 0.2979 - dense_2_acc_4: 0.2680 - dense_2_acc_5: 0.3431 - dense_2_acc_6: 0.4683 - val_loss: 34.7410 - val_dense_2_loss_1: 2.8725 - val_dense_2_loss_2: 6.6798 - val_dense_2_loss_3: 6.5639 - val_dense_2_loss_4: 6.6906 - val_dense_2_loss_5: 6.5524 - val_dense_2_loss_6: 5.3818 - val_dense_2_acc_1: 0.5088 - val_dense_2_acc_2: 0.1754 - val_dense_2_acc_3: 0.1404 - val_dense_2_acc_4: 0.1579 - val_dense_2_acc_5: 0.2456 - val_dense_2_acc_6: 0.4035\n",
      "Epoch 63/200\n",
      "2800/2810 [============================>.] - ETA: 0s - loss: 16.7709 - dense_2_loss_1: 1.7961 - dense_2_loss_2: 2.9292 - dense_2_loss_3: 3.0535 - dense_2_loss_4: 3.3683 - dense_2_loss_5: 3.0830 - dense_2_loss_6: 2.5409 - dense_2_acc_1: 0.5621 - dense_2_acc_2: 0.3393 - dense_2_acc_3: 0.3100 - dense_2_acc_4: 0.2629 - dense_2_acc_5: 0.3368 - dense_2_acc_6: 0.4754Epoch 00062: saving model to main_model_weights_new.h5\n",
      "2810/2810 [==============================] - 130s - loss: 16.7781 - dense_2_loss_1: 1.7955 - dense_2_loss_2: 2.9283 - dense_2_loss_3: 3.0559 - dense_2_loss_4: 3.3725 - dense_2_loss_5: 3.0818 - dense_2_loss_6: 2.5440 - dense_2_acc_1: 0.5623 - dense_2_acc_2: 0.3395 - dense_2_acc_3: 0.3100 - dense_2_acc_4: 0.2626 - dense_2_acc_5: 0.3363 - dense_2_acc_6: 0.4744 - val_loss: 35.0383 - val_dense_2_loss_1: 2.8603 - val_dense_2_loss_2: 6.8749 - val_dense_2_loss_3: 6.9220 - val_dense_2_loss_4: 6.8514 - val_dense_2_loss_5: 6.6530 - val_dense_2_loss_6: 4.8766 - val_dense_2_acc_1: 0.5439 - val_dense_2_acc_2: 0.1228 - val_dense_2_acc_3: 0.1404 - val_dense_2_acc_4: 0.1579 - val_dense_2_acc_5: 0.2281 - val_dense_2_acc_6: 0.4035\n",
      "Epoch 64/200\n",
      "2800/2810 [============================>.] - ETA: 0s - loss: 16.6042 - dense_2_loss_1: 1.7874 - dense_2_loss_2: 2.8783 - dense_2_loss_3: 3.0064 - dense_2_loss_4: 3.3093 - dense_2_loss_5: 3.0930 - dense_2_loss_6: 2.5298 - dense_2_acc_1: 0.5539 - dense_2_acc_2: 0.3511 - dense_2_acc_3: 0.3146 - dense_2_acc_4: 0.2704 - dense_2_acc_5: 0.3454 - dense_2_acc_6: 0.4700Epoch 00063: saving model to main_model_weights_new.h5\n",
      "2810/2810 [==============================] - 130s - loss: 16.6031 - dense_2_loss_1: 1.7880 - dense_2_loss_2: 2.8761 - dense_2_loss_3: 3.0066 - dense_2_loss_4: 3.3115 - dense_2_loss_5: 3.0926 - dense_2_loss_6: 2.5284 - dense_2_acc_1: 0.5541 - dense_2_acc_2: 0.3516 - dense_2_acc_3: 0.3149 - dense_2_acc_4: 0.2705 - dense_2_acc_5: 0.3456 - dense_2_acc_6: 0.4701 - val_loss: 36.2740 - val_dense_2_loss_1: 3.1805 - val_dense_2_loss_2: 7.3261 - val_dense_2_loss_3: 6.8550 - val_dense_2_loss_4: 6.9708 - val_dense_2_loss_5: 6.7861 - val_dense_2_loss_6: 5.1555 - val_dense_2_acc_1: 0.4912 - val_dense_2_acc_2: 0.1404 - val_dense_2_acc_3: 0.1754 - val_dense_2_acc_4: 0.1404 - val_dense_2_acc_5: 0.2456 - val_dense_2_acc_6: 0.4211\n",
      "Epoch 65/200\n",
      "2800/2810 [============================>.] - ETA: 0s - loss: 16.3942 - dense_2_loss_1: 1.7456 - dense_2_loss_2: 2.8311 - dense_2_loss_3: 2.9578 - dense_2_loss_4: 3.2757 - dense_2_loss_5: 3.0820 - dense_2_loss_6: 2.5020 - dense_2_acc_1: 0.5714 - dense_2_acc_2: 0.3682 - dense_2_acc_3: 0.3193 - dense_2_acc_4: 0.2704 - dense_2_acc_5: 0.3368 - dense_2_acc_6: 0.4757Epoch 00064: saving model to main_model_weights_new.h5\n",
      "2810/2810 [==============================] - 129s - loss: 16.4038 - dense_2_loss_1: 1.7508 - dense_2_loss_2: 2.8324 - dense_2_loss_3: 2.9593 - dense_2_loss_4: 3.2770 - dense_2_loss_5: 3.0821 - dense_2_loss_6: 2.5022 - dense_2_acc_1: 0.5708 - dense_2_acc_2: 0.3676 - dense_2_acc_3: 0.3189 - dense_2_acc_4: 0.2698 - dense_2_acc_5: 0.3370 - dense_2_acc_6: 0.4758 - val_loss: 34.5619 - val_dense_2_loss_1: 2.9564 - val_dense_2_loss_2: 6.7283 - val_dense_2_loss_3: 6.4084 - val_dense_2_loss_4: 6.8534 - val_dense_2_loss_5: 6.5033 - val_dense_2_loss_6: 5.1120 - val_dense_2_acc_1: 0.5263 - val_dense_2_acc_2: 0.1579 - val_dense_2_acc_3: 0.1754 - val_dense_2_acc_4: 0.1228 - val_dense_2_acc_5: 0.2281 - val_dense_2_acc_6: 0.4035\n",
      "Epoch 66/200\n",
      "2800/2810 [============================>.] - ETA: 0s - loss: 16.1415 - dense_2_loss_1: 1.7197 - dense_2_loss_2: 2.7947 - dense_2_loss_3: 2.9356 - dense_2_loss_4: 3.2337 - dense_2_loss_5: 2.9938 - dense_2_loss_6: 2.4639 - dense_2_acc_1: 0.5771 - dense_2_acc_2: 0.3646 - dense_2_acc_3: 0.3221 - dense_2_acc_4: 0.2807 - dense_2_acc_5: 0.3557 - dense_2_acc_6: 0.4793Epoch 00065: saving model to main_model_weights_new.h5\n",
      "2810/2810 [==============================] - 129s - loss: 16.1530 - dense_2_loss_1: 1.7209 - dense_2_loss_2: 2.7939 - dense_2_loss_3: 2.9370 - dense_2_loss_4: 3.2369 - dense_2_loss_5: 2.9993 - dense_2_loss_6: 2.4651 - dense_2_acc_1: 0.5772 - dense_2_acc_2: 0.3644 - dense_2_acc_3: 0.3217 - dense_2_acc_4: 0.2801 - dense_2_acc_5: 0.3548 - dense_2_acc_6: 0.4790 - val_loss: 34.5264 - val_dense_2_loss_1: 2.8950 - val_dense_2_loss_2: 6.5874 - val_dense_2_loss_3: 6.6621 - val_dense_2_loss_4: 7.0238 - val_dense_2_loss_5: 6.3122 - val_dense_2_loss_6: 5.0459 - val_dense_2_acc_1: 0.4561 - val_dense_2_acc_2: 0.1579 - val_dense_2_acc_3: 0.1404 - val_dense_2_acc_4: 0.1053 - val_dense_2_acc_5: 0.1930 - val_dense_2_acc_6: 0.3509\n",
      "Epoch 67/200\n",
      "2800/2810 [============================>.] - ETA: 0s - loss: 16.0578 - dense_2_loss_1: 1.7174 - dense_2_loss_2: 2.7876 - dense_2_loss_3: 2.9192 - dense_2_loss_4: 3.2257 - dense_2_loss_5: 2.9885 - dense_2_loss_6: 2.4195 - dense_2_acc_1: 0.5664 - dense_2_acc_2: 0.3636 - dense_2_acc_3: 0.3307 - dense_2_acc_4: 0.2911 - dense_2_acc_5: 0.3546 - dense_2_acc_6: 0.4879Epoch 00066: saving model to main_model_weights_new.h5\n",
      "2810/2810 [==============================] - 127s - loss: 16.0658 - dense_2_loss_1: 1.7169 - dense_2_loss_2: 2.7902 - dense_2_loss_3: 2.9190 - dense_2_loss_4: 3.2282 - dense_2_loss_5: 2.9899 - dense_2_loss_6: 2.4216 - dense_2_acc_1: 0.5662 - dense_2_acc_2: 0.3630 - dense_2_acc_3: 0.3310 - dense_2_acc_4: 0.2907 - dense_2_acc_5: 0.3544 - dense_2_acc_6: 0.4872 - val_loss: 36.0420 - val_dense_2_loss_1: 2.9574 - val_dense_2_loss_2: 6.9973 - val_dense_2_loss_3: 6.9915 - val_dense_2_loss_4: 7.3279 - val_dense_2_loss_5: 6.6802 - val_dense_2_loss_6: 5.0876 - val_dense_2_acc_1: 0.5088 - val_dense_2_acc_2: 0.1579 - val_dense_2_acc_3: 0.1754 - val_dense_2_acc_4: 0.1053 - val_dense_2_acc_5: 0.2456 - val_dense_2_acc_6: 0.3860\n",
      "Epoch 68/200\n",
      "2800/2810 [============================>.] - ETA: 0s - loss: 16.0121 - dense_2_loss_1: 1.7304 - dense_2_loss_2: 2.7422 - dense_2_loss_3: 2.9225 - dense_2_loss_4: 3.1882 - dense_2_loss_5: 2.9880 - dense_2_loss_6: 2.4408 - dense_2_acc_1: 0.5657 - dense_2_acc_2: 0.3636 - dense_2_acc_3: 0.3336 - dense_2_acc_4: 0.2854 - dense_2_acc_5: 0.3604 - dense_2_acc_6: 0.4857Epoch 00067: saving model to main_model_weights_new.h5\n",
      "2810/2810 [==============================] - 129s - loss: 16.0066 - dense_2_loss_1: 1.7325 - dense_2_loss_2: 2.7411 - dense_2_loss_3: 2.9218 - dense_2_loss_4: 3.1877 - dense_2_loss_5: 2.9867 - dense_2_loss_6: 2.4368 - dense_2_acc_1: 0.5655 - dense_2_acc_2: 0.3630 - dense_2_acc_3: 0.3335 - dense_2_acc_4: 0.2847 - dense_2_acc_5: 0.3609 - dense_2_acc_6: 0.4861 - val_loss: 35.2260 - val_dense_2_loss_1: 2.8553 - val_dense_2_loss_2: 6.7052 - val_dense_2_loss_3: 6.7034 - val_dense_2_loss_4: 6.9914 - val_dense_2_loss_5: 6.6729 - val_dense_2_loss_6: 5.2977 - val_dense_2_acc_1: 0.5439 - val_dense_2_acc_2: 0.1754 - val_dense_2_acc_3: 0.1579 - val_dense_2_acc_4: 0.1754 - val_dense_2_acc_5: 0.2105 - val_dense_2_acc_6: 0.4211\n",
      "Epoch 69/200\n",
      "2800/2810 [============================>.] - ETA: 0s - loss: 15.6634 - dense_2_loss_1: 1.6838 - dense_2_loss_2: 2.6843 - dense_2_loss_3: 2.8598 - dense_2_loss_4: 3.1206 - dense_2_loss_5: 2.9547 - dense_2_loss_6: 2.3602 - dense_2_acc_1: 0.5775 - dense_2_acc_2: 0.3704 - dense_2_acc_3: 0.3350 - dense_2_acc_4: 0.3000 - dense_2_acc_5: 0.3564 - dense_2_acc_6: 0.4943Epoch 00068: saving model to main_model_weights_new.h5\n",
      "2810/2810 [==============================] - 129s - loss: 15.6594 - dense_2_loss_1: 1.6825 - dense_2_loss_2: 2.6846 - dense_2_loss_3: 2.8598 - dense_2_loss_4: 3.1208 - dense_2_loss_5: 2.9544 - dense_2_loss_6: 2.3574 - dense_2_acc_1: 0.5772 - dense_2_acc_2: 0.3701 - dense_2_acc_3: 0.3349 - dense_2_acc_4: 0.3004 - dense_2_acc_5: 0.3566 - dense_2_acc_6: 0.4950 - val_loss: 35.6012 - val_dense_2_loss_1: 2.8209 - val_dense_2_loss_2: 6.6675 - val_dense_2_loss_3: 6.9007 - val_dense_2_loss_4: 7.0868 - val_dense_2_loss_5: 6.6571 - val_dense_2_loss_6: 5.4682 - val_dense_2_acc_1: 0.4912 - val_dense_2_acc_2: 0.1754 - val_dense_2_acc_3: 0.1754 - val_dense_2_acc_4: 0.1754 - val_dense_2_acc_5: 0.2281 - val_dense_2_acc_6: 0.3860\n",
      "Epoch 70/200\n",
      "2800/2810 [============================>.] - ETA: 0s - loss: 15.6610 - dense_2_loss_1: 1.7058 - dense_2_loss_2: 2.7145 - dense_2_loss_3: 2.8122 - dense_2_loss_4: 3.1126 - dense_2_loss_5: 2.8951 - dense_2_loss_6: 2.4209 - dense_2_acc_1: 0.5686 - dense_2_acc_2: 0.3761 - dense_2_acc_3: 0.3464 - dense_2_acc_4: 0.2946 - dense_2_acc_5: 0.3643 - dense_2_acc_6: 0.4896Epoch 00069: saving model to main_model_weights_new.h5\n",
      "2810/2810 [==============================] - 130s - loss: 15.6577 - dense_2_loss_1: 1.7054 - dense_2_loss_2: 2.7130 - dense_2_loss_3: 2.8118 - dense_2_loss_4: 3.1125 - dense_2_loss_5: 2.8945 - dense_2_loss_6: 2.4204 - dense_2_acc_1: 0.5690 - dense_2_acc_2: 0.3769 - dense_2_acc_3: 0.3466 - dense_2_acc_4: 0.2943 - dense_2_acc_5: 0.3644 - dense_2_acc_6: 0.4893 - val_loss: 35.5220 - val_dense_2_loss_1: 3.0977 - val_dense_2_loss_2: 6.8487 - val_dense_2_loss_3: 7.0683 - val_dense_2_loss_4: 7.1144 - val_dense_2_loss_5: 6.4246 - val_dense_2_loss_6: 4.9685 - val_dense_2_acc_1: 0.4737 - val_dense_2_acc_2: 0.1579 - val_dense_2_acc_3: 0.1404 - val_dense_2_acc_4: 0.1053 - val_dense_2_acc_5: 0.2456 - val_dense_2_acc_6: 0.3684\n",
      "Epoch 71/200\n",
      "2800/2810 [============================>.] - ETA: 0s - loss: 15.4813 - dense_2_loss_1: 1.6766 - dense_2_loss_2: 2.6839 - dense_2_loss_3: 2.8017 - dense_2_loss_4: 3.0727 - dense_2_loss_5: 2.8946 - dense_2_loss_6: 2.3519 - dense_2_acc_1: 0.5804 - dense_2_acc_2: 0.3661 - dense_2_acc_3: 0.3482 - dense_2_acc_4: 0.2982 - dense_2_acc_5: 0.3657 - dense_2_acc_6: 0.4971Epoch 00070: saving model to main_model_weights_new.h5\n",
      "2810/2810 [==============================] - 129s - loss: 15.4906 - dense_2_loss_1: 1.6768 - dense_2_loss_2: 2.6837 - dense_2_loss_3: 2.8029 - dense_2_loss_4: 3.0748 - dense_2_loss_5: 2.8998 - dense_2_loss_6: 2.3526 - dense_2_acc_1: 0.5794 - dense_2_acc_2: 0.3658 - dense_2_acc_3: 0.3480 - dense_2_acc_4: 0.2986 - dense_2_acc_5: 0.3658 - dense_2_acc_6: 0.4975 - val_loss: 36.0684 - val_dense_2_loss_1: 3.2786 - val_dense_2_loss_2: 6.8432 - val_dense_2_loss_3: 7.0169 - val_dense_2_loss_4: 7.1097 - val_dense_2_loss_5: 6.4537 - val_dense_2_loss_6: 5.3663 - val_dense_2_acc_1: 0.4386 - val_dense_2_acc_2: 0.1579 - val_dense_2_acc_3: 0.1579 - val_dense_2_acc_4: 0.1228 - val_dense_2_acc_5: 0.2281 - val_dense_2_acc_6: 0.4211\n",
      "Epoch 72/200\n",
      "2800/2810 [============================>.] - ETA: 0s - loss: 15.1950 - dense_2_loss_1: 1.6424 - dense_2_loss_2: 2.6009 - dense_2_loss_3: 2.7494 - dense_2_loss_4: 3.0166 - dense_2_loss_5: 2.8541 - dense_2_loss_6: 2.3315 - dense_2_acc_1: 0.5875 - dense_2_acc_2: 0.3786 - dense_2_acc_3: 0.3468 - dense_2_acc_4: 0.3146 - dense_2_acc_5: 0.3671 - dense_2_acc_6: 0.5018Epoch 00071: saving model to main_model_weights_new.h5\n",
      "2810/2810 [==============================] - 129s - loss: 15.2017 - dense_2_loss_1: 1.6448 - dense_2_loss_2: 2.6071 - dense_2_loss_3: 2.7500 - dense_2_loss_4: 3.0150 - dense_2_loss_5: 2.8519 - dense_2_loss_6: 2.3329 - dense_2_acc_1: 0.5872 - dense_2_acc_2: 0.3779 - dense_2_acc_3: 0.3470 - dense_2_acc_4: 0.3157 - dense_2_acc_5: 0.3676 - dense_2_acc_6: 0.5011 - val_loss: 36.2254 - val_dense_2_loss_1: 2.9123 - val_dense_2_loss_2: 6.5039 - val_dense_2_loss_3: 6.8617 - val_dense_2_loss_4: 7.0898 - val_dense_2_loss_5: 6.9627 - val_dense_2_loss_6: 5.8950 - val_dense_2_acc_1: 0.4561 - val_dense_2_acc_2: 0.1930 - val_dense_2_acc_3: 0.1228 - val_dense_2_acc_4: 0.1579 - val_dense_2_acc_5: 0.2105 - val_dense_2_acc_6: 0.4035\n",
      "Epoch 73/200\n",
      "2800/2810 [============================>.] - ETA: 0s - loss: 15.0383 - dense_2_loss_1: 1.6271 - dense_2_loss_2: 2.5815 - dense_2_loss_3: 2.6886 - dense_2_loss_4: 3.0094 - dense_2_loss_5: 2.8115 - dense_2_loss_6: 2.3203 - dense_2_acc_1: 0.5893 - dense_2_acc_2: 0.3850 - dense_2_acc_3: 0.3596 - dense_2_acc_4: 0.3061 - dense_2_acc_5: 0.3707 - dense_2_acc_6: 0.5011Epoch 00072: saving model to main_model_weights_new.h5\n",
      "2810/2810 [==============================] - 127s - loss: 15.0314 - dense_2_loss_1: 1.6239 - dense_2_loss_2: 2.5791 - dense_2_loss_3: 2.6885 - dense_2_loss_4: 3.0060 - dense_2_loss_5: 2.8141 - dense_2_loss_6: 2.3197 - dense_2_acc_1: 0.5897 - dense_2_acc_2: 0.3858 - dense_2_acc_3: 0.3601 - dense_2_acc_4: 0.3064 - dense_2_acc_5: 0.3698 - dense_2_acc_6: 0.5007 - val_loss: 35.6479 - val_dense_2_loss_1: 2.8829 - val_dense_2_loss_2: 6.5901 - val_dense_2_loss_3: 7.0547 - val_dense_2_loss_4: 7.1030 - val_dense_2_loss_5: 6.7195 - val_dense_2_loss_6: 5.2977 - val_dense_2_acc_1: 0.4737 - val_dense_2_acc_2: 0.1579 - val_dense_2_acc_3: 0.1404 - val_dense_2_acc_4: 0.1579 - val_dense_2_acc_5: 0.2281 - val_dense_2_acc_6: 0.3860\n",
      "Epoch 74/200\n",
      "2800/2810 [============================>.] - ETA: 0s - loss: 15.1145 - dense_2_loss_1: 1.6635 - dense_2_loss_2: 2.5810 - dense_2_loss_3: 2.7056 - dense_2_loss_4: 3.0024 - dense_2_loss_5: 2.8369 - dense_2_loss_6: 2.3251 - dense_2_acc_1: 0.5771 - dense_2_acc_2: 0.3793 - dense_2_acc_3: 0.3543 - dense_2_acc_4: 0.3054 - dense_2_acc_5: 0.3746 - dense_2_acc_6: 0.4996Epoch 00073: saving model to main_model_weights_new.h5\n",
      "2810/2810 [==============================] - 129s - loss: 15.1264 - dense_2_loss_1: 1.6627 - dense_2_loss_2: 2.5869 - dense_2_loss_3: 2.7081 - dense_2_loss_4: 3.0045 - dense_2_loss_5: 2.8402 - dense_2_loss_6: 2.3241 - dense_2_acc_1: 0.5776 - dense_2_acc_2: 0.3790 - dense_2_acc_3: 0.3544 - dense_2_acc_4: 0.3053 - dense_2_acc_5: 0.3744 - dense_2_acc_6: 0.5000 - val_loss: 35.8715 - val_dense_2_loss_1: 2.8785 - val_dense_2_loss_2: 6.8783 - val_dense_2_loss_3: 6.8917 - val_dense_2_loss_4: 7.2763 - val_dense_2_loss_5: 6.7571 - val_dense_2_loss_6: 5.1896 - val_dense_2_acc_1: 0.4561 - val_dense_2_acc_2: 0.1754 - val_dense_2_acc_3: 0.1228 - val_dense_2_acc_4: 0.1404 - val_dense_2_acc_5: 0.2105 - val_dense_2_acc_6: 0.3860\n",
      "Epoch 75/200\n",
      "2800/2810 [============================>.] - ETA: 0s - loss: 14.7747 - dense_2_loss_1: 1.6231 - dense_2_loss_2: 2.5424 - dense_2_loss_3: 2.6484 - dense_2_loss_4: 2.9645 - dense_2_loss_5: 2.7530 - dense_2_loss_6: 2.2432 - dense_2_acc_1: 0.5850 - dense_2_acc_2: 0.3943 - dense_2_acc_3: 0.3625 - dense_2_acc_4: 0.3118 - dense_2_acc_5: 0.3814 - dense_2_acc_6: 0.5089Epoch 00074: saving model to main_model_weights_new.h5\n",
      "2810/2810 [==============================] - 129s - loss: 14.7665 - dense_2_loss_1: 1.6195 - dense_2_loss_2: 2.5419 - dense_2_loss_3: 2.6478 - dense_2_loss_4: 2.9631 - dense_2_loss_5: 2.7534 - dense_2_loss_6: 2.2409 - dense_2_acc_1: 0.5858 - dense_2_acc_2: 0.3943 - dense_2_acc_3: 0.3626 - dense_2_acc_4: 0.3114 - dense_2_acc_5: 0.3819 - dense_2_acc_6: 0.5096 - val_loss: 37.1452 - val_dense_2_loss_1: 2.9509 - val_dense_2_loss_2: 7.4415 - val_dense_2_loss_3: 7.0038 - val_dense_2_loss_4: 7.3007 - val_dense_2_loss_5: 6.4968 - val_dense_2_loss_6: 5.9515 - val_dense_2_acc_1: 0.4737 - val_dense_2_acc_2: 0.1754 - val_dense_2_acc_3: 0.1228 - val_dense_2_acc_4: 0.1228 - val_dense_2_acc_5: 0.2281 - val_dense_2_acc_6: 0.3860\n",
      "Epoch 76/200\n",
      "2800/2810 [============================>.] - ETA: 0s - loss: 14.7494 - dense_2_loss_1: 1.5968 - dense_2_loss_2: 2.5009 - dense_2_loss_3: 2.6541 - dense_2_loss_4: 2.9703 - dense_2_loss_5: 2.7786 - dense_2_loss_6: 2.2487 - dense_2_acc_1: 0.5889 - dense_2_acc_2: 0.3889 - dense_2_acc_3: 0.3650 - dense_2_acc_4: 0.3146 - dense_2_acc_5: 0.3782 - dense_2_acc_6: 0.5050Epoch 00075: saving model to main_model_weights_new.h5\n",
      "2810/2810 [==============================] - 129s - loss: 14.7478 - dense_2_loss_1: 1.5966 - dense_2_loss_2: 2.5026 - dense_2_loss_3: 2.6522 - dense_2_loss_4: 2.9703 - dense_2_loss_5: 2.7778 - dense_2_loss_6: 2.2481 - dense_2_acc_1: 0.5886 - dense_2_acc_2: 0.3883 - dense_2_acc_3: 0.3651 - dense_2_acc_4: 0.3146 - dense_2_acc_5: 0.3776 - dense_2_acc_6: 0.5046 - val_loss: 35.9355 - val_dense_2_loss_1: 2.8219 - val_dense_2_loss_2: 6.8554 - val_dense_2_loss_3: 6.8611 - val_dense_2_loss_4: 6.8630 - val_dense_2_loss_5: 6.9153 - val_dense_2_loss_6: 5.6188 - val_dense_2_acc_1: 0.5439 - val_dense_2_acc_2: 0.1579 - val_dense_2_acc_3: 0.1754 - val_dense_2_acc_4: 0.1930 - val_dense_2_acc_5: 0.2456 - val_dense_2_acc_6: 0.3860\n",
      "Epoch 77/200\n",
      "2800/2810 [============================>.] - ETA: 0s - loss: 14.6333 - dense_2_loss_1: 1.5656 - dense_2_loss_2: 2.5035 - dense_2_loss_3: 2.6469 - dense_2_loss_4: 2.9180 - dense_2_loss_5: 2.7303 - dense_2_loss_6: 2.2690 - dense_2_acc_1: 0.5946 - dense_2_acc_2: 0.3968 - dense_2_acc_3: 0.3700 - dense_2_acc_4: 0.3193 - dense_2_acc_5: 0.3871 - dense_2_acc_6: 0.5029Epoch 00076: saving model to main_model_weights_new.h5\n",
      "2810/2810 [==============================] - 130s - loss: 14.6235 - dense_2_loss_1: 1.5617 - dense_2_loss_2: 2.5043 - dense_2_loss_3: 2.6428 - dense_2_loss_4: 2.9171 - dense_2_loss_5: 2.7296 - dense_2_loss_6: 2.2680 - dense_2_acc_1: 0.5954 - dense_2_acc_2: 0.3964 - dense_2_acc_3: 0.3712 - dense_2_acc_4: 0.3192 - dense_2_acc_5: 0.3872 - dense_2_acc_6: 0.5021 - val_loss: 35.4058 - val_dense_2_loss_1: 2.9080 - val_dense_2_loss_2: 6.8812 - val_dense_2_loss_3: 6.9852 - val_dense_2_loss_4: 6.9255 - val_dense_2_loss_5: 6.5485 - val_dense_2_loss_6: 5.1574 - val_dense_2_acc_1: 0.5439 - val_dense_2_acc_2: 0.1930 - val_dense_2_acc_3: 0.1579 - val_dense_2_acc_4: 0.1754 - val_dense_2_acc_5: 0.2281 - val_dense_2_acc_6: 0.4211\n",
      "Epoch 78/200\n",
      "2800/2810 [============================>.] - ETA: 0s - loss: 14.4571 - dense_2_loss_1: 1.5507 - dense_2_loss_2: 2.4544 - dense_2_loss_3: 2.6011 - dense_2_loss_4: 2.9138 - dense_2_loss_5: 2.6891 - dense_2_loss_6: 2.2481 - dense_2_acc_1: 0.6050 - dense_2_acc_2: 0.4025 - dense_2_acc_3: 0.3736 - dense_2_acc_4: 0.3139 - dense_2_acc_5: 0.3811 - dense_2_acc_6: 0.5039Epoch 00077: saving model to main_model_weights_new.h5\n",
      "2810/2810 [==============================] - 128s - loss: 14.4638 - dense_2_loss_1: 1.5491 - dense_2_loss_2: 2.4566 - dense_2_loss_3: 2.6031 - dense_2_loss_4: 2.9174 - dense_2_loss_5: 2.6904 - dense_2_loss_6: 2.2472 - dense_2_acc_1: 0.6060 - dense_2_acc_2: 0.4025 - dense_2_acc_3: 0.3730 - dense_2_acc_4: 0.3142 - dense_2_acc_5: 0.3815 - dense_2_acc_6: 0.5043 - val_loss: 34.4948 - val_dense_2_loss_1: 2.8249 - val_dense_2_loss_2: 6.5633 - val_dense_2_loss_3: 6.7306 - val_dense_2_loss_4: 6.9644 - val_dense_2_loss_5: 6.4881 - val_dense_2_loss_6: 4.9234 - val_dense_2_acc_1: 0.5263 - val_dense_2_acc_2: 0.1754 - val_dense_2_acc_3: 0.1579 - val_dense_2_acc_4: 0.1228 - val_dense_2_acc_5: 0.2105 - val_dense_2_acc_6: 0.4211\n",
      "Epoch 79/200\n",
      "2800/2810 [============================>.] - ETA: 0s - loss: 14.4484 - dense_2_loss_1: 1.5731 - dense_2_loss_2: 2.4515 - dense_2_loss_3: 2.6035 - dense_2_loss_4: 2.8888 - dense_2_loss_5: 2.6939 - dense_2_loss_6: 2.2376 - dense_2_acc_1: 0.5936 - dense_2_acc_2: 0.4064 - dense_2_acc_3: 0.3793 - dense_2_acc_4: 0.3279 - dense_2_acc_5: 0.3957 - dense_2_acc_6: 0.5093Epoch 00078: saving model to main_model_weights_new.h5\n",
      "2810/2810 [==============================] - 128s - loss: 14.4359 - dense_2_loss_1: 1.5702 - dense_2_loss_2: 2.4489 - dense_2_loss_3: 2.6012 - dense_2_loss_4: 2.8880 - dense_2_loss_5: 2.6917 - dense_2_loss_6: 2.2360 - dense_2_acc_1: 0.5936 - dense_2_acc_2: 0.4071 - dense_2_acc_3: 0.3797 - dense_2_acc_4: 0.3278 - dense_2_acc_5: 0.3957 - dense_2_acc_6: 0.5093 - val_loss: 34.7879 - val_dense_2_loss_1: 2.9267 - val_dense_2_loss_2: 6.7397 - val_dense_2_loss_3: 6.7264 - val_dense_2_loss_4: 7.2003 - val_dense_2_loss_5: 6.2213 - val_dense_2_loss_6: 4.9734 - val_dense_2_acc_1: 0.4386 - val_dense_2_acc_2: 0.1754 - val_dense_2_acc_3: 0.1579 - val_dense_2_acc_4: 0.1579 - val_dense_2_acc_5: 0.1930 - val_dense_2_acc_6: 0.3684\n",
      "Epoch 80/200\n",
      "2800/2810 [============================>.] - ETA: 0s - loss: 14.1858 - dense_2_loss_1: 1.5086 - dense_2_loss_2: 2.4386 - dense_2_loss_3: 2.5532 - dense_2_loss_4: 2.8357 - dense_2_loss_5: 2.6357 - dense_2_loss_6: 2.2139 - dense_2_acc_1: 0.5975 - dense_2_acc_2: 0.3975 - dense_2_acc_3: 0.3889 - dense_2_acc_4: 0.3339 - dense_2_acc_5: 0.3839 - dense_2_acc_6: 0.5114Epoch 00079: saving model to main_model_weights_new.h5\n",
      "2810/2810 [==============================] - 130s - loss: 14.1938 - dense_2_loss_1: 1.5105 - dense_2_loss_2: 2.4417 - dense_2_loss_3: 2.5524 - dense_2_loss_4: 2.8388 - dense_2_loss_5: 2.6368 - dense_2_loss_6: 2.2136 - dense_2_acc_1: 0.5968 - dense_2_acc_2: 0.3972 - dense_2_acc_3: 0.3890 - dense_2_acc_4: 0.3335 - dense_2_acc_5: 0.3836 - dense_2_acc_6: 0.5107 - val_loss: 34.7680 - val_dense_2_loss_1: 3.0142 - val_dense_2_loss_2: 6.4524 - val_dense_2_loss_3: 6.7761 - val_dense_2_loss_4: 6.9846 - val_dense_2_loss_5: 6.4802 - val_dense_2_loss_6: 5.0604 - val_dense_2_acc_1: 0.4737 - val_dense_2_acc_2: 0.1579 - val_dense_2_acc_3: 0.1754 - val_dense_2_acc_4: 0.1228 - val_dense_2_acc_5: 0.2281 - val_dense_2_acc_6: 0.3509\n",
      "Epoch 81/200\n",
      "2800/2810 [============================>.] - ETA: 0s - loss: 14.2790 - dense_2_loss_1: 1.5355 - dense_2_loss_2: 2.4374 - dense_2_loss_3: 2.5715 - dense_2_loss_4: 2.8781 - dense_2_loss_5: 2.6403 - dense_2_loss_6: 2.2161 - dense_2_acc_1: 0.6043 - dense_2_acc_2: 0.4039 - dense_2_acc_3: 0.3846 - dense_2_acc_4: 0.3279 - dense_2_acc_5: 0.3893 - dense_2_acc_6: 0.5043Epoch 00080: saving model to main_model_weights_new.h5\n",
      "2810/2810 [==============================] - 130s - loss: 14.2727 - dense_2_loss_1: 1.5316 - dense_2_loss_2: 2.4373 - dense_2_loss_3: 2.5724 - dense_2_loss_4: 2.8768 - dense_2_loss_5: 2.6412 - dense_2_loss_6: 2.2132 - dense_2_acc_1: 0.6050 - dense_2_acc_2: 0.4039 - dense_2_acc_3: 0.3843 - dense_2_acc_4: 0.3281 - dense_2_acc_5: 0.3893 - dense_2_acc_6: 0.5046 - val_loss: 35.4383 - val_dense_2_loss_1: 2.9176 - val_dense_2_loss_2: 6.7890 - val_dense_2_loss_3: 6.9318 - val_dense_2_loss_4: 6.9950 - val_dense_2_loss_5: 6.5222 - val_dense_2_loss_6: 5.2828 - val_dense_2_acc_1: 0.4912 - val_dense_2_acc_2: 0.1754 - val_dense_2_acc_3: 0.1404 - val_dense_2_acc_4: 0.1754 - val_dense_2_acc_5: 0.2105 - val_dense_2_acc_6: 0.3684\n",
      "Epoch 82/200\n",
      "2800/2810 [============================>.] - ETA: 0s - loss: 13.8398 - dense_2_loss_1: 1.4915 - dense_2_loss_2: 2.3401 - dense_2_loss_3: 2.4566 - dense_2_loss_4: 2.8067 - dense_2_loss_5: 2.5906 - dense_2_loss_6: 2.1543 - dense_2_acc_1: 0.6079 - dense_2_acc_2: 0.4243 - dense_2_acc_3: 0.3957 - dense_2_acc_4: 0.3346 - dense_2_acc_5: 0.4011 - dense_2_acc_6: 0.5207Epoch 00081: saving model to main_model_weights_new.h5\n",
      "2810/2810 [==============================] - 129s - loss: 13.8617 - dense_2_loss_1: 1.4977 - dense_2_loss_2: 2.3441 - dense_2_loss_3: 2.4621 - dense_2_loss_4: 2.8082 - dense_2_loss_5: 2.5940 - dense_2_loss_6: 2.1555 - dense_2_acc_1: 0.6071 - dense_2_acc_2: 0.4242 - dense_2_acc_3: 0.3947 - dense_2_acc_4: 0.3342 - dense_2_acc_5: 0.4000 - dense_2_acc_6: 0.5203 - val_loss: 35.2429 - val_dense_2_loss_1: 2.9912 - val_dense_2_loss_2: 6.8459 - val_dense_2_loss_3: 6.6550 - val_dense_2_loss_4: 7.1581 - val_dense_2_loss_5: 6.5104 - val_dense_2_loss_6: 5.0823 - val_dense_2_acc_1: 0.4912 - val_dense_2_acc_2: 0.2105 - val_dense_2_acc_3: 0.2105 - val_dense_2_acc_4: 0.1579 - val_dense_2_acc_5: 0.2105 - val_dense_2_acc_6: 0.3684\n",
      "Epoch 83/200\n",
      "2800/2810 [============================>.] - ETA: 0s - loss: 13.6564 - dense_2_loss_1: 1.4653 - dense_2_loss_2: 2.3002 - dense_2_loss_3: 2.4392 - dense_2_loss_4: 2.7490 - dense_2_loss_5: 2.5822 - dense_2_loss_6: 2.1205 - dense_2_acc_1: 0.6143 - dense_2_acc_2: 0.4389 - dense_2_acc_3: 0.4025 - dense_2_acc_4: 0.3461 - dense_2_acc_5: 0.3907 - dense_2_acc_6: 0.5182Epoch 00082: saving model to main_model_weights_new.h5\n",
      "2810/2810 [==============================] - 127s - loss: 13.6517 - dense_2_loss_1: 1.4634 - dense_2_loss_2: 2.2986 - dense_2_loss_3: 2.4397 - dense_2_loss_4: 2.7473 - dense_2_loss_5: 2.5822 - dense_2_loss_6: 2.1205 - dense_2_acc_1: 0.6146 - dense_2_acc_2: 0.4391 - dense_2_acc_3: 0.4025 - dense_2_acc_4: 0.3459 - dense_2_acc_5: 0.3907 - dense_2_acc_6: 0.5185 - val_loss: 35.9464 - val_dense_2_loss_1: 2.8377 - val_dense_2_loss_2: 6.5763 - val_dense_2_loss_3: 6.9190 - val_dense_2_loss_4: 7.3208 - val_dense_2_loss_5: 7.0470 - val_dense_2_loss_6: 5.2456 - val_dense_2_acc_1: 0.4912 - val_dense_2_acc_2: 0.2105 - val_dense_2_acc_3: 0.1754 - val_dense_2_acc_4: 0.1579 - val_dense_2_acc_5: 0.2105 - val_dense_2_acc_6: 0.3860\n",
      "Epoch 84/200\n",
      "2800/2810 [============================>.] - ETA: 0s - loss: 13.8202 - dense_2_loss_1: 1.4988 - dense_2_loss_2: 2.3609 - dense_2_loss_3: 2.5119 - dense_2_loss_4: 2.7393 - dense_2_loss_5: 2.5620 - dense_2_loss_6: 2.1474 - dense_2_acc_1: 0.6050 - dense_2_acc_2: 0.4250 - dense_2_acc_3: 0.3904 - dense_2_acc_4: 0.3536 - dense_2_acc_5: 0.4014 - dense_2_acc_6: 0.5214Epoch 00083: saving model to main_model_weights_new.h5\n",
      "2810/2810 [==============================] - 129s - loss: 13.8224 - dense_2_loss_1: 1.4953 - dense_2_loss_2: 2.3621 - dense_2_loss_3: 2.5125 - dense_2_loss_4: 2.7424 - dense_2_loss_5: 2.5627 - dense_2_loss_6: 2.1475 - dense_2_acc_1: 0.6060 - dense_2_acc_2: 0.4249 - dense_2_acc_3: 0.3904 - dense_2_acc_4: 0.3530 - dense_2_acc_5: 0.4014 - dense_2_acc_6: 0.5214 - val_loss: 36.5791 - val_dense_2_loss_1: 2.8942 - val_dense_2_loss_2: 7.2736 - val_dense_2_loss_3: 6.8765 - val_dense_2_loss_4: 7.3380 - val_dense_2_loss_5: 6.8511 - val_dense_2_loss_6: 5.3458 - val_dense_2_acc_1: 0.5088 - val_dense_2_acc_2: 0.1754 - val_dense_2_acc_3: 0.1228 - val_dense_2_acc_4: 0.1228 - val_dense_2_acc_5: 0.2105 - val_dense_2_acc_6: 0.4035\n",
      "Epoch 85/200\n",
      "2800/2810 [============================>.] - ETA: 0s - loss: 13.5258 - dense_2_loss_1: 1.4608 - dense_2_loss_2: 2.2869 - dense_2_loss_3: 2.4331 - dense_2_loss_4: 2.7100 - dense_2_loss_5: 2.5230 - dense_2_loss_6: 2.1120 - dense_2_acc_1: 0.6139 - dense_2_acc_2: 0.4350 - dense_2_acc_3: 0.3979 - dense_2_acc_4: 0.3568 - dense_2_acc_5: 0.4125 - dense_2_acc_6: 0.5225Epoch 00084: saving model to main_model_weights_new.h5\n",
      "2810/2810 [==============================] - 129s - loss: 13.5246 - dense_2_loss_1: 1.4623 - dense_2_loss_2: 2.2855 - dense_2_loss_3: 2.4329 - dense_2_loss_4: 2.7106 - dense_2_loss_5: 2.5235 - dense_2_loss_6: 2.1099 - dense_2_acc_1: 0.6139 - dense_2_acc_2: 0.4352 - dense_2_acc_3: 0.3979 - dense_2_acc_4: 0.3569 - dense_2_acc_5: 0.4125 - dense_2_acc_6: 0.5228 - val_loss: 36.6650 - val_dense_2_loss_1: 2.9353 - val_dense_2_loss_2: 6.8496 - val_dense_2_loss_3: 6.8212 - val_dense_2_loss_4: 7.2633 - val_dense_2_loss_5: 6.8202 - val_dense_2_loss_6: 5.9753 - val_dense_2_acc_1: 0.4912 - val_dense_2_acc_2: 0.1754 - val_dense_2_acc_3: 0.1579 - val_dense_2_acc_4: 0.1228 - val_dense_2_acc_5: 0.2456 - val_dense_2_acc_6: 0.4211\n",
      "Epoch 86/200\n",
      "2800/2810 [============================>.] - ETA: 0s - loss: 13.4610 - dense_2_loss_1: 1.4472 - dense_2_loss_2: 2.2591 - dense_2_loss_3: 2.4162 - dense_2_loss_4: 2.7098 - dense_2_loss_5: 2.5318 - dense_2_loss_6: 2.0969 - dense_2_acc_1: 0.6207 - dense_2_acc_2: 0.4425 - dense_2_acc_3: 0.4025 - dense_2_acc_4: 0.3564 - dense_2_acc_5: 0.4211 - dense_2_acc_6: 0.5393Epoch 00085: saving model to main_model_weights_new.h5\n",
      "2810/2810 [==============================] - 130s - loss: 13.4624 - dense_2_loss_1: 1.4528 - dense_2_loss_2: 2.2581 - dense_2_loss_3: 2.4184 - dense_2_loss_4: 2.7085 - dense_2_loss_5: 2.5288 - dense_2_loss_6: 2.0958 - dense_2_acc_1: 0.6199 - dense_2_acc_2: 0.4423 - dense_2_acc_3: 0.4021 - dense_2_acc_4: 0.3562 - dense_2_acc_5: 0.4214 - dense_2_acc_6: 0.5395 - val_loss: 35.3835 - val_dense_2_loss_1: 2.8901 - val_dense_2_loss_2: 6.7127 - val_dense_2_loss_3: 6.8682 - val_dense_2_loss_4: 7.0705 - val_dense_2_loss_5: 6.5000 - val_dense_2_loss_6: 5.3419 - val_dense_2_acc_1: 0.4737 - val_dense_2_acc_2: 0.1754 - val_dense_2_acc_3: 0.1754 - val_dense_2_acc_4: 0.0877 - val_dense_2_acc_5: 0.1754 - val_dense_2_acc_6: 0.3158\n",
      "Epoch 87/200\n",
      "2800/2810 [============================>.] - ETA: 0s - loss: 13.3043 - dense_2_loss_1: 1.4302 - dense_2_loss_2: 2.2615 - dense_2_loss_3: 2.3672 - dense_2_loss_4: 2.6503 - dense_2_loss_5: 2.4871 - dense_2_loss_6: 2.1079 - dense_2_acc_1: 0.6193 - dense_2_acc_2: 0.4421 - dense_2_acc_3: 0.4214 - dense_2_acc_4: 0.3621 - dense_2_acc_5: 0.4200 - dense_2_acc_6: 0.5186Epoch 00086: saving model to main_model_weights_new.h5\n",
      "2810/2810 [==============================] - 130s - loss: 13.3269 - dense_2_loss_1: 1.4328 - dense_2_loss_2: 2.2643 - dense_2_loss_3: 2.3716 - dense_2_loss_4: 2.6547 - dense_2_loss_5: 2.4920 - dense_2_loss_6: 2.1116 - dense_2_acc_1: 0.6196 - dense_2_acc_2: 0.4413 - dense_2_acc_3: 0.4206 - dense_2_acc_4: 0.3612 - dense_2_acc_5: 0.4196 - dense_2_acc_6: 0.5181 - val_loss: 36.7355 - val_dense_2_loss_1: 2.8799 - val_dense_2_loss_2: 7.0354 - val_dense_2_loss_3: 7.0361 - val_dense_2_loss_4: 7.6235 - val_dense_2_loss_5: 6.9176 - val_dense_2_loss_6: 5.2431 - val_dense_2_acc_1: 0.5263 - val_dense_2_acc_2: 0.1754 - val_dense_2_acc_3: 0.1930 - val_dense_2_acc_4: 0.1579 - val_dense_2_acc_5: 0.2105 - val_dense_2_acc_6: 0.3860\n",
      "Epoch 88/200\n",
      "2800/2810 [============================>.] - ETA: 0s - loss: 13.2803 - dense_2_loss_1: 1.4237 - dense_2_loss_2: 2.2471 - dense_2_loss_3: 2.3926 - dense_2_loss_4: 2.6598 - dense_2_loss_5: 2.4848 - dense_2_loss_6: 2.0723 - dense_2_acc_1: 0.6129 - dense_2_acc_2: 0.4429 - dense_2_acc_3: 0.4079 - dense_2_acc_4: 0.3614 - dense_2_acc_5: 0.4218 - dense_2_acc_6: 0.5250Epoch 00087: saving model to main_model_weights_new.h5\n",
      "2810/2810 [==============================] - 129s - loss: 13.2799 - dense_2_loss_1: 1.4218 - dense_2_loss_2: 2.2456 - dense_2_loss_3: 2.3958 - dense_2_loss_4: 2.6578 - dense_2_loss_5: 2.4830 - dense_2_loss_6: 2.0760 - dense_2_acc_1: 0.6132 - dense_2_acc_2: 0.4431 - dense_2_acc_3: 0.4068 - dense_2_acc_4: 0.3616 - dense_2_acc_5: 0.4221 - dense_2_acc_6: 0.5246 - val_loss: 36.3435 - val_dense_2_loss_1: 2.8282 - val_dense_2_loss_2: 7.2012 - val_dense_2_loss_3: 6.8216 - val_dense_2_loss_4: 7.2620 - val_dense_2_loss_5: 6.9718 - val_dense_2_loss_6: 5.2586 - val_dense_2_acc_1: 0.5263 - val_dense_2_acc_2: 0.1930 - val_dense_2_acc_3: 0.1930 - val_dense_2_acc_4: 0.1754 - val_dense_2_acc_5: 0.2281 - val_dense_2_acc_6: 0.3860\n",
      "Epoch 89/200\n",
      "2800/2810 [============================>.] - ETA: 0s - loss: 13.2417 - dense_2_loss_1: 1.3673 - dense_2_loss_2: 2.2060 - dense_2_loss_3: 2.3739 - dense_2_loss_4: 2.6522 - dense_2_loss_5: 2.5411 - dense_2_loss_6: 2.1013 - dense_2_acc_1: 0.6275 - dense_2_acc_2: 0.4589 - dense_2_acc_3: 0.4193 - dense_2_acc_4: 0.3596 - dense_2_acc_5: 0.3989 - dense_2_acc_6: 0.5321Epoch 00088: saving model to main_model_weights_new.h5\n",
      "2810/2810 [==============================] - 130s - loss: 13.2547 - dense_2_loss_1: 1.3680 - dense_2_loss_2: 2.2087 - dense_2_loss_3: 2.3738 - dense_2_loss_4: 2.6554 - dense_2_loss_5: 2.5504 - dense_2_loss_6: 2.0984 - dense_2_acc_1: 0.6267 - dense_2_acc_2: 0.4577 - dense_2_acc_3: 0.4196 - dense_2_acc_4: 0.3594 - dense_2_acc_5: 0.3982 - dense_2_acc_6: 0.5327 - val_loss: 35.7605 - val_dense_2_loss_1: 3.0949 - val_dense_2_loss_2: 6.9137 - val_dense_2_loss_3: 6.8969 - val_dense_2_loss_4: 7.0813 - val_dense_2_loss_5: 6.6318 - val_dense_2_loss_6: 5.1417 - val_dense_2_acc_1: 0.5088 - val_dense_2_acc_2: 0.1754 - val_dense_2_acc_3: 0.1579 - val_dense_2_acc_4: 0.1228 - val_dense_2_acc_5: 0.1754 - val_dense_2_acc_6: 0.3509\n",
      "Epoch 90/200\n",
      "2800/2810 [============================>.] - ETA: 0s - loss: 13.2337 - dense_2_loss_1: 1.3834 - dense_2_loss_2: 2.2333 - dense_2_loss_3: 2.3597 - dense_2_loss_4: 2.7058 - dense_2_loss_5: 2.4838 - dense_2_loss_6: 2.0676 - dense_2_acc_1: 0.6371 - dense_2_acc_2: 0.4400 - dense_2_acc_3: 0.4143 - dense_2_acc_4: 0.3579 - dense_2_acc_5: 0.4229 - dense_2_acc_6: 0.5321Epoch 00089: saving model to main_model_weights_new.h5\n",
      "2810/2810 [==============================] - 130s - loss: 13.2419 - dense_2_loss_1: 1.3849 - dense_2_loss_2: 2.2349 - dense_2_loss_3: 2.3598 - dense_2_loss_4: 2.7049 - dense_2_loss_5: 2.4886 - dense_2_loss_6: 2.0688 - dense_2_acc_1: 0.6367 - dense_2_acc_2: 0.4399 - dense_2_acc_3: 0.4142 - dense_2_acc_4: 0.3577 - dense_2_acc_5: 0.4221 - dense_2_acc_6: 0.5320 - val_loss: 35.2130 - val_dense_2_loss_1: 2.7890 - val_dense_2_loss_2: 6.7710 - val_dense_2_loss_3: 6.9282 - val_dense_2_loss_4: 7.0080 - val_dense_2_loss_5: 6.6498 - val_dense_2_loss_6: 5.0671 - val_dense_2_acc_1: 0.5439 - val_dense_2_acc_2: 0.1930 - val_dense_2_acc_3: 0.2456 - val_dense_2_acc_4: 0.1754 - val_dense_2_acc_5: 0.1930 - val_dense_2_acc_6: 0.4035\n",
      "Epoch 91/200\n",
      "2800/2810 [============================>.] - ETA: 0s - loss: 13.2434 - dense_2_loss_1: 1.3960 - dense_2_loss_2: 2.2040 - dense_2_loss_3: 2.4034 - dense_2_loss_4: 2.6623 - dense_2_loss_5: 2.5198 - dense_2_loss_6: 2.0579 - dense_2_acc_1: 0.6286 - dense_2_acc_2: 0.4607 - dense_2_acc_3: 0.4064 - dense_2_acc_4: 0.3611 - dense_2_acc_5: 0.4236 - dense_2_acc_6: 0.5293Epoch 00090: saving model to main_model_weights_new.h5\n",
      "2810/2810 [==============================] - 129s - loss: 13.2600 - dense_2_loss_1: 1.3988 - dense_2_loss_2: 2.2017 - dense_2_loss_3: 2.4046 - dense_2_loss_4: 2.6663 - dense_2_loss_5: 2.5256 - dense_2_loss_6: 2.0631 - dense_2_acc_1: 0.6281 - dense_2_acc_2: 0.4609 - dense_2_acc_3: 0.4071 - dense_2_acc_4: 0.3605 - dense_2_acc_5: 0.4224 - dense_2_acc_6: 0.5281 - val_loss: 37.1258 - val_dense_2_loss_1: 2.9554 - val_dense_2_loss_2: 7.3290 - val_dense_2_loss_3: 7.0308 - val_dense_2_loss_4: 7.2245 - val_dense_2_loss_5: 7.1245 - val_dense_2_loss_6: 5.4616 - val_dense_2_acc_1: 0.5263 - val_dense_2_acc_2: 0.2105 - val_dense_2_acc_3: 0.1579 - val_dense_2_acc_4: 0.1228 - val_dense_2_acc_5: 0.2105 - val_dense_2_acc_6: 0.3860\n",
      "Epoch 92/200\n",
      "2800/2810 [============================>.] - ETA: 0s - loss: 13.0910 - dense_2_loss_1: 1.3938 - dense_2_loss_2: 2.1667 - dense_2_loss_3: 2.3453 - dense_2_loss_4: 2.6398 - dense_2_loss_5: 2.4512 - dense_2_loss_6: 2.0942 - dense_2_acc_1: 0.6214 - dense_2_acc_2: 0.4579 - dense_2_acc_3: 0.4189 - dense_2_acc_4: 0.3586 - dense_2_acc_5: 0.4179 - dense_2_acc_6: 0.5332Epoch 00091: saving model to main_model_weights_new.h5\n",
      "2810/2810 [==============================] - 130s - loss: 13.1142 - dense_2_loss_1: 1.3936 - dense_2_loss_2: 2.1661 - dense_2_loss_3: 2.3518 - dense_2_loss_4: 2.6491 - dense_2_loss_5: 2.4578 - dense_2_loss_6: 2.0957 - dense_2_acc_1: 0.6210 - dense_2_acc_2: 0.4580 - dense_2_acc_3: 0.4189 - dense_2_acc_4: 0.3577 - dense_2_acc_5: 0.4178 - dense_2_acc_6: 0.5327 - val_loss: 36.8826 - val_dense_2_loss_1: 2.8569 - val_dense_2_loss_2: 7.1995 - val_dense_2_loss_3: 7.2434 - val_dense_2_loss_4: 7.1340 - val_dense_2_loss_5: 6.9936 - val_dense_2_loss_6: 5.4553 - val_dense_2_acc_1: 0.5614 - val_dense_2_acc_2: 0.1754 - val_dense_2_acc_3: 0.1754 - val_dense_2_acc_4: 0.1579 - val_dense_2_acc_5: 0.2281 - val_dense_2_acc_6: 0.3860\n",
      "Epoch 93/200\n",
      "2800/2810 [============================>.] - ETA: 0s - loss: 12.7691 - dense_2_loss_1: 1.3378 - dense_2_loss_2: 2.1151 - dense_2_loss_3: 2.3125 - dense_2_loss_4: 2.6054 - dense_2_loss_5: 2.4211 - dense_2_loss_6: 1.9771 - dense_2_acc_1: 0.6332 - dense_2_acc_2: 0.4700 - dense_2_acc_3: 0.4239 - dense_2_acc_4: 0.3743 - dense_2_acc_5: 0.4282 - dense_2_acc_6: 0.5443Epoch 00092: saving model to main_model_weights_new.h5\n",
      "2810/2810 [==============================] - 129s - loss: 12.7736 - dense_2_loss_1: 1.3389 - dense_2_loss_2: 2.1181 - dense_2_loss_3: 2.3093 - dense_2_loss_4: 2.6052 - dense_2_loss_5: 2.4243 - dense_2_loss_6: 1.9778 - dense_2_acc_1: 0.6331 - dense_2_acc_2: 0.4694 - dense_2_acc_3: 0.4246 - dense_2_acc_4: 0.3744 - dense_2_acc_5: 0.4274 - dense_2_acc_6: 0.5438 - val_loss: 35.3429 - val_dense_2_loss_1: 2.7621 - val_dense_2_loss_2: 6.9028 - val_dense_2_loss_3: 6.8825 - val_dense_2_loss_4: 7.0619 - val_dense_2_loss_5: 6.5433 - val_dense_2_loss_6: 5.1902 - val_dense_2_acc_1: 0.5439 - val_dense_2_acc_2: 0.1930 - val_dense_2_acc_3: 0.2105 - val_dense_2_acc_4: 0.1754 - val_dense_2_acc_5: 0.2105 - val_dense_2_acc_6: 0.3509\n",
      "Epoch 94/200\n",
      "2800/2810 [============================>.] - ETA: 0s - loss: 12.6782 - dense_2_loss_1: 1.3405 - dense_2_loss_2: 2.1341 - dense_2_loss_3: 2.2471 - dense_2_loss_4: 2.5739 - dense_2_loss_5: 2.3811 - dense_2_loss_6: 2.0015 - dense_2_acc_1: 0.6318 - dense_2_acc_2: 0.4743 - dense_2_acc_3: 0.4636 - dense_2_acc_4: 0.3664 - dense_2_acc_5: 0.4200 - dense_2_acc_6: 0.5464Epoch 00093: saving model to main_model_weights_new.h5\n",
      "2810/2810 [==============================] - 129s - loss: 12.6915 - dense_2_loss_1: 1.3444 - dense_2_loss_2: 2.1349 - dense_2_loss_3: 2.2519 - dense_2_loss_4: 2.5749 - dense_2_loss_5: 2.3844 - dense_2_loss_6: 2.0010 - dense_2_acc_1: 0.6306 - dense_2_acc_2: 0.4744 - dense_2_acc_3: 0.4630 - dense_2_acc_4: 0.3662 - dense_2_acc_5: 0.4199 - dense_2_acc_6: 0.5463 - val_loss: 35.6772 - val_dense_2_loss_1: 2.9248 - val_dense_2_loss_2: 6.6954 - val_dense_2_loss_3: 6.9722 - val_dense_2_loss_4: 6.9805 - val_dense_2_loss_5: 6.7029 - val_dense_2_loss_6: 5.4014 - val_dense_2_acc_1: 0.5614 - val_dense_2_acc_2: 0.2105 - val_dense_2_acc_3: 0.1930 - val_dense_2_acc_4: 0.1579 - val_dense_2_acc_5: 0.2105 - val_dense_2_acc_6: 0.4035\n",
      "Epoch 95/200\n",
      "2800/2810 [============================>.] - ETA: 0s - loss: 12.7482 - dense_2_loss_1: 1.3258 - dense_2_loss_2: 2.1240 - dense_2_loss_3: 2.2745 - dense_2_loss_4: 2.5865 - dense_2_loss_5: 2.4175 - dense_2_loss_6: 2.0199 - dense_2_acc_1: 0.6382 - dense_2_acc_2: 0.4668 - dense_2_acc_3: 0.4407 - dense_2_acc_4: 0.3696 - dense_2_acc_5: 0.4346 - dense_2_acc_6: 0.5400Epoch 00094: saving model to main_model_weights_new.h5\n",
      "2810/2810 [==============================] - 130s - loss: 12.7650 - dense_2_loss_1: 1.3272 - dense_2_loss_2: 2.1278 - dense_2_loss_3: 2.2770 - dense_2_loss_4: 2.5897 - dense_2_loss_5: 2.4243 - dense_2_loss_6: 2.0190 - dense_2_acc_1: 0.6381 - dense_2_acc_2: 0.4662 - dense_2_acc_3: 0.4402 - dense_2_acc_4: 0.3694 - dense_2_acc_5: 0.4342 - dense_2_acc_6: 0.5399 - val_loss: 35.8895 - val_dense_2_loss_1: 2.8413 - val_dense_2_loss_2: 6.7579 - val_dense_2_loss_3: 6.8126 - val_dense_2_loss_4: 7.1143 - val_dense_2_loss_5: 6.9292 - val_dense_2_loss_6: 5.4342 - val_dense_2_acc_1: 0.4737 - val_dense_2_acc_2: 0.2105 - val_dense_2_acc_3: 0.2105 - val_dense_2_acc_4: 0.1404 - val_dense_2_acc_5: 0.1930 - val_dense_2_acc_6: 0.3158\n",
      "Epoch 96/200\n",
      "2800/2810 [============================>.] - ETA: 0s - loss: 12.5901 - dense_2_loss_1: 1.3235 - dense_2_loss_2: 2.0984 - dense_2_loss_3: 2.2447 - dense_2_loss_4: 2.5601 - dense_2_loss_5: 2.3868 - dense_2_loss_6: 1.9766 - dense_2_acc_1: 0.6432 - dense_2_acc_2: 0.4796 - dense_2_acc_3: 0.4482 - dense_2_acc_4: 0.3764 - dense_2_acc_5: 0.4196 - dense_2_acc_6: 0.5511Epoch 00095: saving model to main_model_weights_new.h5\n",
      "2810/2810 [==============================] - 130s - loss: 12.5964 - dense_2_loss_1: 1.3229 - dense_2_loss_2: 2.1022 - dense_2_loss_3: 2.2496 - dense_2_loss_4: 2.5585 - dense_2_loss_5: 2.3852 - dense_2_loss_6: 1.9779 - dense_2_acc_1: 0.6431 - dense_2_acc_2: 0.4794 - dense_2_acc_3: 0.4477 - dense_2_acc_4: 0.3769 - dense_2_acc_5: 0.4199 - dense_2_acc_6: 0.5509 - val_loss: 37.0808 - val_dense_2_loss_1: 2.9894 - val_dense_2_loss_2: 7.0848 - val_dense_2_loss_3: 6.9529 - val_dense_2_loss_4: 7.5493 - val_dense_2_loss_5: 6.9524 - val_dense_2_loss_6: 5.5520 - val_dense_2_acc_1: 0.5614 - val_dense_2_acc_2: 0.2281 - val_dense_2_acc_3: 0.1579 - val_dense_2_acc_4: 0.1404 - val_dense_2_acc_5: 0.2105 - val_dense_2_acc_6: 0.3509\n",
      "Epoch 97/200\n",
      "2800/2810 [============================>.] - ETA: 0s - loss: 12.6569 - dense_2_loss_1: 1.3110 - dense_2_loss_2: 2.0926 - dense_2_loss_3: 2.2726 - dense_2_loss_4: 2.5409 - dense_2_loss_5: 2.3969 - dense_2_loss_6: 2.0428 - dense_2_acc_1: 0.6525 - dense_2_acc_2: 0.4696 - dense_2_acc_3: 0.4325 - dense_2_acc_4: 0.3861 - dense_2_acc_5: 0.4175 - dense_2_acc_6: 0.5386Epoch 00096: saving model to main_model_weights_new.h5\n",
      "2810/2810 [==============================] - 129s - loss: 12.6704 - dense_2_loss_1: 1.3165 - dense_2_loss_2: 2.0937 - dense_2_loss_3: 2.2725 - dense_2_loss_4: 2.5437 - dense_2_loss_5: 2.3995 - dense_2_loss_6: 2.0444 - dense_2_acc_1: 0.6512 - dense_2_acc_2: 0.4690 - dense_2_acc_3: 0.4320 - dense_2_acc_4: 0.3858 - dense_2_acc_5: 0.4167 - dense_2_acc_6: 0.5384 - val_loss: 37.1683 - val_dense_2_loss_1: 3.0257 - val_dense_2_loss_2: 7.2582 - val_dense_2_loss_3: 7.1442 - val_dense_2_loss_4: 7.2893 - val_dense_2_loss_5: 6.9180 - val_dense_2_loss_6: 5.5330 - val_dense_2_acc_1: 0.5263 - val_dense_2_acc_2: 0.1754 - val_dense_2_acc_3: 0.1930 - val_dense_2_acc_4: 0.1579 - val_dense_2_acc_5: 0.1930 - val_dense_2_acc_6: 0.3684\n",
      "Epoch 98/200\n",
      "2800/2810 [============================>.] - ETA: 0s - loss: 12.3531 - dense_2_loss_1: 1.3002 - dense_2_loss_2: 2.0060 - dense_2_loss_3: 2.1784 - dense_2_loss_4: 2.5037 - dense_2_loss_5: 2.3663 - dense_2_loss_6: 1.9986 - dense_2_acc_1: 0.6443 - dense_2_acc_2: 0.4896 - dense_2_acc_3: 0.4371 - dense_2_acc_4: 0.3871 - dense_2_acc_5: 0.4364 - dense_2_acc_6: 0.5486Epoch 00097: saving model to main_model_weights_new.h5\n",
      "2810/2810 [==============================] - 130s - loss: 12.3606 - dense_2_loss_1: 1.2992 - dense_2_loss_2: 2.0047 - dense_2_loss_3: 2.1831 - dense_2_loss_4: 2.5023 - dense_2_loss_5: 2.3652 - dense_2_loss_6: 2.0061 - dense_2_acc_1: 0.6445 - dense_2_acc_2: 0.4900 - dense_2_acc_3: 0.4367 - dense_2_acc_4: 0.3875 - dense_2_acc_5: 0.4359 - dense_2_acc_6: 0.5480 - val_loss: 37.2125 - val_dense_2_loss_1: 3.1510 - val_dense_2_loss_2: 7.4279 - val_dense_2_loss_3: 7.0703 - val_dense_2_loss_4: 7.4304 - val_dense_2_loss_5: 6.6784 - val_dense_2_loss_6: 5.4544 - val_dense_2_acc_1: 0.5263 - val_dense_2_acc_2: 0.2105 - val_dense_2_acc_3: 0.1579 - val_dense_2_acc_4: 0.1053 - val_dense_2_acc_5: 0.1930 - val_dense_2_acc_6: 0.3158\n",
      "Epoch 99/200\n",
      "2800/2810 [============================>.] - ETA: 0s - loss: 12.4476 - dense_2_loss_1: 1.2855 - dense_2_loss_2: 2.0616 - dense_2_loss_3: 2.2133 - dense_2_loss_4: 2.5339 - dense_2_loss_5: 2.4055 - dense_2_loss_6: 1.9477 - dense_2_acc_1: 0.6446 - dense_2_acc_2: 0.4721 - dense_2_acc_3: 0.4407 - dense_2_acc_4: 0.3821 - dense_2_acc_5: 0.4264 - dense_2_acc_6: 0.5471Epoch 00098: saving model to main_model_weights_new.h5\n",
      "2810/2810 [==============================] - 129s - loss: 12.4586 - dense_2_loss_1: 1.2854 - dense_2_loss_2: 2.0646 - dense_2_loss_3: 2.2140 - dense_2_loss_4: 2.5361 - dense_2_loss_5: 2.4081 - dense_2_loss_6: 1.9503 - dense_2_acc_1: 0.6441 - dense_2_acc_2: 0.4722 - dense_2_acc_3: 0.4413 - dense_2_acc_4: 0.3819 - dense_2_acc_5: 0.4263 - dense_2_acc_6: 0.5459 - val_loss: 35.9493 - val_dense_2_loss_1: 2.9330 - val_dense_2_loss_2: 6.9398 - val_dense_2_loss_3: 6.9384 - val_dense_2_loss_4: 7.1149 - val_dense_2_loss_5: 6.7021 - val_dense_2_loss_6: 5.3210 - val_dense_2_acc_1: 0.5088 - val_dense_2_acc_2: 0.2281 - val_dense_2_acc_3: 0.1404 - val_dense_2_acc_4: 0.1404 - val_dense_2_acc_5: 0.1930 - val_dense_2_acc_6: 0.2807\n",
      "Epoch 100/200\n",
      "2800/2810 [============================>.] - ETA: 0s - loss: 12.3807 - dense_2_loss_1: 1.2705 - dense_2_loss_2: 2.0586 - dense_2_loss_3: 2.2211 - dense_2_loss_4: 2.5002 - dense_2_loss_5: 2.3609 - dense_2_loss_6: 1.9694 - dense_2_acc_1: 0.6446 - dense_2_acc_2: 0.4718 - dense_2_acc_3: 0.4336 - dense_2_acc_4: 0.3882 - dense_2_acc_5: 0.4318 - dense_2_acc_6: 0.5393Epoch 00099: saving model to main_model_weights_new.h5\n",
      "2810/2810 [==============================] - 130s - loss: 12.3792 - dense_2_loss_1: 1.2705 - dense_2_loss_2: 2.0576 - dense_2_loss_3: 2.2220 - dense_2_loss_4: 2.5004 - dense_2_loss_5: 2.3611 - dense_2_loss_6: 1.9676 - dense_2_acc_1: 0.6448 - dense_2_acc_2: 0.4719 - dense_2_acc_3: 0.4327 - dense_2_acc_4: 0.3875 - dense_2_acc_5: 0.4324 - dense_2_acc_6: 0.5395 - val_loss: 37.1304 - val_dense_2_loss_1: 2.7143 - val_dense_2_loss_2: 7.1853 - val_dense_2_loss_3: 7.0316 - val_dense_2_loss_4: 7.1367 - val_dense_2_loss_5: 7.1178 - val_dense_2_loss_6: 5.9446 - val_dense_2_acc_1: 0.5439 - val_dense_2_acc_2: 0.1930 - val_dense_2_acc_3: 0.1930 - val_dense_2_acc_4: 0.1754 - val_dense_2_acc_5: 0.1930 - val_dense_2_acc_6: 0.3860\n",
      "Epoch 101/200\n",
      "2800/2810 [============================>.] - ETA: 0s - loss: 12.3230 - dense_2_loss_1: 1.2828 - dense_2_loss_2: 2.0352 - dense_2_loss_3: 2.1735 - dense_2_loss_4: 2.5118 - dense_2_loss_5: 2.3473 - dense_2_loss_6: 1.9724 - dense_2_acc_1: 0.6429 - dense_2_acc_2: 0.4746 - dense_2_acc_3: 0.4454 - dense_2_acc_4: 0.3861 - dense_2_acc_5: 0.4343 - dense_2_acc_6: 0.5461Epoch 00100: saving model to main_model_weights_new.h5\n",
      "2810/2810 [==============================] - 129s - loss: 12.3124 - dense_2_loss_1: 1.2818 - dense_2_loss_2: 2.0331 - dense_2_loss_3: 2.1739 - dense_2_loss_4: 2.5095 - dense_2_loss_5: 2.3451 - dense_2_loss_6: 1.9690 - dense_2_acc_1: 0.6434 - dense_2_acc_2: 0.4758 - dense_2_acc_3: 0.4456 - dense_2_acc_4: 0.3861 - dense_2_acc_5: 0.4349 - dense_2_acc_6: 0.5466 - val_loss: 36.2136 - val_dense_2_loss_1: 2.7984 - val_dense_2_loss_2: 6.9641 - val_dense_2_loss_3: 7.1803 - val_dense_2_loss_4: 7.0623 - val_dense_2_loss_5: 6.7645 - val_dense_2_loss_6: 5.4440 - val_dense_2_acc_1: 0.5614 - val_dense_2_acc_2: 0.1754 - val_dense_2_acc_3: 0.1579 - val_dense_2_acc_4: 0.1053 - val_dense_2_acc_5: 0.2105 - val_dense_2_acc_6: 0.3509\n",
      "Epoch 102/200\n",
      "2800/2810 [============================>.] - ETA: 0s - loss: 12.1463 - dense_2_loss_1: 1.2782 - dense_2_loss_2: 1.9854 - dense_2_loss_3: 2.1529 - dense_2_loss_4: 2.4836 - dense_2_loss_5: 2.3289 - dense_2_loss_6: 1.9173 - dense_2_acc_1: 0.6579 - dense_2_acc_2: 0.4914 - dense_2_acc_3: 0.4479 - dense_2_acc_4: 0.3918 - dense_2_acc_5: 0.4350 - dense_2_acc_6: 0.5507Epoch 00101: saving model to main_model_weights_new.h5\n",
      "2810/2810 [==============================] - 130s - loss: 12.1344 - dense_2_loss_1: 1.2756 - dense_2_loss_2: 1.9820 - dense_2_loss_3: 2.1509 - dense_2_loss_4: 2.4810 - dense_2_loss_5: 2.3302 - dense_2_loss_6: 1.9146 - dense_2_acc_1: 0.6584 - dense_2_acc_2: 0.4925 - dense_2_acc_3: 0.4477 - dense_2_acc_4: 0.3918 - dense_2_acc_5: 0.4349 - dense_2_acc_6: 0.5512 - val_loss: 36.8895 - val_dense_2_loss_1: 2.8057 - val_dense_2_loss_2: 7.3046 - val_dense_2_loss_3: 7.1758 - val_dense_2_loss_4: 7.0306 - val_dense_2_loss_5: 6.8441 - val_dense_2_loss_6: 5.7287 - val_dense_2_acc_1: 0.5439 - val_dense_2_acc_2: 0.1754 - val_dense_2_acc_3: 0.1579 - val_dense_2_acc_4: 0.1930 - val_dense_2_acc_5: 0.2105 - val_dense_2_acc_6: 0.3333\n",
      "Epoch 103/200\n",
      "2800/2810 [============================>.] - ETA: 0s - loss: 12.1097 - dense_2_loss_1: 1.2349 - dense_2_loss_2: 1.9813 - dense_2_loss_3: 2.1828 - dense_2_loss_4: 2.4737 - dense_2_loss_5: 2.3334 - dense_2_loss_6: 1.9036 - dense_2_acc_1: 0.6593 - dense_2_acc_2: 0.4979 - dense_2_acc_3: 0.4389 - dense_2_acc_4: 0.3857 - dense_2_acc_5: 0.4379 - dense_2_acc_6: 0.5443Epoch 00102: saving model to main_model_weights_new.h5\n",
      "2810/2810 [==============================] - 129s - loss: 12.1026 - dense_2_loss_1: 1.2358 - dense_2_loss_2: 1.9805 - dense_2_loss_3: 2.1819 - dense_2_loss_4: 2.4724 - dense_2_loss_5: 2.3305 - dense_2_loss_6: 1.9014 - dense_2_acc_1: 0.6591 - dense_2_acc_2: 0.4975 - dense_2_acc_3: 0.4395 - dense_2_acc_4: 0.3865 - dense_2_acc_5: 0.4388 - dense_2_acc_6: 0.5452 - val_loss: 37.0251 - val_dense_2_loss_1: 3.0424 - val_dense_2_loss_2: 7.3015 - val_dense_2_loss_3: 7.3397 - val_dense_2_loss_4: 7.1812 - val_dense_2_loss_5: 6.7566 - val_dense_2_loss_6: 5.4036 - val_dense_2_acc_1: 0.5088 - val_dense_2_acc_2: 0.1930 - val_dense_2_acc_3: 0.1930 - val_dense_2_acc_4: 0.1404 - val_dense_2_acc_5: 0.2281 - val_dense_2_acc_6: 0.3509\n",
      "Epoch 104/200\n",
      "2800/2810 [============================>.] - ETA: 0s - loss: 11.9211 - dense_2_loss_1: 1.2172 - dense_2_loss_2: 1.9793 - dense_2_loss_3: 2.1258 - dense_2_loss_4: 2.4160 - dense_2_loss_5: 2.2789 - dense_2_loss_6: 1.9039 - dense_2_acc_1: 0.6586 - dense_2_acc_2: 0.4932 - dense_2_acc_3: 0.4593 - dense_2_acc_4: 0.4000 - dense_2_acc_5: 0.4475 - dense_2_acc_6: 0.5646Epoch 00103: saving model to main_model_weights_new.h5\n",
      "2810/2810 [==============================] - 130s - loss: 11.9220 - dense_2_loss_1: 1.2162 - dense_2_loss_2: 1.9769 - dense_2_loss_3: 2.1248 - dense_2_loss_4: 2.4167 - dense_2_loss_5: 2.2808 - dense_2_loss_6: 1.9066 - dense_2_acc_1: 0.6584 - dense_2_acc_2: 0.4940 - dense_2_acc_3: 0.4594 - dense_2_acc_4: 0.3996 - dense_2_acc_5: 0.4473 - dense_2_acc_6: 0.5637 - val_loss: 38.5984 - val_dense_2_loss_1: 3.3043 - val_dense_2_loss_2: 7.7935 - val_dense_2_loss_3: 7.4660 - val_dense_2_loss_4: 7.4181 - val_dense_2_loss_5: 6.7144 - val_dense_2_loss_6: 5.9020 - val_dense_2_acc_1: 0.5614 - val_dense_2_acc_2: 0.1930 - val_dense_2_acc_3: 0.1404 - val_dense_2_acc_4: 0.1579 - val_dense_2_acc_5: 0.2281 - val_dense_2_acc_6: 0.3684\n",
      "Epoch 105/200\n",
      "2800/2810 [============================>.] - ETA: 0s - loss: 11.8579 - dense_2_loss_1: 1.2019 - dense_2_loss_2: 1.8878 - dense_2_loss_3: 2.1000 - dense_2_loss_4: 2.4517 - dense_2_loss_5: 2.2989 - dense_2_loss_6: 1.9177 - dense_2_acc_1: 0.6718 - dense_2_acc_2: 0.5104 - dense_2_acc_3: 0.4725 - dense_2_acc_4: 0.3950 - dense_2_acc_5: 0.4568 - dense_2_acc_6: 0.5511Epoch 00104: saving model to main_model_weights_new.h5\n",
      "2810/2810 [==============================] - 127s - loss: 11.8684 - dense_2_loss_1: 1.2009 - dense_2_loss_2: 1.8921 - dense_2_loss_3: 2.1028 - dense_2_loss_4: 2.4542 - dense_2_loss_5: 2.2991 - dense_2_loss_6: 1.9193 - dense_2_acc_1: 0.6722 - dense_2_acc_2: 0.5100 - dense_2_acc_3: 0.4722 - dense_2_acc_4: 0.3947 - dense_2_acc_5: 0.4562 - dense_2_acc_6: 0.5505 - val_loss: 36.4239 - val_dense_2_loss_1: 2.9241 - val_dense_2_loss_2: 7.4125 - val_dense_2_loss_3: 6.8824 - val_dense_2_loss_4: 7.1875 - val_dense_2_loss_5: 6.6880 - val_dense_2_loss_6: 5.3294 - val_dense_2_acc_1: 0.5263 - val_dense_2_acc_2: 0.2281 - val_dense_2_acc_3: 0.2105 - val_dense_2_acc_4: 0.1404 - val_dense_2_acc_5: 0.2105 - val_dense_2_acc_6: 0.3684\n",
      "Epoch 106/200\n",
      "2800/2810 [============================>.] - ETA: 0s - loss: 11.8422 - dense_2_loss_1: 1.2139 - dense_2_loss_2: 1.9233 - dense_2_loss_3: 2.0876 - dense_2_loss_4: 2.4346 - dense_2_loss_5: 2.2858 - dense_2_loss_6: 1.8971 - dense_2_acc_1: 0.6632 - dense_2_acc_2: 0.5057 - dense_2_acc_3: 0.4629 - dense_2_acc_4: 0.3925 - dense_2_acc_5: 0.4482 - dense_2_acc_6: 0.5557Epoch 00105: saving model to main_model_weights_new.h5\n",
      "2810/2810 [==============================] - 130s - loss: 11.8447 - dense_2_loss_1: 1.2134 - dense_2_loss_2: 1.9213 - dense_2_loss_3: 2.0896 - dense_2_loss_4: 2.4346 - dense_2_loss_5: 2.2876 - dense_2_loss_6: 1.8984 - dense_2_acc_1: 0.6626 - dense_2_acc_2: 0.5057 - dense_2_acc_3: 0.4623 - dense_2_acc_4: 0.3925 - dense_2_acc_5: 0.4477 - dense_2_acc_6: 0.5548 - val_loss: 36.6828 - val_dense_2_loss_1: 2.8111 - val_dense_2_loss_2: 7.1400 - val_dense_2_loss_3: 7.0278 - val_dense_2_loss_4: 7.3105 - val_dense_2_loss_5: 6.5472 - val_dense_2_loss_6: 5.8463 - val_dense_2_acc_1: 0.5614 - val_dense_2_acc_2: 0.1228 - val_dense_2_acc_3: 0.2105 - val_dense_2_acc_4: 0.1754 - val_dense_2_acc_5: 0.2456 - val_dense_2_acc_6: 0.4035\n",
      "Epoch 107/200\n",
      "2800/2810 [============================>.] - ETA: 0s - loss: 11.7303 - dense_2_loss_1: 1.2026 - dense_2_loss_2: 1.9240 - dense_2_loss_3: 2.0530 - dense_2_loss_4: 2.3897 - dense_2_loss_5: 2.2387 - dense_2_loss_6: 1.9223 - dense_2_acc_1: 0.6568 - dense_2_acc_2: 0.5068 - dense_2_acc_3: 0.4693 - dense_2_acc_4: 0.4175 - dense_2_acc_5: 0.4539 - dense_2_acc_6: 0.5525Epoch 00106: saving model to main_model_weights_new.h5\n",
      "2810/2810 [==============================] - 129s - loss: 11.7235 - dense_2_loss_1: 1.2001 - dense_2_loss_2: 1.9197 - dense_2_loss_3: 2.0504 - dense_2_loss_4: 2.3916 - dense_2_loss_5: 2.2380 - dense_2_loss_6: 1.9237 - dense_2_acc_1: 0.6577 - dense_2_acc_2: 0.5082 - dense_2_acc_3: 0.4698 - dense_2_acc_4: 0.4167 - dense_2_acc_5: 0.4541 - dense_2_acc_6: 0.5520 - val_loss: 38.6044 - val_dense_2_loss_1: 2.9351 - val_dense_2_loss_2: 7.3261 - val_dense_2_loss_3: 7.3389 - val_dense_2_loss_4: 7.4857 - val_dense_2_loss_5: 7.4702 - val_dense_2_loss_6: 6.0484 - val_dense_2_acc_1: 0.5263 - val_dense_2_acc_2: 0.1404 - val_dense_2_acc_3: 0.1404 - val_dense_2_acc_4: 0.1404 - val_dense_2_acc_5: 0.2105 - val_dense_2_acc_6: 0.4561\n",
      "Epoch 108/200\n",
      "2800/2810 [============================>.] - ETA: 0s - loss: 11.6475 - dense_2_loss_1: 1.1851 - dense_2_loss_2: 1.8886 - dense_2_loss_3: 2.0310 - dense_2_loss_4: 2.3834 - dense_2_loss_5: 2.2733 - dense_2_loss_6: 1.8862 - dense_2_acc_1: 0.6789 - dense_2_acc_2: 0.5096 - dense_2_acc_3: 0.4618 - dense_2_acc_4: 0.4150 - dense_2_acc_5: 0.4443 - dense_2_acc_6: 0.5464Epoch 00107: saving model to main_model_weights_new.h5\n",
      "2810/2810 [==============================] - 129s - loss: 11.6446 - dense_2_loss_1: 1.1859 - dense_2_loss_2: 1.8911 - dense_2_loss_3: 2.0322 - dense_2_loss_4: 2.3823 - dense_2_loss_5: 2.2710 - dense_2_loss_6: 1.8821 - dense_2_acc_1: 0.6779 - dense_2_acc_2: 0.5093 - dense_2_acc_3: 0.4616 - dense_2_acc_4: 0.4149 - dense_2_acc_5: 0.4448 - dense_2_acc_6: 0.5473 - val_loss: 38.2491 - val_dense_2_loss_1: 2.9807 - val_dense_2_loss_2: 7.3595 - val_dense_2_loss_3: 7.2228 - val_dense_2_loss_4: 7.2148 - val_dense_2_loss_5: 7.2671 - val_dense_2_loss_6: 6.2043 - val_dense_2_acc_1: 0.5614 - val_dense_2_acc_2: 0.1754 - val_dense_2_acc_3: 0.2105 - val_dense_2_acc_4: 0.1579 - val_dense_2_acc_5: 0.2456 - val_dense_2_acc_6: 0.4211\n",
      "Epoch 109/200\n",
      "2800/2810 [============================>.] - ETA: 0s - loss: 11.6609 - dense_2_loss_1: 1.1765 - dense_2_loss_2: 1.9234 - dense_2_loss_3: 2.0857 - dense_2_loss_4: 2.3453 - dense_2_loss_5: 2.2388 - dense_2_loss_6: 1.8912 - dense_2_acc_1: 0.6804 - dense_2_acc_2: 0.5107 - dense_2_acc_3: 0.4668 - dense_2_acc_4: 0.4182 - dense_2_acc_5: 0.4554 - dense_2_acc_6: 0.5564Epoch 00108: saving model to main_model_weights_new.h5\n",
      "2810/2810 [==============================] - 129s - loss: 11.6820 - dense_2_loss_1: 1.1826 - dense_2_loss_2: 1.9245 - dense_2_loss_3: 2.0883 - dense_2_loss_4: 2.3496 - dense_2_loss_5: 2.2420 - dense_2_loss_6: 1.8950 - dense_2_acc_1: 0.6790 - dense_2_acc_2: 0.5107 - dense_2_acc_3: 0.4662 - dense_2_acc_4: 0.4174 - dense_2_acc_5: 0.4544 - dense_2_acc_6: 0.5559 - val_loss: 35.8849 - val_dense_2_loss_1: 2.8185 - val_dense_2_loss_2: 6.7824 - val_dense_2_loss_3: 7.0536 - val_dense_2_loss_4: 7.2519 - val_dense_2_loss_5: 6.7053 - val_dense_2_loss_6: 5.2732 - val_dense_2_acc_1: 0.5439 - val_dense_2_acc_2: 0.2105 - val_dense_2_acc_3: 0.2632 - val_dense_2_acc_4: 0.1053 - val_dense_2_acc_5: 0.2456 - val_dense_2_acc_6: 0.4035\n",
      "Epoch 110/200\n",
      "2800/2810 [============================>.] - ETA: 0s - loss: 11.6017 - dense_2_loss_1: 1.1657 - dense_2_loss_2: 1.8347 - dense_2_loss_3: 2.0632 - dense_2_loss_4: 2.3783 - dense_2_loss_5: 2.2533 - dense_2_loss_6: 1.9066 - dense_2_acc_1: 0.6721 - dense_2_acc_2: 0.5311 - dense_2_acc_3: 0.4693 - dense_2_acc_4: 0.4225 - dense_2_acc_5: 0.4521 - dense_2_acc_6: 0.5514Epoch 00109: saving model to main_model_weights_new.h5\n",
      "2810/2810 [==============================] - 130s - loss: 11.6115 - dense_2_loss_1: 1.1648 - dense_2_loss_2: 1.8358 - dense_2_loss_3: 2.0657 - dense_2_loss_4: 2.3813 - dense_2_loss_5: 2.2563 - dense_2_loss_6: 1.9075 - dense_2_acc_1: 0.6722 - dense_2_acc_2: 0.5306 - dense_2_acc_3: 0.4690 - dense_2_acc_4: 0.4228 - dense_2_acc_5: 0.4520 - dense_2_acc_6: 0.5509 - val_loss: 40.4717 - val_dense_2_loss_1: 3.0490 - val_dense_2_loss_2: 7.7324 - val_dense_2_loss_3: 7.5394 - val_dense_2_loss_4: 7.8225 - val_dense_2_loss_5: 7.5905 - val_dense_2_loss_6: 6.7378 - val_dense_2_acc_1: 0.5439 - val_dense_2_acc_2: 0.2105 - val_dense_2_acc_3: 0.2105 - val_dense_2_acc_4: 0.1579 - val_dense_2_acc_5: 0.2105 - val_dense_2_acc_6: 0.3860\n",
      "Epoch 111/200\n",
      "2800/2810 [============================>.] - ETA: 0s - loss: 11.3636 - dense_2_loss_1: 1.1574 - dense_2_loss_2: 1.8441 - dense_2_loss_3: 1.9956 - dense_2_loss_4: 2.3103 - dense_2_loss_5: 2.2312 - dense_2_loss_6: 1.8250 - dense_2_acc_1: 0.6646 - dense_2_acc_2: 0.5168 - dense_2_acc_3: 0.4846 - dense_2_acc_4: 0.4229 - dense_2_acc_5: 0.4554 - dense_2_acc_6: 0.5657Epoch 00110: saving model to main_model_weights_new.h5\n",
      "2810/2810 [==============================] - 129s - loss: 11.3750 - dense_2_loss_1: 1.1592 - dense_2_loss_2: 1.8461 - dense_2_loss_3: 1.9990 - dense_2_loss_4: 2.3108 - dense_2_loss_5: 2.2343 - dense_2_loss_6: 1.8256 - dense_2_acc_1: 0.6644 - dense_2_acc_2: 0.5167 - dense_2_acc_3: 0.4843 - dense_2_acc_4: 0.4231 - dense_2_acc_5: 0.4552 - dense_2_acc_6: 0.5651 - val_loss: 37.4849 - val_dense_2_loss_1: 3.0546 - val_dense_2_loss_2: 7.2349 - val_dense_2_loss_3: 7.0037 - val_dense_2_loss_4: 7.5611 - val_dense_2_loss_5: 6.9878 - val_dense_2_loss_6: 5.6428 - val_dense_2_acc_1: 0.5263 - val_dense_2_acc_2: 0.1930 - val_dense_2_acc_3: 0.1754 - val_dense_2_acc_4: 0.1754 - val_dense_2_acc_5: 0.1930 - val_dense_2_acc_6: 0.3333\n",
      "Epoch 112/200\n",
      "2800/2810 [============================>.] - ETA: 0s - loss: 11.4433 - dense_2_loss_1: 1.1713 - dense_2_loss_2: 1.8059 - dense_2_loss_3: 2.0493 - dense_2_loss_4: 2.3628 - dense_2_loss_5: 2.2067 - dense_2_loss_6: 1.8473 - dense_2_acc_1: 0.6836 - dense_2_acc_2: 0.5343 - dense_2_acc_3: 0.4786 - dense_2_acc_4: 0.4193 - dense_2_acc_5: 0.4564 - dense_2_acc_6: 0.5639Epoch 00111: saving model to main_model_weights_new.h5\n",
      "2810/2810 [==============================] - 130s - loss: 11.4361 - dense_2_loss_1: 1.1747 - dense_2_loss_2: 1.8025 - dense_2_loss_3: 2.0496 - dense_2_loss_4: 2.3588 - dense_2_loss_5: 2.2041 - dense_2_loss_6: 1.8465 - dense_2_acc_1: 0.6829 - dense_2_acc_2: 0.5345 - dense_2_acc_3: 0.4783 - dense_2_acc_4: 0.4203 - dense_2_acc_5: 0.4573 - dense_2_acc_6: 0.5637 - val_loss: 35.7753 - val_dense_2_loss_1: 2.8287 - val_dense_2_loss_2: 6.6839 - val_dense_2_loss_3: 7.0919 - val_dense_2_loss_4: 7.0372 - val_dense_2_loss_5: 6.6784 - val_dense_2_loss_6: 5.4552 - val_dense_2_acc_1: 0.5439 - val_dense_2_acc_2: 0.1754 - val_dense_2_acc_3: 0.1930 - val_dense_2_acc_4: 0.1930 - val_dense_2_acc_5: 0.2105 - val_dense_2_acc_6: 0.3333\n",
      "Epoch 113/200\n",
      "2800/2810 [============================>.] - ETA: 0s - loss: 11.2446 - dense_2_loss_1: 1.1352 - dense_2_loss_2: 1.7958 - dense_2_loss_3: 1.9731 - dense_2_loss_4: 2.3106 - dense_2_loss_5: 2.2089 - dense_2_loss_6: 1.8211 - dense_2_acc_1: 0.6875 - dense_2_acc_2: 0.5239 - dense_2_acc_3: 0.4825 - dense_2_acc_4: 0.4300 - dense_2_acc_5: 0.4693 - dense_2_acc_6: 0.5689Epoch 00112: saving model to main_model_weights_new.h5\n",
      "2810/2810 [==============================] - 129s - loss: 11.2550 - dense_2_loss_1: 1.1337 - dense_2_loss_2: 1.7990 - dense_2_loss_3: 1.9755 - dense_2_loss_4: 2.3129 - dense_2_loss_5: 2.2084 - dense_2_loss_6: 1.8254 - dense_2_acc_1: 0.6883 - dense_2_acc_2: 0.5228 - dense_2_acc_3: 0.4826 - dense_2_acc_4: 0.4295 - dense_2_acc_5: 0.4687 - dense_2_acc_6: 0.5687 - val_loss: 37.9593 - val_dense_2_loss_1: 3.0120 - val_dense_2_loss_2: 7.3469 - val_dense_2_loss_3: 7.3777 - val_dense_2_loss_4: 7.3031 - val_dense_2_loss_5: 6.9732 - val_dense_2_loss_6: 5.9464 - val_dense_2_acc_1: 0.5088 - val_dense_2_acc_2: 0.2105 - val_dense_2_acc_3: 0.1579 - val_dense_2_acc_4: 0.1930 - val_dense_2_acc_5: 0.2105 - val_dense_2_acc_6: 0.3509\n",
      "Epoch 114/200\n",
      "2800/2810 [============================>.] - ETA: 0s - loss: 11.1410 - dense_2_loss_1: 1.1506 - dense_2_loss_2: 1.7735 - dense_2_loss_3: 1.9671 - dense_2_loss_4: 2.2667 - dense_2_loss_5: 2.1254 - dense_2_loss_6: 1.8576 - dense_2_acc_1: 0.6789 - dense_2_acc_2: 0.5382 - dense_2_acc_3: 0.4843 - dense_2_acc_4: 0.4407 - dense_2_acc_5: 0.4682 - dense_2_acc_6: 0.5679Epoch 00113: saving model to main_model_weights_new.h5\n",
      "2810/2810 [==============================] - 129s - loss: 11.1413 - dense_2_loss_1: 1.1498 - dense_2_loss_2: 1.7736 - dense_2_loss_3: 1.9674 - dense_2_loss_4: 2.2639 - dense_2_loss_5: 2.1280 - dense_2_loss_6: 1.8586 - dense_2_acc_1: 0.6794 - dense_2_acc_2: 0.5377 - dense_2_acc_3: 0.4843 - dense_2_acc_4: 0.4409 - dense_2_acc_5: 0.4687 - dense_2_acc_6: 0.5687 - val_loss: 36.9338 - val_dense_2_loss_1: 2.8387 - val_dense_2_loss_2: 7.1337 - val_dense_2_loss_3: 7.3384 - val_dense_2_loss_4: 7.2812 - val_dense_2_loss_5: 6.4809 - val_dense_2_loss_6: 5.8610 - val_dense_2_acc_1: 0.5614 - val_dense_2_acc_2: 0.2105 - val_dense_2_acc_3: 0.1579 - val_dense_2_acc_4: 0.1579 - val_dense_2_acc_5: 0.2456 - val_dense_2_acc_6: 0.3158\n",
      "Epoch 115/200\n",
      "2800/2810 [============================>.] - ETA: 0s - loss: 11.0254 - dense_2_loss_1: 1.1083 - dense_2_loss_2: 1.7906 - dense_2_loss_3: 1.8889 - dense_2_loss_4: 2.2691 - dense_2_loss_5: 2.1760 - dense_2_loss_6: 1.7925 - dense_2_acc_1: 0.6925 - dense_2_acc_2: 0.5254 - dense_2_acc_3: 0.5075 - dense_2_acc_4: 0.4189 - dense_2_acc_5: 0.4664 - dense_2_acc_6: 0.5729Epoch 00114: saving model to main_model_weights_new.h5\n",
      "2810/2810 [==============================] - 129s - loss: 11.0322 - dense_2_loss_1: 1.1090 - dense_2_loss_2: 1.7928 - dense_2_loss_3: 1.8883 - dense_2_loss_4: 2.2666 - dense_2_loss_5: 2.1780 - dense_2_loss_6: 1.7974 - dense_2_acc_1: 0.6922 - dense_2_acc_2: 0.5242 - dense_2_acc_3: 0.5071 - dense_2_acc_4: 0.4196 - dense_2_acc_5: 0.4662 - dense_2_acc_6: 0.5719 - val_loss: 36.7278 - val_dense_2_loss_1: 3.0726 - val_dense_2_loss_2: 6.8579 - val_dense_2_loss_3: 7.0062 - val_dense_2_loss_4: 7.6584 - val_dense_2_loss_5: 6.6865 - val_dense_2_loss_6: 5.4462 - val_dense_2_acc_1: 0.5088 - val_dense_2_acc_2: 0.1754 - val_dense_2_acc_3: 0.1404 - val_dense_2_acc_4: 0.0877 - val_dense_2_acc_5: 0.1930 - val_dense_2_acc_6: 0.3333\n",
      "Epoch 116/200\n",
      "2800/2810 [============================>.] - ETA: 0s - loss: 10.8159 - dense_2_loss_1: 1.0887 - dense_2_loss_2: 1.6971 - dense_2_loss_3: 1.9286 - dense_2_loss_4: 2.1680 - dense_2_loss_5: 2.1672 - dense_2_loss_6: 1.7663 - dense_2_acc_1: 0.6914 - dense_2_acc_2: 0.5529 - dense_2_acc_3: 0.4993 - dense_2_acc_4: 0.4418 - dense_2_acc_5: 0.4721 - dense_2_acc_6: 0.5739Epoch 00115: saving model to main_model_weights_new.h5\n",
      "2810/2810 [==============================] - 129s - loss: 10.8151 - dense_2_loss_1: 1.0862 - dense_2_loss_2: 1.6976 - dense_2_loss_3: 1.9253 - dense_2_loss_4: 2.1707 - dense_2_loss_5: 2.1683 - dense_2_loss_6: 1.7670 - dense_2_acc_1: 0.6922 - dense_2_acc_2: 0.5530 - dense_2_acc_3: 0.5000 - dense_2_acc_4: 0.4413 - dense_2_acc_5: 0.4719 - dense_2_acc_6: 0.5740 - val_loss: 37.4697 - val_dense_2_loss_1: 2.9420 - val_dense_2_loss_2: 7.1475 - val_dense_2_loss_3: 7.2097 - val_dense_2_loss_4: 7.3175 - val_dense_2_loss_5: 6.9750 - val_dense_2_loss_6: 5.8781 - val_dense_2_acc_1: 0.5263 - val_dense_2_acc_2: 0.1754 - val_dense_2_acc_3: 0.2105 - val_dense_2_acc_4: 0.1579 - val_dense_2_acc_5: 0.2105 - val_dense_2_acc_6: 0.4035\n",
      "Epoch 117/200\n",
      "2800/2810 [============================>.] - ETA: 0s - loss: 10.7482 - dense_2_loss_1: 1.0791 - dense_2_loss_2: 1.7242 - dense_2_loss_3: 1.8492 - dense_2_loss_4: 2.1895 - dense_2_loss_5: 2.1298 - dense_2_loss_6: 1.7766 - dense_2_acc_1: 0.6961 - dense_2_acc_2: 0.5332 - dense_2_acc_3: 0.5129 - dense_2_acc_4: 0.4354 - dense_2_acc_5: 0.4761 - dense_2_acc_6: 0.5704Epoch 00116: saving model to main_model_weights_new.h5\n",
      "2810/2810 [==============================] - 129s - loss: 10.7498 - dense_2_loss_1: 1.0812 - dense_2_loss_2: 1.7215 - dense_2_loss_3: 1.8498 - dense_2_loss_4: 2.1896 - dense_2_loss_5: 2.1309 - dense_2_loss_6: 1.7769 - dense_2_acc_1: 0.6957 - dense_2_acc_2: 0.5338 - dense_2_acc_3: 0.5132 - dense_2_acc_4: 0.4349 - dense_2_acc_5: 0.4758 - dense_2_acc_6: 0.5705 - val_loss: 38.8291 - val_dense_2_loss_1: 3.4876 - val_dense_2_loss_2: 7.3590 - val_dense_2_loss_3: 7.5243 - val_dense_2_loss_4: 7.6340 - val_dense_2_loss_5: 7.2402 - val_dense_2_loss_6: 5.5840 - val_dense_2_acc_1: 0.5263 - val_dense_2_acc_2: 0.2105 - val_dense_2_acc_3: 0.1404 - val_dense_2_acc_4: 0.1053 - val_dense_2_acc_5: 0.2456 - val_dense_2_acc_6: 0.3684\n",
      "Epoch 118/200\n",
      "2800/2810 [============================>.] - ETA: 0s - loss: 10.7104 - dense_2_loss_1: 1.0954 - dense_2_loss_2: 1.6476 - dense_2_loss_3: 1.8743 - dense_2_loss_4: 2.1890 - dense_2_loss_5: 2.1160 - dense_2_loss_6: 1.7881 - dense_2_acc_1: 0.6836 - dense_2_acc_2: 0.5661 - dense_2_acc_3: 0.5204 - dense_2_acc_4: 0.4400 - dense_2_acc_5: 0.4796 - dense_2_acc_6: 0.5686Epoch 00117: saving model to main_model_weights_new.h5\n",
      "2810/2810 [==============================] - 127s - loss: 10.7183 - dense_2_loss_1: 1.0959 - dense_2_loss_2: 1.6510 - dense_2_loss_3: 1.8752 - dense_2_loss_4: 2.1905 - dense_2_loss_5: 2.1176 - dense_2_loss_6: 1.7879 - dense_2_acc_1: 0.6836 - dense_2_acc_2: 0.5658 - dense_2_acc_3: 0.5199 - dense_2_acc_4: 0.4395 - dense_2_acc_5: 0.4797 - dense_2_acc_6: 0.5690 - val_loss: 38.0394 - val_dense_2_loss_1: 3.1483 - val_dense_2_loss_2: 7.4548 - val_dense_2_loss_3: 7.1960 - val_dense_2_loss_4: 7.6126 - val_dense_2_loss_5: 6.9373 - val_dense_2_loss_6: 5.6903 - val_dense_2_acc_1: 0.5088 - val_dense_2_acc_2: 0.1754 - val_dense_2_acc_3: 0.1754 - val_dense_2_acc_4: 0.1579 - val_dense_2_acc_5: 0.1930 - val_dense_2_acc_6: 0.3333\n",
      "Epoch 119/200\n",
      "2800/2810 [============================>.] - ETA: 0s - loss: 10.5471 - dense_2_loss_1: 1.0883 - dense_2_loss_2: 1.6698 - dense_2_loss_3: 1.8355 - dense_2_loss_4: 2.1310 - dense_2_loss_5: 2.0668 - dense_2_loss_6: 1.7558 - dense_2_acc_1: 0.6961 - dense_2_acc_2: 0.5629 - dense_2_acc_3: 0.5211 - dense_2_acc_4: 0.4589 - dense_2_acc_5: 0.4854 - dense_2_acc_6: 0.5825Epoch 00118: saving model to main_model_weights_new.h5\n",
      "2810/2810 [==============================] - 126s - loss: 10.5433 - dense_2_loss_1: 1.0902 - dense_2_loss_2: 1.6705 - dense_2_loss_3: 1.8346 - dense_2_loss_4: 2.1290 - dense_2_loss_5: 2.0645 - dense_2_loss_6: 1.7545 - dense_2_acc_1: 0.6950 - dense_2_acc_2: 0.5623 - dense_2_acc_3: 0.5214 - dense_2_acc_4: 0.4594 - dense_2_acc_5: 0.4858 - dense_2_acc_6: 0.5826 - val_loss: 36.7814 - val_dense_2_loss_1: 2.9521 - val_dense_2_loss_2: 7.1304 - val_dense_2_loss_3: 6.9556 - val_dense_2_loss_4: 7.3532 - val_dense_2_loss_5: 6.6653 - val_dense_2_loss_6: 5.7248 - val_dense_2_acc_1: 0.5263 - val_dense_2_acc_2: 0.1754 - val_dense_2_acc_3: 0.1930 - val_dense_2_acc_4: 0.1228 - val_dense_2_acc_5: 0.2456 - val_dense_2_acc_6: 0.3333\n",
      "Epoch 120/200\n",
      "2800/2810 [============================>.] - ETA: 0s - loss: 10.4334 - dense_2_loss_1: 1.0881 - dense_2_loss_2: 1.6349 - dense_2_loss_3: 1.8223 - dense_2_loss_4: 2.1040 - dense_2_loss_5: 2.0641 - dense_2_loss_6: 1.7200 - dense_2_acc_1: 0.6904 - dense_2_acc_2: 0.5682 - dense_2_acc_3: 0.5189 - dense_2_acc_4: 0.4586 - dense_2_acc_5: 0.4864 - dense_2_acc_6: 0.5850Epoch 00119: saving model to main_model_weights_new.h5\n",
      "2810/2810 [==============================] - 128s - loss: 10.4284 - dense_2_loss_1: 1.0851 - dense_2_loss_2: 1.6342 - dense_2_loss_3: 1.8228 - dense_2_loss_4: 2.1007 - dense_2_loss_5: 2.0648 - dense_2_loss_6: 1.7208 - dense_2_acc_1: 0.6911 - dense_2_acc_2: 0.5680 - dense_2_acc_3: 0.5192 - dense_2_acc_4: 0.4594 - dense_2_acc_5: 0.4868 - dense_2_acc_6: 0.5847 - val_loss: 38.2373 - val_dense_2_loss_1: 3.1782 - val_dense_2_loss_2: 7.5102 - val_dense_2_loss_3: 7.5357 - val_dense_2_loss_4: 7.3416 - val_dense_2_loss_5: 6.6719 - val_dense_2_loss_6: 5.9997 - val_dense_2_acc_1: 0.5439 - val_dense_2_acc_2: 0.1754 - val_dense_2_acc_3: 0.1930 - val_dense_2_acc_4: 0.1053 - val_dense_2_acc_5: 0.2281 - val_dense_2_acc_6: 0.3860\n",
      "Epoch 121/200\n",
      "2800/2810 [============================>.] - ETA: 0s - loss: 10.3697 - dense_2_loss_1: 1.0395 - dense_2_loss_2: 1.5975 - dense_2_loss_3: 1.8764 - dense_2_loss_4: 2.0981 - dense_2_loss_5: 2.0406 - dense_2_loss_6: 1.7177 - dense_2_acc_1: 0.6957 - dense_2_acc_2: 0.5754 - dense_2_acc_3: 0.5093 - dense_2_acc_4: 0.4511 - dense_2_acc_5: 0.4921 - dense_2_acc_6: 0.5868Epoch 00120: saving model to main_model_weights_new.h5\n",
      "2810/2810 [==============================] - 127s - loss: 10.3763 - dense_2_loss_1: 1.0391 - dense_2_loss_2: 1.6002 - dense_2_loss_3: 1.8783 - dense_2_loss_4: 2.1023 - dense_2_loss_5: 2.0406 - dense_2_loss_6: 1.7158 - dense_2_acc_1: 0.6961 - dense_2_acc_2: 0.5740 - dense_2_acc_3: 0.5096 - dense_2_acc_4: 0.4509 - dense_2_acc_5: 0.4925 - dense_2_acc_6: 0.5868 - val_loss: 37.9215 - val_dense_2_loss_1: 3.0537 - val_dense_2_loss_2: 7.2218 - val_dense_2_loss_3: 7.4377 - val_dense_2_loss_4: 7.3385 - val_dense_2_loss_5: 6.6235 - val_dense_2_loss_6: 6.2462 - val_dense_2_acc_1: 0.5263 - val_dense_2_acc_2: 0.2105 - val_dense_2_acc_3: 0.1579 - val_dense_2_acc_4: 0.1404 - val_dense_2_acc_5: 0.2456 - val_dense_2_acc_6: 0.3684\n",
      "Epoch 122/200\n",
      "2800/2810 [============================>.] - ETA: 0s - loss: 10.1312 - dense_2_loss_1: 1.0387 - dense_2_loss_2: 1.5745 - dense_2_loss_3: 1.7761 - dense_2_loss_4: 2.0621 - dense_2_loss_5: 1.9493 - dense_2_loss_6: 1.7305 - dense_2_acc_1: 0.6961 - dense_2_acc_2: 0.5771 - dense_2_acc_3: 0.5325 - dense_2_acc_4: 0.4586 - dense_2_acc_5: 0.5118 - dense_2_acc_6: 0.5786Epoch 00121: saving model to main_model_weights_new.h5\n",
      "2810/2810 [==============================] - 129s - loss: 10.1543 - dense_2_loss_1: 1.0406 - dense_2_loss_2: 1.5825 - dense_2_loss_3: 1.7796 - dense_2_loss_4: 2.0671 - dense_2_loss_5: 1.9516 - dense_2_loss_6: 1.7330 - dense_2_acc_1: 0.6957 - dense_2_acc_2: 0.5765 - dense_2_acc_3: 0.5313 - dense_2_acc_4: 0.4584 - dense_2_acc_5: 0.5114 - dense_2_acc_6: 0.5779 - val_loss: 37.7100 - val_dense_2_loss_1: 3.0658 - val_dense_2_loss_2: 7.1287 - val_dense_2_loss_3: 7.1473 - val_dense_2_loss_4: 7.7420 - val_dense_2_loss_5: 6.6862 - val_dense_2_loss_6: 5.9399 - val_dense_2_acc_1: 0.5439 - val_dense_2_acc_2: 0.1930 - val_dense_2_acc_3: 0.1579 - val_dense_2_acc_4: 0.0702 - val_dense_2_acc_5: 0.2105 - val_dense_2_acc_6: 0.2982\n",
      "Epoch 123/200\n",
      "2800/2810 [============================>.] - ETA: 0s - loss: 10.2099 - dense_2_loss_1: 1.0265 - dense_2_loss_2: 1.5784 - dense_2_loss_3: 1.7698 - dense_2_loss_4: 2.1320 - dense_2_loss_5: 2.0239 - dense_2_loss_6: 1.6794 - dense_2_acc_1: 0.6993 - dense_2_acc_2: 0.5732 - dense_2_acc_3: 0.5339 - dense_2_acc_4: 0.4629 - dense_2_acc_5: 0.4929 - dense_2_acc_6: 0.5961Epoch 00122: saving model to main_model_weights_new.h5\n",
      "2810/2810 [==============================] - 130s - loss: 10.2094 - dense_2_loss_1: 1.0272 - dense_2_loss_2: 1.5826 - dense_2_loss_3: 1.7708 - dense_2_loss_4: 2.1290 - dense_2_loss_5: 2.0224 - dense_2_loss_6: 1.6773 - dense_2_acc_1: 0.6986 - dense_2_acc_2: 0.5730 - dense_2_acc_3: 0.5335 - dense_2_acc_4: 0.4633 - dense_2_acc_5: 0.4932 - dense_2_acc_6: 0.5968 - val_loss: 37.6030 - val_dense_2_loss_1: 2.9433 - val_dense_2_loss_2: 7.2934 - val_dense_2_loss_3: 7.1782 - val_dense_2_loss_4: 7.6692 - val_dense_2_loss_5: 6.7251 - val_dense_2_loss_6: 5.7939 - val_dense_2_acc_1: 0.5088 - val_dense_2_acc_2: 0.1930 - val_dense_2_acc_3: 0.1754 - val_dense_2_acc_4: 0.1404 - val_dense_2_acc_5: 0.2105 - val_dense_2_acc_6: 0.3333\n",
      "Epoch 124/200\n",
      "2800/2810 [============================>.] - ETA: 0s - loss: 9.9740 - dense_2_loss_1: 1.0260 - dense_2_loss_2: 1.5458 - dense_2_loss_3: 1.7293 - dense_2_loss_4: 2.0397 - dense_2_loss_5: 1.9598 - dense_2_loss_6: 1.6734 - dense_2_acc_1: 0.6996 - dense_2_acc_2: 0.5754 - dense_2_acc_3: 0.5489 - dense_2_acc_4: 0.4821 - dense_2_acc_5: 0.5093 - dense_2_acc_6: 0.5946Epoch 00123: saving model to main_model_weights_new.h5\n",
      "2810/2810 [==============================] - 129s - loss: 9.9795 - dense_2_loss_1: 1.0292 - dense_2_loss_2: 1.5484 - dense_2_loss_3: 1.7302 - dense_2_loss_4: 2.0408 - dense_2_loss_5: 1.9599 - dense_2_loss_6: 1.6711 - dense_2_acc_1: 0.6993 - dense_2_acc_2: 0.5751 - dense_2_acc_3: 0.5488 - dense_2_acc_4: 0.4819 - dense_2_acc_5: 0.5093 - dense_2_acc_6: 0.5954 - val_loss: 38.8648 - val_dense_2_loss_1: 3.0503 - val_dense_2_loss_2: 7.3691 - val_dense_2_loss_3: 7.3969 - val_dense_2_loss_4: 7.8826 - val_dense_2_loss_5: 6.9372 - val_dense_2_loss_6: 6.2285 - val_dense_2_acc_1: 0.4737 - val_dense_2_acc_2: 0.1930 - val_dense_2_acc_3: 0.1754 - val_dense_2_acc_4: 0.1404 - val_dense_2_acc_5: 0.2632 - val_dense_2_acc_6: 0.3860\n",
      "Epoch 125/200\n",
      "2800/2810 [============================>.] - ETA: 0s - loss: 9.8461 - dense_2_loss_1: 0.9891 - dense_2_loss_2: 1.5753 - dense_2_loss_3: 1.7219 - dense_2_loss_4: 2.0120 - dense_2_loss_5: 1.9106 - dense_2_loss_6: 1.6371 - dense_2_acc_1: 0.7107 - dense_2_acc_2: 0.5814 - dense_2_acc_3: 0.5379 - dense_2_acc_4: 0.4800 - dense_2_acc_5: 0.5161 - dense_2_acc_6: 0.6014Epoch 00124: saving model to main_model_weights_new.h5\n",
      "2810/2810 [==============================] - 131s - loss: 9.8450 - dense_2_loss_1: 0.9872 - dense_2_loss_2: 1.5738 - dense_2_loss_3: 1.7186 - dense_2_loss_4: 2.0139 - dense_2_loss_5: 1.9174 - dense_2_loss_6: 1.6340 - dense_2_acc_1: 0.7107 - dense_2_acc_2: 0.5815 - dense_2_acc_3: 0.5388 - dense_2_acc_4: 0.4804 - dense_2_acc_5: 0.5153 - dense_2_acc_6: 0.6021 - val_loss: 38.1459 - val_dense_2_loss_1: 3.1104 - val_dense_2_loss_2: 7.3956 - val_dense_2_loss_3: 7.3521 - val_dense_2_loss_4: 7.7738 - val_dense_2_loss_5: 6.7745 - val_dense_2_loss_6: 5.7396 - val_dense_2_acc_1: 0.5439 - val_dense_2_acc_2: 0.2105 - val_dense_2_acc_3: 0.1754 - val_dense_2_acc_4: 0.1404 - val_dense_2_acc_5: 0.1930 - val_dense_2_acc_6: 0.2982\n",
      "Epoch 126/200\n",
      "2800/2810 [============================>.] - ETA: 0s - loss: 9.9497 - dense_2_loss_1: 1.0360 - dense_2_loss_2: 1.5566 - dense_2_loss_3: 1.7572 - dense_2_loss_4: 2.0288 - dense_2_loss_5: 1.9283 - dense_2_loss_6: 1.6428 - dense_2_acc_1: 0.7018 - dense_2_acc_2: 0.5818 - dense_2_acc_3: 0.5350 - dense_2_acc_4: 0.4811 - dense_2_acc_5: 0.5089 - dense_2_acc_6: 0.5993Epoch 00125: saving model to main_model_weights_new.h5\n",
      "2810/2810 [==============================] - 129s - loss: 9.9652 - dense_2_loss_1: 1.0394 - dense_2_loss_2: 1.5592 - dense_2_loss_3: 1.7603 - dense_2_loss_4: 2.0305 - dense_2_loss_5: 1.9311 - dense_2_loss_6: 1.6447 - dense_2_acc_1: 0.7018 - dense_2_acc_2: 0.5815 - dense_2_acc_3: 0.5349 - dense_2_acc_4: 0.4808 - dense_2_acc_5: 0.5085 - dense_2_acc_6: 0.5986 - val_loss: 38.0470 - val_dense_2_loss_1: 3.0564 - val_dense_2_loss_2: 7.2555 - val_dense_2_loss_3: 7.3218 - val_dense_2_loss_4: 7.6955 - val_dense_2_loss_5: 7.0491 - val_dense_2_loss_6: 5.6686 - val_dense_2_acc_1: 0.4912 - val_dense_2_acc_2: 0.2281 - val_dense_2_acc_3: 0.1579 - val_dense_2_acc_4: 0.1579 - val_dense_2_acc_5: 0.1930 - val_dense_2_acc_6: 0.2456\n",
      "Epoch 127/200\n",
      "2800/2810 [============================>.] - ETA: 0s - loss: 9.7070 - dense_2_loss_1: 0.9976 - dense_2_loss_2: 1.5323 - dense_2_loss_3: 1.7251 - dense_2_loss_4: 1.9876 - dense_2_loss_5: 1.8667 - dense_2_loss_6: 1.5977 - dense_2_acc_1: 0.7061 - dense_2_acc_2: 0.5693 - dense_2_acc_3: 0.5500 - dense_2_acc_4: 0.4871 - dense_2_acc_5: 0.5200 - dense_2_acc_6: 0.6189Epoch 00126: saving model to main_model_weights_new.h5\n",
      "2810/2810 [==============================] - 129s - loss: 9.7115 - dense_2_loss_1: 0.9968 - dense_2_loss_2: 1.5323 - dense_2_loss_3: 1.7228 - dense_2_loss_4: 1.9886 - dense_2_loss_5: 1.8699 - dense_2_loss_6: 1.6011 - dense_2_acc_1: 0.7068 - dense_2_acc_2: 0.5694 - dense_2_acc_3: 0.5505 - dense_2_acc_4: 0.4868 - dense_2_acc_5: 0.5196 - dense_2_acc_6: 0.6192 - val_loss: 36.9802 - val_dense_2_loss_1: 2.8104 - val_dense_2_loss_2: 7.0780 - val_dense_2_loss_3: 7.1302 - val_dense_2_loss_4: 7.8222 - val_dense_2_loss_5: 6.4505 - val_dense_2_loss_6: 5.6888 - val_dense_2_acc_1: 0.5614 - val_dense_2_acc_2: 0.1754 - val_dense_2_acc_3: 0.2281 - val_dense_2_acc_4: 0.1228 - val_dense_2_acc_5: 0.2456 - val_dense_2_acc_6: 0.2632\n",
      "Epoch 128/200\n",
      "2800/2810 [============================>.] - ETA: 0s - loss: 9.6681 - dense_2_loss_1: 0.9876 - dense_2_loss_2: 1.4839 - dense_2_loss_3: 1.6997 - dense_2_loss_4: 2.0031 - dense_2_loss_5: 1.8952 - dense_2_loss_6: 1.5986 - dense_2_acc_1: 0.7236 - dense_2_acc_2: 0.6075 - dense_2_acc_3: 0.5525 - dense_2_acc_4: 0.4843 - dense_2_acc_5: 0.5168 - dense_2_acc_6: 0.6075Epoch 00127: saving model to main_model_weights_new.h5\n",
      "2810/2810 [==============================] - 130s - loss: 9.6844 - dense_2_loss_1: 0.9887 - dense_2_loss_2: 1.4882 - dense_2_loss_3: 1.7018 - dense_2_loss_4: 2.0077 - dense_2_loss_5: 1.8973 - dense_2_loss_6: 1.6007 - dense_2_acc_1: 0.7224 - dense_2_acc_2: 0.6064 - dense_2_acc_3: 0.5520 - dense_2_acc_4: 0.4840 - dense_2_acc_5: 0.5164 - dense_2_acc_6: 0.6075 - val_loss: 38.5489 - val_dense_2_loss_1: 3.1402 - val_dense_2_loss_2: 7.4252 - val_dense_2_loss_3: 7.1340 - val_dense_2_loss_4: 7.5489 - val_dense_2_loss_5: 7.3645 - val_dense_2_loss_6: 5.9361 - val_dense_2_acc_1: 0.5439 - val_dense_2_acc_2: 0.1754 - val_dense_2_acc_3: 0.2105 - val_dense_2_acc_4: 0.1228 - val_dense_2_acc_5: 0.1754 - val_dense_2_acc_6: 0.3860\n",
      "Epoch 129/200\n",
      "2800/2810 [============================>.] - ETA: 0s - loss: 9.6503 - dense_2_loss_1: 0.9971 - dense_2_loss_2: 1.5338 - dense_2_loss_3: 1.6700 - dense_2_loss_4: 1.9252 - dense_2_loss_5: 1.9125 - dense_2_loss_6: 1.6117 - dense_2_acc_1: 0.7157 - dense_2_acc_2: 0.5814 - dense_2_acc_3: 0.5446 - dense_2_acc_4: 0.4946 - dense_2_acc_5: 0.5214 - dense_2_acc_6: 0.6043Epoch 00128: saving model to main_model_weights_new.h5\n",
      "2810/2810 [==============================] - 129s - loss: 9.6559 - dense_2_loss_1: 0.9975 - dense_2_loss_2: 1.5315 - dense_2_loss_3: 1.6682 - dense_2_loss_4: 1.9277 - dense_2_loss_5: 1.9174 - dense_2_loss_6: 1.6137 - dense_2_acc_1: 0.7153 - dense_2_acc_2: 0.5822 - dense_2_acc_3: 0.5448 - dense_2_acc_4: 0.4940 - dense_2_acc_5: 0.5206 - dense_2_acc_6: 0.6046 - val_loss: 36.5767 - val_dense_2_loss_1: 2.7838 - val_dense_2_loss_2: 6.7697 - val_dense_2_loss_3: 7.3740 - val_dense_2_loss_4: 7.6037 - val_dense_2_loss_5: 6.4591 - val_dense_2_loss_6: 5.5864 - val_dense_2_acc_1: 0.5439 - val_dense_2_acc_2: 0.1930 - val_dense_2_acc_3: 0.1404 - val_dense_2_acc_4: 0.1228 - val_dense_2_acc_5: 0.2281 - val_dense_2_acc_6: 0.2281\n",
      "Epoch 130/200\n",
      "2800/2810 [============================>.] - ETA: 0s - loss: 9.5569 - dense_2_loss_1: 0.9870 - dense_2_loss_2: 1.4989 - dense_2_loss_3: 1.6396 - dense_2_loss_4: 1.9242 - dense_2_loss_5: 1.8940 - dense_2_loss_6: 1.6132 - dense_2_acc_1: 0.7132 - dense_2_acc_2: 0.5946 - dense_2_acc_3: 0.5632 - dense_2_acc_4: 0.5043 - dense_2_acc_5: 0.5071 - dense_2_acc_6: 0.6107Epoch 00129: saving model to main_model_weights_new.h5\n",
      "2810/2810 [==============================] - 130s - loss: 9.5733 - dense_2_loss_1: 0.9881 - dense_2_loss_2: 1.5030 - dense_2_loss_3: 1.6434 - dense_2_loss_4: 1.9276 - dense_2_loss_5: 1.8958 - dense_2_loss_6: 1.6153 - dense_2_acc_1: 0.7125 - dense_2_acc_2: 0.5936 - dense_2_acc_3: 0.5626 - dense_2_acc_4: 0.5028 - dense_2_acc_5: 0.5071 - dense_2_acc_6: 0.6100 - val_loss: 38.7743 - val_dense_2_loss_1: 3.0135 - val_dense_2_loss_2: 7.3170 - val_dense_2_loss_3: 7.6860 - val_dense_2_loss_4: 7.8282 - val_dense_2_loss_5: 6.7640 - val_dense_2_loss_6: 6.1656 - val_dense_2_acc_1: 0.4912 - val_dense_2_acc_2: 0.1930 - val_dense_2_acc_3: 0.1404 - val_dense_2_acc_4: 0.1228 - val_dense_2_acc_5: 0.2105 - val_dense_2_acc_6: 0.3509\n",
      "Epoch 131/200\n",
      "2800/2810 [============================>.] - ETA: 0s - loss: 9.4507 - dense_2_loss_1: 0.9790 - dense_2_loss_2: 1.4782 - dense_2_loss_3: 1.6179 - dense_2_loss_4: 1.8992 - dense_2_loss_5: 1.8602 - dense_2_loss_6: 1.6162 - dense_2_acc_1: 0.7221 - dense_2_acc_2: 0.5946 - dense_2_acc_3: 0.5679 - dense_2_acc_4: 0.5011 - dense_2_acc_5: 0.5357 - dense_2_acc_6: 0.6154Epoch 00130: saving model to main_model_weights_new.h5\n",
      "2810/2810 [==============================] - 129s - loss: 9.4500 - dense_2_loss_1: 0.9801 - dense_2_loss_2: 1.4753 - dense_2_loss_3: 1.6199 - dense_2_loss_4: 1.8990 - dense_2_loss_5: 1.8591 - dense_2_loss_6: 1.6166 - dense_2_acc_1: 0.7217 - dense_2_acc_2: 0.5957 - dense_2_acc_3: 0.5673 - dense_2_acc_4: 0.5007 - dense_2_acc_5: 0.5359 - dense_2_acc_6: 0.6153 - val_loss: 38.1604 - val_dense_2_loss_1: 2.7856 - val_dense_2_loss_2: 7.5206 - val_dense_2_loss_3: 7.3441 - val_dense_2_loss_4: 7.6265 - val_dense_2_loss_5: 6.9204 - val_dense_2_loss_6: 5.9632 - val_dense_2_acc_1: 0.5263 - val_dense_2_acc_2: 0.2281 - val_dense_2_acc_3: 0.1930 - val_dense_2_acc_4: 0.1228 - val_dense_2_acc_5: 0.2105 - val_dense_2_acc_6: 0.3333\n",
      "Epoch 132/200\n",
      "2800/2810 [============================>.] - ETA: 0s - loss: 9.2747 - dense_2_loss_1: 0.9710 - dense_2_loss_2: 1.4489 - dense_2_loss_3: 1.6453 - dense_2_loss_4: 1.8873 - dense_2_loss_5: 1.8064 - dense_2_loss_6: 1.5159 - dense_2_acc_1: 0.7204 - dense_2_acc_2: 0.6071 - dense_2_acc_3: 0.5664 - dense_2_acc_4: 0.4979 - dense_2_acc_5: 0.5396 - dense_2_acc_6: 0.6118Epoch 00131: saving model to main_model_weights_new.h5\n",
      "2810/2810 [==============================] - 128s - loss: 9.2707 - dense_2_loss_1: 0.9732 - dense_2_loss_2: 1.4472 - dense_2_loss_3: 1.6445 - dense_2_loss_4: 1.8855 - dense_2_loss_5: 1.8077 - dense_2_loss_6: 1.5127 - dense_2_acc_1: 0.7203 - dense_2_acc_2: 0.6075 - dense_2_acc_3: 0.5669 - dense_2_acc_4: 0.4982 - dense_2_acc_5: 0.5399 - dense_2_acc_6: 0.6125 - val_loss: 38.2420 - val_dense_2_loss_1: 2.9618 - val_dense_2_loss_2: 7.4277 - val_dense_2_loss_3: 7.5703 - val_dense_2_loss_4: 7.6372 - val_dense_2_loss_5: 6.6520 - val_dense_2_loss_6: 5.9930 - val_dense_2_acc_1: 0.5088 - val_dense_2_acc_2: 0.2281 - val_dense_2_acc_3: 0.1754 - val_dense_2_acc_4: 0.1404 - val_dense_2_acc_5: 0.2281 - val_dense_2_acc_6: 0.3509\n",
      "Epoch 133/200\n",
      "2800/2810 [============================>.] - ETA: 0s - loss: 9.2045 - dense_2_loss_1: 0.9447 - dense_2_loss_2: 1.4402 - dense_2_loss_3: 1.6260 - dense_2_loss_4: 1.8421 - dense_2_loss_5: 1.7924 - dense_2_loss_6: 1.5591 - dense_2_acc_1: 0.7186 - dense_2_acc_2: 0.6000 - dense_2_acc_3: 0.5679 - dense_2_acc_4: 0.5221 - dense_2_acc_5: 0.5361 - dense_2_acc_6: 0.6214Epoch 00132: saving model to main_model_weights_new.h5\n",
      "2810/2810 [==============================] - 131s - loss: 9.2176 - dense_2_loss_1: 0.9471 - dense_2_loss_2: 1.4431 - dense_2_loss_3: 1.6265 - dense_2_loss_4: 1.8449 - dense_2_loss_5: 1.7962 - dense_2_loss_6: 1.5597 - dense_2_acc_1: 0.7178 - dense_2_acc_2: 0.5996 - dense_2_acc_3: 0.5680 - dense_2_acc_4: 0.5217 - dense_2_acc_5: 0.5356 - dense_2_acc_6: 0.6210 - val_loss: 38.7454 - val_dense_2_loss_1: 2.8624 - val_dense_2_loss_2: 7.2012 - val_dense_2_loss_3: 7.3269 - val_dense_2_loss_4: 7.7147 - val_dense_2_loss_5: 7.6287 - val_dense_2_loss_6: 6.0117 - val_dense_2_acc_1: 0.5439 - val_dense_2_acc_2: 0.1930 - val_dense_2_acc_3: 0.2105 - val_dense_2_acc_4: 0.1754 - val_dense_2_acc_5: 0.2281 - val_dense_2_acc_6: 0.3684\n",
      "Epoch 134/200\n",
      "2800/2810 [============================>.] - ETA: 0s - loss: 9.1262 - dense_2_loss_1: 0.9635 - dense_2_loss_2: 1.4688 - dense_2_loss_3: 1.5675 - dense_2_loss_4: 1.8194 - dense_2_loss_5: 1.7606 - dense_2_loss_6: 1.5463 - dense_2_acc_1: 0.7296 - dense_2_acc_2: 0.6036 - dense_2_acc_3: 0.5811 - dense_2_acc_4: 0.5104 - dense_2_acc_5: 0.5314 - dense_2_acc_6: 0.6161Epoch 00133: saving model to main_model_weights_new.h5\n",
      "2810/2810 [==============================] - 129s - loss: 9.1299 - dense_2_loss_1: 0.9621 - dense_2_loss_2: 1.4668 - dense_2_loss_3: 1.5686 - dense_2_loss_4: 1.8189 - dense_2_loss_5: 1.7611 - dense_2_loss_6: 1.5524 - dense_2_acc_1: 0.7295 - dense_2_acc_2: 0.6043 - dense_2_acc_3: 0.5811 - dense_2_acc_4: 0.5110 - dense_2_acc_5: 0.5317 - dense_2_acc_6: 0.6153 - val_loss: 39.2853 - val_dense_2_loss_1: 2.8141 - val_dense_2_loss_2: 7.5965 - val_dense_2_loss_3: 7.5033 - val_dense_2_loss_4: 8.0066 - val_dense_2_loss_5: 7.1241 - val_dense_2_loss_6: 6.2407 - val_dense_2_acc_1: 0.5439 - val_dense_2_acc_2: 0.2105 - val_dense_2_acc_3: 0.1930 - val_dense_2_acc_4: 0.1404 - val_dense_2_acc_5: 0.2105 - val_dense_2_acc_6: 0.3860\n",
      "Epoch 135/200\n",
      "2800/2810 [============================>.] - ETA: 0s - loss: 9.1122 - dense_2_loss_1: 0.9461 - dense_2_loss_2: 1.4372 - dense_2_loss_3: 1.5869 - dense_2_loss_4: 1.8456 - dense_2_loss_5: 1.7462 - dense_2_loss_6: 1.5502 - dense_2_acc_1: 0.7282 - dense_2_acc_2: 0.6161 - dense_2_acc_3: 0.5696 - dense_2_acc_4: 0.5157 - dense_2_acc_5: 0.5350 - dense_2_acc_6: 0.6168Epoch 00134: saving model to main_model_weights_new.h5\n",
      "2810/2810 [==============================] - 129s - loss: 9.1199 - dense_2_loss_1: 0.9494 - dense_2_loss_2: 1.4372 - dense_2_loss_3: 1.5880 - dense_2_loss_4: 1.8465 - dense_2_loss_5: 1.7469 - dense_2_loss_6: 1.5520 - dense_2_acc_1: 0.7274 - dense_2_acc_2: 0.6160 - dense_2_acc_3: 0.5694 - dense_2_acc_4: 0.5153 - dense_2_acc_5: 0.5349 - dense_2_acc_6: 0.6164 - val_loss: 39.9561 - val_dense_2_loss_1: 2.9264 - val_dense_2_loss_2: 7.7637 - val_dense_2_loss_3: 7.3624 - val_dense_2_loss_4: 7.9547 - val_dense_2_loss_5: 7.6655 - val_dense_2_loss_6: 6.2834 - val_dense_2_acc_1: 0.5263 - val_dense_2_acc_2: 0.1754 - val_dense_2_acc_3: 0.1754 - val_dense_2_acc_4: 0.1754 - val_dense_2_acc_5: 0.2105 - val_dense_2_acc_6: 0.3860\n",
      "Epoch 136/200\n",
      "2800/2810 [============================>.] - ETA: 0s - loss: 8.9889 - dense_2_loss_1: 0.9396 - dense_2_loss_2: 1.3870 - dense_2_loss_3: 1.5894 - dense_2_loss_4: 1.8521 - dense_2_loss_5: 1.7699 - dense_2_loss_6: 1.4508 - dense_2_acc_1: 0.7321 - dense_2_acc_2: 0.6246 - dense_2_acc_3: 0.5739 - dense_2_acc_4: 0.5171 - dense_2_acc_5: 0.5554 - dense_2_acc_6: 0.6368Epoch 00135: saving model to main_model_weights_new.h5\n",
      "2810/2810 [==============================] - 128s - loss: 8.9852 - dense_2_loss_1: 0.9398 - dense_2_loss_2: 1.3866 - dense_2_loss_3: 1.5876 - dense_2_loss_4: 1.8525 - dense_2_loss_5: 1.7686 - dense_2_loss_6: 1.4501 - dense_2_acc_1: 0.7320 - dense_2_acc_2: 0.6246 - dense_2_acc_3: 0.5744 - dense_2_acc_4: 0.5167 - dense_2_acc_5: 0.5559 - dense_2_acc_6: 0.6363 - val_loss: 39.3822 - val_dense_2_loss_1: 3.0603 - val_dense_2_loss_2: 7.6332 - val_dense_2_loss_3: 7.1863 - val_dense_2_loss_4: 7.9009 - val_dense_2_loss_5: 7.3431 - val_dense_2_loss_6: 6.2584 - val_dense_2_acc_1: 0.5439 - val_dense_2_acc_2: 0.1930 - val_dense_2_acc_3: 0.1404 - val_dense_2_acc_4: 0.1579 - val_dense_2_acc_5: 0.2105 - val_dense_2_acc_6: 0.2807\n",
      "Epoch 137/200\n",
      "2800/2810 [============================>.] - ETA: 0s - loss: 8.7278 - dense_2_loss_1: 0.9150 - dense_2_loss_2: 1.3771 - dense_2_loss_3: 1.5115 - dense_2_loss_4: 1.7395 - dense_2_loss_5: 1.7048 - dense_2_loss_6: 1.4799 - dense_2_acc_1: 0.7321 - dense_2_acc_2: 0.6243 - dense_2_acc_3: 0.5911 - dense_2_acc_4: 0.5375 - dense_2_acc_5: 0.5564 - dense_2_acc_6: 0.6336Epoch 00136: saving model to main_model_weights_new.h5\n",
      "2810/2810 [==============================] - 127s - loss: 8.7251 - dense_2_loss_1: 0.9178 - dense_2_loss_2: 1.3755 - dense_2_loss_3: 1.5126 - dense_2_loss_4: 1.7393 - dense_2_loss_5: 1.7031 - dense_2_loss_6: 1.4769 - dense_2_acc_1: 0.7313 - dense_2_acc_2: 0.6246 - dense_2_acc_3: 0.5904 - dense_2_acc_4: 0.5370 - dense_2_acc_5: 0.5566 - dense_2_acc_6: 0.6342 - val_loss: 39.1709 - val_dense_2_loss_1: 2.9715 - val_dense_2_loss_2: 7.3749 - val_dense_2_loss_3: 7.2898 - val_dense_2_loss_4: 8.0308 - val_dense_2_loss_5: 7.0826 - val_dense_2_loss_6: 6.4214 - val_dense_2_acc_1: 0.4737 - val_dense_2_acc_2: 0.1579 - val_dense_2_acc_3: 0.1930 - val_dense_2_acc_4: 0.1579 - val_dense_2_acc_5: 0.2281 - val_dense_2_acc_6: 0.3684\n",
      "Epoch 138/200\n",
      "2800/2810 [============================>.] - ETA: 0s - loss: 8.8584 - dense_2_loss_1: 0.9248 - dense_2_loss_2: 1.3932 - dense_2_loss_3: 1.5415 - dense_2_loss_4: 1.7667 - dense_2_loss_5: 1.7461 - dense_2_loss_6: 1.4861 - dense_2_acc_1: 0.7268 - dense_2_acc_2: 0.6218 - dense_2_acc_3: 0.5861 - dense_2_acc_4: 0.5368 - dense_2_acc_5: 0.5507 - dense_2_acc_6: 0.6407Epoch 00137: saving model to main_model_weights_new.h5\n",
      "2810/2810 [==============================] - 129s - loss: 8.8545 - dense_2_loss_1: 0.9232 - dense_2_loss_2: 1.3918 - dense_2_loss_3: 1.5390 - dense_2_loss_4: 1.7655 - dense_2_loss_5: 1.7515 - dense_2_loss_6: 1.4835 - dense_2_acc_1: 0.7270 - dense_2_acc_2: 0.6214 - dense_2_acc_3: 0.5868 - dense_2_acc_4: 0.5370 - dense_2_acc_5: 0.5495 - dense_2_acc_6: 0.6416 - val_loss: 40.3201 - val_dense_2_loss_1: 3.1323 - val_dense_2_loss_2: 8.1082 - val_dense_2_loss_3: 7.5183 - val_dense_2_loss_4: 7.9099 - val_dense_2_loss_5: 7.3886 - val_dense_2_loss_6: 6.2628 - val_dense_2_acc_1: 0.5263 - val_dense_2_acc_2: 0.1930 - val_dense_2_acc_3: 0.1754 - val_dense_2_acc_4: 0.0877 - val_dense_2_acc_5: 0.2456 - val_dense_2_acc_6: 0.4211\n",
      "Epoch 139/200\n",
      "2800/2810 [============================>.] - ETA: 0s - loss: 8.6656 - dense_2_loss_1: 0.9346 - dense_2_loss_2: 1.3073 - dense_2_loss_3: 1.4690 - dense_2_loss_4: 1.7476 - dense_2_loss_5: 1.7473 - dense_2_loss_6: 1.4599 - dense_2_acc_1: 0.7261 - dense_2_acc_2: 0.6275 - dense_2_acc_3: 0.5964 - dense_2_acc_4: 0.5314 - dense_2_acc_5: 0.5507 - dense_2_acc_6: 0.6404Epoch 00138: saving model to main_model_weights_new.h5\n",
      "2810/2810 [==============================] - 130s - loss: 8.6680 - dense_2_loss_1: 0.9360 - dense_2_loss_2: 1.3067 - dense_2_loss_3: 1.4694 - dense_2_loss_4: 1.7464 - dense_2_loss_5: 1.7489 - dense_2_loss_6: 1.4605 - dense_2_acc_1: 0.7256 - dense_2_acc_2: 0.6278 - dense_2_acc_3: 0.5968 - dense_2_acc_4: 0.5313 - dense_2_acc_5: 0.5512 - dense_2_acc_6: 0.6399 - val_loss: 39.1477 - val_dense_2_loss_1: 3.0586 - val_dense_2_loss_2: 7.7636 - val_dense_2_loss_3: 7.3665 - val_dense_2_loss_4: 8.0437 - val_dense_2_loss_5: 6.8819 - val_dense_2_loss_6: 6.0335 - val_dense_2_acc_1: 0.4737 - val_dense_2_acc_2: 0.1930 - val_dense_2_acc_3: 0.2456 - val_dense_2_acc_4: 0.1053 - val_dense_2_acc_5: 0.2632 - val_dense_2_acc_6: 0.3158\n",
      "Epoch 140/200\n",
      "2800/2810 [============================>.] - ETA: 0s - loss: 8.7130 - dense_2_loss_1: 0.9308 - dense_2_loss_2: 1.3498 - dense_2_loss_3: 1.5075 - dense_2_loss_4: 1.7458 - dense_2_loss_5: 1.6963 - dense_2_loss_6: 1.4828 - dense_2_acc_1: 0.7179 - dense_2_acc_2: 0.6332 - dense_2_acc_3: 0.5864 - dense_2_acc_4: 0.5300 - dense_2_acc_5: 0.5589 - dense_2_acc_6: 0.6411Epoch 00139: saving model to main_model_weights_new.h5\n",
      "2810/2810 [==============================] - 130s - loss: 8.7063 - dense_2_loss_1: 0.9299 - dense_2_loss_2: 1.3497 - dense_2_loss_3: 1.5076 - dense_2_loss_4: 1.7426 - dense_2_loss_5: 1.6955 - dense_2_loss_6: 1.4809 - dense_2_acc_1: 0.7181 - dense_2_acc_2: 0.6335 - dense_2_acc_3: 0.5861 - dense_2_acc_4: 0.5313 - dense_2_acc_5: 0.5587 - dense_2_acc_6: 0.6416 - val_loss: 40.0399 - val_dense_2_loss_1: 3.2442 - val_dense_2_loss_2: 8.3050 - val_dense_2_loss_3: 7.4324 - val_dense_2_loss_4: 7.9409 - val_dense_2_loss_5: 6.9994 - val_dense_2_loss_6: 6.1180 - val_dense_2_acc_1: 0.5439 - val_dense_2_acc_2: 0.1930 - val_dense_2_acc_3: 0.2105 - val_dense_2_acc_4: 0.1228 - val_dense_2_acc_5: 0.1754 - val_dense_2_acc_6: 0.3333\n",
      "Epoch 141/200\n",
      "2800/2810 [============================>.] - ETA: 0s - loss: 8.6205 - dense_2_loss_1: 0.9291 - dense_2_loss_2: 1.3395 - dense_2_loss_3: 1.4752 - dense_2_loss_4: 1.7394 - dense_2_loss_5: 1.6767 - dense_2_loss_6: 1.4606 - dense_2_acc_1: 0.7296 - dense_2_acc_2: 0.6386 - dense_2_acc_3: 0.5979 - dense_2_acc_4: 0.5407 - dense_2_acc_5: 0.5775 - dense_2_acc_6: 0.6386Epoch 00140: saving model to main_model_weights_new.h5\n",
      "2810/2810 [==============================] - 129s - loss: 8.6158 - dense_2_loss_1: 0.9275 - dense_2_loss_2: 1.3395 - dense_2_loss_3: 1.4758 - dense_2_loss_4: 1.7382 - dense_2_loss_5: 1.6749 - dense_2_loss_6: 1.4598 - dense_2_acc_1: 0.7299 - dense_2_acc_2: 0.6388 - dense_2_acc_3: 0.5975 - dense_2_acc_4: 0.5402 - dense_2_acc_5: 0.5779 - dense_2_acc_6: 0.6388 - val_loss: 38.8850 - val_dense_2_loss_1: 2.9738 - val_dense_2_loss_2: 7.2067 - val_dense_2_loss_3: 7.4999 - val_dense_2_loss_4: 8.2260 - val_dense_2_loss_5: 7.0315 - val_dense_2_loss_6: 5.9470 - val_dense_2_acc_1: 0.5088 - val_dense_2_acc_2: 0.1754 - val_dense_2_acc_3: 0.1404 - val_dense_2_acc_4: 0.1579 - val_dense_2_acc_5: 0.2632 - val_dense_2_acc_6: 0.3684\n",
      "Epoch 142/200\n",
      "2800/2810 [============================>.] - ETA: 0s - loss: 8.2894 - dense_2_loss_1: 0.8926 - dense_2_loss_2: 1.3074 - dense_2_loss_3: 1.4775 - dense_2_loss_4: 1.6353 - dense_2_loss_5: 1.6270 - dense_2_loss_6: 1.3496 - dense_2_acc_1: 0.7404 - dense_2_acc_2: 0.6311 - dense_2_acc_3: 0.5918 - dense_2_acc_4: 0.5557 - dense_2_acc_5: 0.5682 - dense_2_acc_6: 0.6529Epoch 00141: saving model to main_model_weights_new.h5\n",
      "2810/2810 [==============================] - 130s - loss: 8.2924 - dense_2_loss_1: 0.8955 - dense_2_loss_2: 1.3081 - dense_2_loss_3: 1.4804 - dense_2_loss_4: 1.6355 - dense_2_loss_5: 1.6256 - dense_2_loss_6: 1.3473 - dense_2_acc_1: 0.7399 - dense_2_acc_2: 0.6310 - dense_2_acc_3: 0.5911 - dense_2_acc_4: 0.5559 - dense_2_acc_5: 0.5687 - dense_2_acc_6: 0.6534 - val_loss: 40.0770 - val_dense_2_loss_1: 3.2093 - val_dense_2_loss_2: 7.8462 - val_dense_2_loss_3: 7.9014 - val_dense_2_loss_4: 8.0869 - val_dense_2_loss_5: 6.6802 - val_dense_2_loss_6: 6.3530 - val_dense_2_acc_1: 0.5088 - val_dense_2_acc_2: 0.1754 - val_dense_2_acc_3: 0.1404 - val_dense_2_acc_4: 0.1228 - val_dense_2_acc_5: 0.1930 - val_dense_2_acc_6: 0.3333\n",
      "Epoch 143/200\n",
      "2800/2810 [============================>.] - ETA: 0s - loss: 8.3021 - dense_2_loss_1: 0.8817 - dense_2_loss_2: 1.3110 - dense_2_loss_3: 1.4384 - dense_2_loss_4: 1.6867 - dense_2_loss_5: 1.5938 - dense_2_loss_6: 1.3905 - dense_2_acc_1: 0.7421 - dense_2_acc_2: 0.6343 - dense_2_acc_3: 0.6093 - dense_2_acc_4: 0.5643 - dense_2_acc_5: 0.5825 - dense_2_acc_6: 0.6504Epoch 00142: saving model to main_model_weights_new.h5\n",
      "2810/2810 [==============================] - 127s - loss: 8.2990 - dense_2_loss_1: 0.8799 - dense_2_loss_2: 1.3095 - dense_2_loss_3: 1.4358 - dense_2_loss_4: 1.6845 - dense_2_loss_5: 1.5976 - dense_2_loss_6: 1.3917 - dense_2_acc_1: 0.7423 - dense_2_acc_2: 0.6349 - dense_2_acc_3: 0.6103 - dense_2_acc_4: 0.5651 - dense_2_acc_5: 0.5822 - dense_2_acc_6: 0.6498 - val_loss: 41.3549 - val_dense_2_loss_1: 3.3338 - val_dense_2_loss_2: 8.0096 - val_dense_2_loss_3: 7.5815 - val_dense_2_loss_4: 8.4652 - val_dense_2_loss_5: 7.4195 - val_dense_2_loss_6: 6.5454 - val_dense_2_acc_1: 0.5088 - val_dense_2_acc_2: 0.1930 - val_dense_2_acc_3: 0.1579 - val_dense_2_acc_4: 0.1404 - val_dense_2_acc_5: 0.2807 - val_dense_2_acc_6: 0.3860\n",
      "Epoch 144/200\n",
      "2800/2810 [============================>.] - ETA: 0s - loss: 8.2122 - dense_2_loss_1: 0.8770 - dense_2_loss_2: 1.3088 - dense_2_loss_3: 1.4585 - dense_2_loss_4: 1.6677 - dense_2_loss_5: 1.5836 - dense_2_loss_6: 1.3165 - dense_2_acc_1: 0.7432 - dense_2_acc_2: 0.6371 - dense_2_acc_3: 0.5954 - dense_2_acc_4: 0.5475 - dense_2_acc_5: 0.5739 - dense_2_acc_6: 0.6564Epoch 00143: saving model to main_model_weights_new.h5\n",
      "2810/2810 [==============================] - 127s - loss: 8.2128 - dense_2_loss_1: 0.8779 - dense_2_loss_2: 1.3089 - dense_2_loss_3: 1.4577 - dense_2_loss_4: 1.6676 - dense_2_loss_5: 1.5851 - dense_2_loss_6: 1.3156 - dense_2_acc_1: 0.7427 - dense_2_acc_2: 0.6370 - dense_2_acc_3: 0.5950 - dense_2_acc_4: 0.5470 - dense_2_acc_5: 0.5733 - dense_2_acc_6: 0.6562 - val_loss: 41.2264 - val_dense_2_loss_1: 3.0837 - val_dense_2_loss_2: 7.6512 - val_dense_2_loss_3: 7.5740 - val_dense_2_loss_4: 8.2628 - val_dense_2_loss_5: 7.4588 - val_dense_2_loss_6: 7.1960 - val_dense_2_acc_1: 0.5439 - val_dense_2_acc_2: 0.1754 - val_dense_2_acc_3: 0.1579 - val_dense_2_acc_4: 0.1404 - val_dense_2_acc_5: 0.2456 - val_dense_2_acc_6: 0.4035\n",
      "Epoch 145/200\n",
      "2800/2810 [============================>.] - ETA: 0s - loss: 8.2140 - dense_2_loss_1: 0.8604 - dense_2_loss_2: 1.2911 - dense_2_loss_3: 1.4293 - dense_2_loss_4: 1.6271 - dense_2_loss_5: 1.6143 - dense_2_loss_6: 1.3919 - dense_2_acc_1: 0.7386 - dense_2_acc_2: 0.6489 - dense_2_acc_3: 0.5982 - dense_2_acc_4: 0.5650 - dense_2_acc_5: 0.5807 - dense_2_acc_6: 0.6525Epoch 00144: saving model to main_model_weights_new.h5\n",
      "2810/2810 [==============================] - 128s - loss: 8.2133 - dense_2_loss_1: 0.8599 - dense_2_loss_2: 1.2904 - dense_2_loss_3: 1.4280 - dense_2_loss_4: 1.6291 - dense_2_loss_5: 1.6158 - dense_2_loss_6: 1.3900 - dense_2_acc_1: 0.7384 - dense_2_acc_2: 0.6488 - dense_2_acc_3: 0.5982 - dense_2_acc_4: 0.5648 - dense_2_acc_5: 0.5811 - dense_2_acc_6: 0.6530 - val_loss: 38.9033 - val_dense_2_loss_1: 3.0153 - val_dense_2_loss_2: 7.4126 - val_dense_2_loss_3: 7.3105 - val_dense_2_loss_4: 8.0140 - val_dense_2_loss_5: 6.9563 - val_dense_2_loss_6: 6.1945 - val_dense_2_acc_1: 0.5439 - val_dense_2_acc_2: 0.1930 - val_dense_2_acc_3: 0.2281 - val_dense_2_acc_4: 0.1053 - val_dense_2_acc_5: 0.2281 - val_dense_2_acc_6: 0.3158\n",
      "Epoch 146/200\n",
      "2800/2810 [============================>.] - ETA: 0s - loss: 8.1884 - dense_2_loss_1: 0.8698 - dense_2_loss_2: 1.2921 - dense_2_loss_3: 1.4154 - dense_2_loss_4: 1.6486 - dense_2_loss_5: 1.6269 - dense_2_loss_6: 1.3356 - dense_2_acc_1: 0.7461 - dense_2_acc_2: 0.6357 - dense_2_acc_3: 0.6100 - dense_2_acc_4: 0.5607 - dense_2_acc_5: 0.5764 - dense_2_acc_6: 0.6589Epoch 00145: saving model to main_model_weights_new.h5\n",
      "2810/2810 [==============================] - 126s - loss: 8.1851 - dense_2_loss_1: 0.8712 - dense_2_loss_2: 1.2911 - dense_2_loss_3: 1.4156 - dense_2_loss_4: 1.6489 - dense_2_loss_5: 1.6259 - dense_2_loss_6: 1.3323 - dense_2_acc_1: 0.7456 - dense_2_acc_2: 0.6359 - dense_2_acc_3: 0.6100 - dense_2_acc_4: 0.5609 - dense_2_acc_5: 0.5765 - dense_2_acc_6: 0.6598 - val_loss: 38.8926 - val_dense_2_loss_1: 2.9767 - val_dense_2_loss_2: 7.5719 - val_dense_2_loss_3: 7.4715 - val_dense_2_loss_4: 7.9590 - val_dense_2_loss_5: 6.7002 - val_dense_2_loss_6: 6.2133 - val_dense_2_acc_1: 0.5263 - val_dense_2_acc_2: 0.2105 - val_dense_2_acc_3: 0.1930 - val_dense_2_acc_4: 0.1754 - val_dense_2_acc_5: 0.2281 - val_dense_2_acc_6: 0.3509\n",
      "Epoch 147/200\n",
      "2800/2810 [============================>.] - ETA: 0s - loss: 7.9929 - dense_2_loss_1: 0.8349 - dense_2_loss_2: 1.2509 - dense_2_loss_3: 1.3923 - dense_2_loss_4: 1.5862 - dense_2_loss_5: 1.5664 - dense_2_loss_6: 1.3621 - dense_2_acc_1: 0.7521 - dense_2_acc_2: 0.6546 - dense_2_acc_3: 0.6132 - dense_2_acc_4: 0.5746 - dense_2_acc_5: 0.6018 - dense_2_acc_6: 0.6525Epoch 00146: saving model to main_model_weights_new.h5\n",
      "2810/2810 [==============================] - 127s - loss: 7.9951 - dense_2_loss_1: 0.8365 - dense_2_loss_2: 1.2510 - dense_2_loss_3: 1.3929 - dense_2_loss_4: 1.5860 - dense_2_loss_5: 1.5658 - dense_2_loss_6: 1.3629 - dense_2_acc_1: 0.7520 - dense_2_acc_2: 0.6548 - dense_2_acc_3: 0.6128 - dense_2_acc_4: 0.5747 - dense_2_acc_5: 0.6014 - dense_2_acc_6: 0.6527 - val_loss: 39.8911 - val_dense_2_loss_1: 3.0658 - val_dense_2_loss_2: 8.1776 - val_dense_2_loss_3: 7.4374 - val_dense_2_loss_4: 8.2344 - val_dense_2_loss_5: 6.9029 - val_dense_2_loss_6: 6.0731 - val_dense_2_acc_1: 0.5439 - val_dense_2_acc_2: 0.1754 - val_dense_2_acc_3: 0.1754 - val_dense_2_acc_4: 0.1579 - val_dense_2_acc_5: 0.2281 - val_dense_2_acc_6: 0.3684\n",
      "Epoch 148/200\n",
      "2800/2810 [============================>.] - ETA: 0s - loss: 8.0535 - dense_2_loss_1: 0.8536 - dense_2_loss_2: 1.2972 - dense_2_loss_3: 1.3901 - dense_2_loss_4: 1.6242 - dense_2_loss_5: 1.5441 - dense_2_loss_6: 1.3442 - dense_2_acc_1: 0.7479 - dense_2_acc_2: 0.6468 - dense_2_acc_3: 0.6193 - dense_2_acc_4: 0.5643 - dense_2_acc_5: 0.5957 - dense_2_acc_6: 0.6614Epoch 00147: saving model to main_model_weights_new.h5\n",
      "2810/2810 [==============================] - 127s - loss: 8.0497 - dense_2_loss_1: 0.8526 - dense_2_loss_2: 1.2962 - dense_2_loss_3: 1.3887 - dense_2_loss_4: 1.6249 - dense_2_loss_5: 1.5437 - dense_2_loss_6: 1.3435 - dense_2_acc_1: 0.7480 - dense_2_acc_2: 0.6466 - dense_2_acc_3: 0.6192 - dense_2_acc_4: 0.5644 - dense_2_acc_5: 0.5957 - dense_2_acc_6: 0.6612 - val_loss: 38.0723 - val_dense_2_loss_1: 3.0125 - val_dense_2_loss_2: 7.1285 - val_dense_2_loss_3: 7.5209 - val_dense_2_loss_4: 7.8146 - val_dense_2_loss_5: 6.4517 - val_dense_2_loss_6: 6.1441 - val_dense_2_acc_1: 0.5263 - val_dense_2_acc_2: 0.1930 - val_dense_2_acc_3: 0.1579 - val_dense_2_acc_4: 0.1404 - val_dense_2_acc_5: 0.2456 - val_dense_2_acc_6: 0.2632\n",
      "Epoch 149/200\n",
      "2800/2810 [============================>.] - ETA: 0s - loss: 7.9645 - dense_2_loss_1: 0.8643 - dense_2_loss_2: 1.2385 - dense_2_loss_3: 1.3568 - dense_2_loss_4: 1.6070 - dense_2_loss_5: 1.5175 - dense_2_loss_6: 1.3804 - dense_2_acc_1: 0.7518 - dense_2_acc_2: 0.6571 - dense_2_acc_3: 0.6239 - dense_2_acc_4: 0.5714 - dense_2_acc_5: 0.5957 - dense_2_acc_6: 0.6632Epoch 00148: saving model to main_model_weights_new.h5\n",
      "2810/2810 [==============================] - 129s - loss: 7.9630 - dense_2_loss_1: 0.8642 - dense_2_loss_2: 1.2391 - dense_2_loss_3: 1.3578 - dense_2_loss_4: 1.6057 - dense_2_loss_5: 1.5164 - dense_2_loss_6: 1.3798 - dense_2_acc_1: 0.7512 - dense_2_acc_2: 0.6566 - dense_2_acc_3: 0.6242 - dense_2_acc_4: 0.5715 - dense_2_acc_5: 0.5964 - dense_2_acc_6: 0.6630 - val_loss: 39.9355 - val_dense_2_loss_1: 3.2095 - val_dense_2_loss_2: 7.4504 - val_dense_2_loss_3: 7.5555 - val_dense_2_loss_4: 8.0828 - val_dense_2_loss_5: 7.0530 - val_dense_2_loss_6: 6.5843 - val_dense_2_acc_1: 0.5439 - val_dense_2_acc_2: 0.2281 - val_dense_2_acc_3: 0.1930 - val_dense_2_acc_4: 0.1579 - val_dense_2_acc_5: 0.2281 - val_dense_2_acc_6: 0.2632\n",
      "Epoch 150/200\n",
      "2800/2810 [============================>.] - ETA: 0s - loss: 7.7220 - dense_2_loss_1: 0.8269 - dense_2_loss_2: 1.1902 - dense_2_loss_3: 1.3629 - dense_2_loss_4: 1.5286 - dense_2_loss_5: 1.5076 - dense_2_loss_6: 1.3057 - dense_2_acc_1: 0.7518 - dense_2_acc_2: 0.6596 - dense_2_acc_3: 0.6218 - dense_2_acc_4: 0.5871 - dense_2_acc_5: 0.6057 - dense_2_acc_6: 0.6686Epoch 00149: saving model to main_model_weights_new.h5\n",
      "2810/2810 [==============================] - 129s - loss: 7.7441 - dense_2_loss_1: 0.8288 - dense_2_loss_2: 1.1908 - dense_2_loss_3: 1.3650 - dense_2_loss_4: 1.5374 - dense_2_loss_5: 1.5132 - dense_2_loss_6: 1.3089 - dense_2_acc_1: 0.7516 - dense_2_acc_2: 0.6594 - dense_2_acc_3: 0.6210 - dense_2_acc_4: 0.5865 - dense_2_acc_5: 0.6053 - dense_2_acc_6: 0.6690 - val_loss: 38.4834 - val_dense_2_loss_1: 3.0425 - val_dense_2_loss_2: 7.4597 - val_dense_2_loss_3: 7.2578 - val_dense_2_loss_4: 7.9472 - val_dense_2_loss_5: 6.7890 - val_dense_2_loss_6: 5.9872 - val_dense_2_acc_1: 0.5088 - val_dense_2_acc_2: 0.1754 - val_dense_2_acc_3: 0.1930 - val_dense_2_acc_4: 0.0877 - val_dense_2_acc_5: 0.2456 - val_dense_2_acc_6: 0.3158\n",
      "Epoch 151/200\n",
      "2800/2810 [============================>.] - ETA: 0s - loss: 7.9229 - dense_2_loss_1: 0.8356 - dense_2_loss_2: 1.2286 - dense_2_loss_3: 1.3653 - dense_2_loss_4: 1.6080 - dense_2_loss_5: 1.5429 - dense_2_loss_6: 1.3426 - dense_2_acc_1: 0.7571 - dense_2_acc_2: 0.6457 - dense_2_acc_3: 0.6254 - dense_2_acc_4: 0.5636 - dense_2_acc_5: 0.5943 - dense_2_acc_6: 0.6689Epoch 00150: saving model to main_model_weights_new.h5\n",
      "2810/2810 [==============================] - 128s - loss: 7.9349 - dense_2_loss_1: 0.8345 - dense_2_loss_2: 1.2302 - dense_2_loss_3: 1.3689 - dense_2_loss_4: 1.6094 - dense_2_loss_5: 1.5443 - dense_2_loss_6: 1.3476 - dense_2_acc_1: 0.7577 - dense_2_acc_2: 0.6456 - dense_2_acc_3: 0.6242 - dense_2_acc_4: 0.5630 - dense_2_acc_5: 0.5940 - dense_2_acc_6: 0.6676 - val_loss: 40.7588 - val_dense_2_loss_1: 3.5009 - val_dense_2_loss_2: 8.0135 - val_dense_2_loss_3: 7.5059 - val_dense_2_loss_4: 8.3326 - val_dense_2_loss_5: 7.1712 - val_dense_2_loss_6: 6.2347 - val_dense_2_acc_1: 0.5088 - val_dense_2_acc_2: 0.1930 - val_dense_2_acc_3: 0.2456 - val_dense_2_acc_4: 0.1228 - val_dense_2_acc_5: 0.2632 - val_dense_2_acc_6: 0.4035\n",
      "Epoch 152/200\n",
      "2800/2810 [============================>.] - ETA: 0s - loss: 7.4999 - dense_2_loss_1: 0.8171 - dense_2_loss_2: 1.1524 - dense_2_loss_3: 1.3288 - dense_2_loss_4: 1.5136 - dense_2_loss_5: 1.4558 - dense_2_loss_6: 1.2322 - dense_2_acc_1: 0.7600 - dense_2_acc_2: 0.6732 - dense_2_acc_3: 0.6264 - dense_2_acc_4: 0.5843 - dense_2_acc_5: 0.6104 - dense_2_acc_6: 0.6825Epoch 00151: saving model to main_model_weights_new.h5\n",
      "2810/2810 [==============================] - 127s - loss: 7.5069 - dense_2_loss_1: 0.8171 - dense_2_loss_2: 1.1511 - dense_2_loss_3: 1.3308 - dense_2_loss_4: 1.5161 - dense_2_loss_5: 1.4573 - dense_2_loss_6: 1.2346 - dense_2_acc_1: 0.7601 - dense_2_acc_2: 0.6733 - dense_2_acc_3: 0.6260 - dense_2_acc_4: 0.5843 - dense_2_acc_5: 0.6100 - dense_2_acc_6: 0.6822 - val_loss: 43.1049 - val_dense_2_loss_1: 3.9124 - val_dense_2_loss_2: 8.3420 - val_dense_2_loss_3: 8.3561 - val_dense_2_loss_4: 8.4864 - val_dense_2_loss_5: 7.2678 - val_dense_2_loss_6: 6.7402 - val_dense_2_acc_1: 0.5263 - val_dense_2_acc_2: 0.1930 - val_dense_2_acc_3: 0.1579 - val_dense_2_acc_4: 0.1930 - val_dense_2_acc_5: 0.2632 - val_dense_2_acc_6: 0.3158\n",
      "Epoch 153/200\n",
      "2800/2810 [============================>.] - ETA: 0s - loss: 7.7368 - dense_2_loss_1: 0.8313 - dense_2_loss_2: 1.2111 - dense_2_loss_3: 1.3223 - dense_2_loss_4: 1.5658 - dense_2_loss_5: 1.4502 - dense_2_loss_6: 1.3561 - dense_2_acc_1: 0.7507 - dense_2_acc_2: 0.6554 - dense_2_acc_3: 0.6293 - dense_2_acc_4: 0.5779 - dense_2_acc_5: 0.6111 - dense_2_acc_6: 0.6582Epoch 00152: saving model to main_model_weights_new.h5\n",
      "2810/2810 [==============================] - 127s - loss: 7.7365 - dense_2_loss_1: 0.8296 - dense_2_loss_2: 1.2135 - dense_2_loss_3: 1.3230 - dense_2_loss_4: 1.5649 - dense_2_loss_5: 1.4487 - dense_2_loss_6: 1.3568 - dense_2_acc_1: 0.7512 - dense_2_acc_2: 0.6555 - dense_2_acc_3: 0.6295 - dense_2_acc_4: 0.5779 - dense_2_acc_5: 0.6114 - dense_2_acc_6: 0.6580 - val_loss: 39.1211 - val_dense_2_loss_1: 3.0111 - val_dense_2_loss_2: 7.5045 - val_dense_2_loss_3: 7.5118 - val_dense_2_loss_4: 7.9222 - val_dense_2_loss_5: 6.9567 - val_dense_2_loss_6: 6.2148 - val_dense_2_acc_1: 0.4561 - val_dense_2_acc_2: 0.1930 - val_dense_2_acc_3: 0.1754 - val_dense_2_acc_4: 0.1579 - val_dense_2_acc_5: 0.2632 - val_dense_2_acc_6: 0.3860\n",
      "Epoch 154/200\n",
      "2800/2810 [============================>.] - ETA: 0s - loss: 7.7122 - dense_2_loss_1: 0.8324 - dense_2_loss_2: 1.1866 - dense_2_loss_3: 1.3287 - dense_2_loss_4: 1.5672 - dense_2_loss_5: 1.4784 - dense_2_loss_6: 1.3189 - dense_2_acc_1: 0.7568 - dense_2_acc_2: 0.6689 - dense_2_acc_3: 0.6311 - dense_2_acc_4: 0.5832 - dense_2_acc_5: 0.6011 - dense_2_acc_6: 0.6775Epoch 00153: saving model to main_model_weights_new.h5\n",
      "2810/2810 [==============================] - 128s - loss: 7.7127 - dense_2_loss_1: 0.8323 - dense_2_loss_2: 1.1839 - dense_2_loss_3: 1.3316 - dense_2_loss_4: 1.5696 - dense_2_loss_5: 1.4765 - dense_2_loss_6: 1.3189 - dense_2_acc_1: 0.7573 - dense_2_acc_2: 0.6694 - dense_2_acc_3: 0.6299 - dense_2_acc_4: 0.5822 - dense_2_acc_5: 0.6018 - dense_2_acc_6: 0.6772 - val_loss: 40.2350 - val_dense_2_loss_1: 3.1241 - val_dense_2_loss_2: 7.7688 - val_dense_2_loss_3: 7.3011 - val_dense_2_loss_4: 8.4635 - val_dense_2_loss_5: 7.3318 - val_dense_2_loss_6: 6.2457 - val_dense_2_acc_1: 0.5263 - val_dense_2_acc_2: 0.1754 - val_dense_2_acc_3: 0.1930 - val_dense_2_acc_4: 0.1754 - val_dense_2_acc_5: 0.2281 - val_dense_2_acc_6: 0.3509\n",
      "Epoch 155/200\n",
      "2800/2810 [============================>.] - ETA: 0s - loss: 7.6441 - dense_2_loss_1: 0.8060 - dense_2_loss_2: 1.1940 - dense_2_loss_3: 1.3196 - dense_2_loss_4: 1.5393 - dense_2_loss_5: 1.5083 - dense_2_loss_6: 1.2770 - dense_2_acc_1: 0.7643 - dense_2_acc_2: 0.6564 - dense_2_acc_3: 0.6279 - dense_2_acc_4: 0.5796 - dense_2_acc_5: 0.6068 - dense_2_acc_6: 0.6786Epoch 00154: saving model to main_model_weights_new.h5\n",
      "2810/2810 [==============================] - 128s - loss: 7.6394 - dense_2_loss_1: 0.8059 - dense_2_loss_2: 1.1958 - dense_2_loss_3: 1.3202 - dense_2_loss_4: 1.5386 - dense_2_loss_5: 1.5057 - dense_2_loss_6: 1.2731 - dense_2_acc_1: 0.7644 - dense_2_acc_2: 0.6555 - dense_2_acc_3: 0.6278 - dense_2_acc_4: 0.5804 - dense_2_acc_5: 0.6071 - dense_2_acc_6: 0.6794 - val_loss: 39.7312 - val_dense_2_loss_1: 3.1593 - val_dense_2_loss_2: 7.3004 - val_dense_2_loss_3: 7.4665 - val_dense_2_loss_4: 8.4133 - val_dense_2_loss_5: 7.0133 - val_dense_2_loss_6: 6.3784 - val_dense_2_acc_1: 0.5439 - val_dense_2_acc_2: 0.2105 - val_dense_2_acc_3: 0.1754 - val_dense_2_acc_4: 0.1228 - val_dense_2_acc_5: 0.2281 - val_dense_2_acc_6: 0.3509\n",
      "Epoch 156/200\n",
      "2800/2810 [============================>.] - ETA: 0s - loss: 7.4738 - dense_2_loss_1: 0.8346 - dense_2_loss_2: 1.1494 - dense_2_loss_3: 1.3051 - dense_2_loss_4: 1.5348 - dense_2_loss_5: 1.4325 - dense_2_loss_6: 1.2173 - dense_2_acc_1: 0.7604 - dense_2_acc_2: 0.6661 - dense_2_acc_3: 0.6454 - dense_2_acc_4: 0.5986 - dense_2_acc_5: 0.6246 - dense_2_acc_6: 0.6964Epoch 00155: saving model to main_model_weights_new.h5\n",
      "2810/2810 [==============================] - 128s - loss: 7.4882 - dense_2_loss_1: 0.8354 - dense_2_loss_2: 1.1515 - dense_2_loss_3: 1.3094 - dense_2_loss_4: 1.5372 - dense_2_loss_5: 1.4360 - dense_2_loss_6: 1.2187 - dense_2_acc_1: 0.7605 - dense_2_acc_2: 0.6662 - dense_2_acc_3: 0.6452 - dense_2_acc_4: 0.5986 - dense_2_acc_5: 0.6235 - dense_2_acc_6: 0.6961 - val_loss: 41.8075 - val_dense_2_loss_1: 3.0384 - val_dense_2_loss_2: 7.6656 - val_dense_2_loss_3: 7.9240 - val_dense_2_loss_4: 8.8767 - val_dense_2_loss_5: 7.4606 - val_dense_2_loss_6: 6.8422 - val_dense_2_acc_1: 0.4561 - val_dense_2_acc_2: 0.2281 - val_dense_2_acc_3: 0.1754 - val_dense_2_acc_4: 0.1754 - val_dense_2_acc_5: 0.2632 - val_dense_2_acc_6: 0.3860\n",
      "Epoch 157/200\n",
      "2800/2810 [============================>.] - ETA: 0s - loss: 7.3916 - dense_2_loss_1: 0.7914 - dense_2_loss_2: 1.1280 - dense_2_loss_3: 1.2746 - dense_2_loss_4: 1.4709 - dense_2_loss_5: 1.4249 - dense_2_loss_6: 1.3019 - dense_2_acc_1: 0.7571 - dense_2_acc_2: 0.6893 - dense_2_acc_3: 0.6468 - dense_2_acc_4: 0.6157 - dense_2_acc_5: 0.6139 - dense_2_acc_6: 0.6743Epoch 00156: saving model to main_model_weights_new.h5\n",
      "2810/2810 [==============================] - 129s - loss: 7.4177 - dense_2_loss_1: 0.7930 - dense_2_loss_2: 1.1342 - dense_2_loss_3: 1.2805 - dense_2_loss_4: 1.4756 - dense_2_loss_5: 1.4318 - dense_2_loss_6: 1.3027 - dense_2_acc_1: 0.7562 - dense_2_acc_2: 0.6883 - dense_2_acc_3: 0.6459 - dense_2_acc_4: 0.6149 - dense_2_acc_5: 0.6135 - dense_2_acc_6: 0.6740 - val_loss: 38.3792 - val_dense_2_loss_1: 3.0894 - val_dense_2_loss_2: 7.1534 - val_dense_2_loss_3: 7.3330 - val_dense_2_loss_4: 8.1031 - val_dense_2_loss_5: 6.7490 - val_dense_2_loss_6: 5.9512 - val_dense_2_acc_1: 0.5439 - val_dense_2_acc_2: 0.2105 - val_dense_2_acc_3: 0.2105 - val_dense_2_acc_4: 0.1404 - val_dense_2_acc_5: 0.2632 - val_dense_2_acc_6: 0.3509\n",
      "Epoch 158/200\n",
      "2800/2810 [============================>.] - ETA: 0s - loss: 7.5378 - dense_2_loss_1: 0.8067 - dense_2_loss_2: 1.1742 - dense_2_loss_3: 1.2971 - dense_2_loss_4: 1.4800 - dense_2_loss_5: 1.4464 - dense_2_loss_6: 1.3334 - dense_2_acc_1: 0.7704 - dense_2_acc_2: 0.6696 - dense_2_acc_3: 0.6296 - dense_2_acc_4: 0.5957 - dense_2_acc_5: 0.6207 - dense_2_acc_6: 0.6757Epoch 00157: saving model to main_model_weights_new.h5\n",
      "2810/2810 [==============================] - 128s - loss: 7.5281 - dense_2_loss_1: 0.8065 - dense_2_loss_2: 1.1717 - dense_2_loss_3: 1.2960 - dense_2_loss_4: 1.4788 - dense_2_loss_5: 1.4443 - dense_2_loss_6: 1.3307 - dense_2_acc_1: 0.7705 - dense_2_acc_2: 0.6705 - dense_2_acc_3: 0.6302 - dense_2_acc_4: 0.5961 - dense_2_acc_5: 0.6210 - dense_2_acc_6: 0.6762 - val_loss: 40.2514 - val_dense_2_loss_1: 3.0272 - val_dense_2_loss_2: 7.5989 - val_dense_2_loss_3: 7.5638 - val_dense_2_loss_4: 8.2632 - val_dense_2_loss_5: 7.2238 - val_dense_2_loss_6: 6.5745 - val_dense_2_acc_1: 0.5614 - val_dense_2_acc_2: 0.2105 - val_dense_2_acc_3: 0.1930 - val_dense_2_acc_4: 0.1754 - val_dense_2_acc_5: 0.2281 - val_dense_2_acc_6: 0.3333\n",
      "Epoch 159/200\n",
      "2800/2810 [============================>.] - ETA: 0s - loss: 7.2708 - dense_2_loss_1: 0.8003 - dense_2_loss_2: 1.1199 - dense_2_loss_3: 1.2633 - dense_2_loss_4: 1.4585 - dense_2_loss_5: 1.4029 - dense_2_loss_6: 1.2258 - dense_2_acc_1: 0.7618 - dense_2_acc_2: 0.6900 - dense_2_acc_3: 0.6482 - dense_2_acc_4: 0.5971 - dense_2_acc_5: 0.6325 - dense_2_acc_6: 0.6725Epoch 00158: saving model to main_model_weights_new.h5\n",
      "2810/2810 [==============================] - 127s - loss: 7.2683 - dense_2_loss_1: 0.7990 - dense_2_loss_2: 1.1214 - dense_2_loss_3: 1.2631 - dense_2_loss_4: 1.4568 - dense_2_loss_5: 1.4048 - dense_2_loss_6: 1.2232 - dense_2_acc_1: 0.7619 - dense_2_acc_2: 0.6897 - dense_2_acc_3: 0.6488 - dense_2_acc_4: 0.5975 - dense_2_acc_5: 0.6320 - dense_2_acc_6: 0.6730 - val_loss: 40.8875 - val_dense_2_loss_1: 3.4279 - val_dense_2_loss_2: 7.7335 - val_dense_2_loss_3: 7.4956 - val_dense_2_loss_4: 8.3254 - val_dense_2_loss_5: 7.4490 - val_dense_2_loss_6: 6.4561 - val_dense_2_acc_1: 0.5263 - val_dense_2_acc_2: 0.2105 - val_dense_2_acc_3: 0.2281 - val_dense_2_acc_4: 0.1404 - val_dense_2_acc_5: 0.2632 - val_dense_2_acc_6: 0.3333\n",
      "Epoch 160/200\n",
      "2800/2810 [============================>.] - ETA: 0s - loss: 7.3764 - dense_2_loss_1: 0.7776 - dense_2_loss_2: 1.1124 - dense_2_loss_3: 1.3108 - dense_2_loss_4: 1.4590 - dense_2_loss_5: 1.4651 - dense_2_loss_6: 1.2515 - dense_2_acc_1: 0.7611 - dense_2_acc_2: 0.6879 - dense_2_acc_3: 0.6379 - dense_2_acc_4: 0.6068 - dense_2_acc_5: 0.6232 - dense_2_acc_6: 0.6914Epoch 00159: saving model to main_model_weights_new.h5\n",
      "2810/2810 [==============================] - 129s - loss: 7.3986 - dense_2_loss_1: 0.7772 - dense_2_loss_2: 1.1105 - dense_2_loss_3: 1.3146 - dense_2_loss_4: 1.4633 - dense_2_loss_5: 1.4783 - dense_2_loss_6: 1.2547 - dense_2_acc_1: 0.7612 - dense_2_acc_2: 0.6886 - dense_2_acc_3: 0.6370 - dense_2_acc_4: 0.6064 - dense_2_acc_5: 0.6221 - dense_2_acc_6: 0.6907 - val_loss: 39.1028 - val_dense_2_loss_1: 3.0081 - val_dense_2_loss_2: 7.4232 - val_dense_2_loss_3: 7.4966 - val_dense_2_loss_4: 7.8690 - val_dense_2_loss_5: 7.1000 - val_dense_2_loss_6: 6.2059 - val_dense_2_acc_1: 0.5263 - val_dense_2_acc_2: 0.2456 - val_dense_2_acc_3: 0.1754 - val_dense_2_acc_4: 0.1754 - val_dense_2_acc_5: 0.1754 - val_dense_2_acc_6: 0.2982\n",
      "Epoch 161/200\n",
      "2800/2810 [============================>.] - ETA: 0s - loss: 7.2061 - dense_2_loss_1: 0.7822 - dense_2_loss_2: 1.1383 - dense_2_loss_3: 1.2203 - dense_2_loss_4: 1.4138 - dense_2_loss_5: 1.4443 - dense_2_loss_6: 1.2073 - dense_2_acc_1: 0.7761 - dense_2_acc_2: 0.6871 - dense_2_acc_3: 0.6639 - dense_2_acc_4: 0.6182 - dense_2_acc_5: 0.6214 - dense_2_acc_6: 0.6936Epoch 00160: saving model to main_model_weights_new.h5\n",
      "2810/2810 [==============================] - 128s - loss: 7.2061 - dense_2_loss_1: 0.7818 - dense_2_loss_2: 1.1396 - dense_2_loss_3: 1.2191 - dense_2_loss_4: 1.4152 - dense_2_loss_5: 1.4440 - dense_2_loss_6: 1.2064 - dense_2_acc_1: 0.7765 - dense_2_acc_2: 0.6872 - dense_2_acc_3: 0.6644 - dense_2_acc_4: 0.6178 - dense_2_acc_5: 0.6217 - dense_2_acc_6: 0.6929 - val_loss: 39.7597 - val_dense_2_loss_1: 2.9805 - val_dense_2_loss_2: 7.7763 - val_dense_2_loss_3: 7.4551 - val_dense_2_loss_4: 7.9097 - val_dense_2_loss_5: 7.2805 - val_dense_2_loss_6: 6.3577 - val_dense_2_acc_1: 0.5263 - val_dense_2_acc_2: 0.1930 - val_dense_2_acc_3: 0.1930 - val_dense_2_acc_4: 0.2105 - val_dense_2_acc_5: 0.2807 - val_dense_2_acc_6: 0.3860\n",
      "Epoch 162/200\n",
      "2800/2810 [============================>.] - ETA: 0s - loss: 7.2321 - dense_2_loss_1: 0.7502 - dense_2_loss_2: 1.1487 - dense_2_loss_3: 1.2332 - dense_2_loss_4: 1.4867 - dense_2_loss_5: 1.4115 - dense_2_loss_6: 1.2018 - dense_2_acc_1: 0.7754 - dense_2_acc_2: 0.6836 - dense_2_acc_3: 0.6496 - dense_2_acc_4: 0.6075 - dense_2_acc_5: 0.6246 - dense_2_acc_6: 0.6954Epoch 00161: saving model to main_model_weights_new.h5\n",
      "2810/2810 [==============================] - 128s - loss: 7.2359 - dense_2_loss_1: 0.7491 - dense_2_loss_2: 1.1469 - dense_2_loss_3: 1.2311 - dense_2_loss_4: 1.4899 - dense_2_loss_5: 1.4180 - dense_2_loss_6: 1.2008 - dense_2_acc_1: 0.7758 - dense_2_acc_2: 0.6843 - dense_2_acc_3: 0.6502 - dense_2_acc_4: 0.6068 - dense_2_acc_5: 0.6242 - dense_2_acc_6: 0.6950 - val_loss: 40.1661 - val_dense_2_loss_1: 3.2877 - val_dense_2_loss_2: 7.4429 - val_dense_2_loss_3: 7.8710 - val_dense_2_loss_4: 8.1686 - val_dense_2_loss_5: 6.8942 - val_dense_2_loss_6: 6.5016 - val_dense_2_acc_1: 0.5088 - val_dense_2_acc_2: 0.2105 - val_dense_2_acc_3: 0.1754 - val_dense_2_acc_4: 0.1404 - val_dense_2_acc_5: 0.2456 - val_dense_2_acc_6: 0.3158\n",
      "Epoch 163/200\n",
      "2800/2810 [============================>.] - ETA: 0s - loss: 7.0920 - dense_2_loss_1: 0.8084 - dense_2_loss_2: 1.0870 - dense_2_loss_3: 1.2516 - dense_2_loss_4: 1.3915 - dense_2_loss_5: 1.3549 - dense_2_loss_6: 1.1986 - dense_2_acc_1: 0.7618 - dense_2_acc_2: 0.6961 - dense_2_acc_3: 0.6539 - dense_2_acc_4: 0.6104 - dense_2_acc_5: 0.6364 - dense_2_acc_6: 0.6986Epoch 00162: saving model to main_model_weights_new.h5\n",
      "2810/2810 [==============================] - 129s - loss: 7.1139 - dense_2_loss_1: 0.8075 - dense_2_loss_2: 1.0904 - dense_2_loss_3: 1.2544 - dense_2_loss_4: 1.3959 - dense_2_loss_5: 1.3596 - dense_2_loss_6: 1.2060 - dense_2_acc_1: 0.7619 - dense_2_acc_2: 0.6954 - dense_2_acc_3: 0.6527 - dense_2_acc_4: 0.6100 - dense_2_acc_5: 0.6349 - dense_2_acc_6: 0.6975 - val_loss: 39.2498 - val_dense_2_loss_1: 2.8445 - val_dense_2_loss_2: 7.3794 - val_dense_2_loss_3: 7.5473 - val_dense_2_loss_4: 7.9230 - val_dense_2_loss_5: 6.8269 - val_dense_2_loss_6: 6.7286 - val_dense_2_acc_1: 0.5439 - val_dense_2_acc_2: 0.1930 - val_dense_2_acc_3: 0.1228 - val_dense_2_acc_4: 0.1053 - val_dense_2_acc_5: 0.1579 - val_dense_2_acc_6: 0.2632\n",
      "Epoch 164/200\n",
      "2800/2810 [============================>.] - ETA: 0s - loss: 7.1463 - dense_2_loss_1: 0.8146 - dense_2_loss_2: 1.0927 - dense_2_loss_3: 1.2122 - dense_2_loss_4: 1.4395 - dense_2_loss_5: 1.3858 - dense_2_loss_6: 1.2015 - dense_2_acc_1: 0.7661 - dense_2_acc_2: 0.6961 - dense_2_acc_3: 0.6618 - dense_2_acc_4: 0.6139 - dense_2_acc_5: 0.6343 - dense_2_acc_6: 0.7036Epoch 00163: saving model to main_model_weights_new.h5\n",
      "2810/2810 [==============================] - 128s - loss: 7.1414 - dense_2_loss_1: 0.8144 - dense_2_loss_2: 1.0923 - dense_2_loss_3: 1.2124 - dense_2_loss_4: 1.4371 - dense_2_loss_5: 1.3850 - dense_2_loss_6: 1.2002 - dense_2_acc_1: 0.7658 - dense_2_acc_2: 0.6957 - dense_2_acc_3: 0.6616 - dense_2_acc_4: 0.6146 - dense_2_acc_5: 0.6342 - dense_2_acc_6: 0.7036 - val_loss: 40.6876 - val_dense_2_loss_1: 3.0806 - val_dense_2_loss_2: 7.4996 - val_dense_2_loss_3: 7.8141 - val_dense_2_loss_4: 8.0165 - val_dense_2_loss_5: 7.2894 - val_dense_2_loss_6: 6.9874 - val_dense_2_acc_1: 0.5439 - val_dense_2_acc_2: 0.2105 - val_dense_2_acc_3: 0.1754 - val_dense_2_acc_4: 0.1754 - val_dense_2_acc_5: 0.2456 - val_dense_2_acc_6: 0.3509\n",
      "Epoch 165/200\n",
      "2800/2810 [============================>.] - ETA: 0s - loss: 7.0891 - dense_2_loss_1: 0.7700 - dense_2_loss_2: 1.0936 - dense_2_loss_3: 1.1653 - dense_2_loss_4: 1.4498 - dense_2_loss_5: 1.4192 - dense_2_loss_6: 1.1912 - dense_2_acc_1: 0.7679 - dense_2_acc_2: 0.6936 - dense_2_acc_3: 0.6675 - dense_2_acc_4: 0.6189 - dense_2_acc_5: 0.6236 - dense_2_acc_6: 0.7032Epoch 00164: saving model to main_model_weights_new.h5\n",
      "2810/2810 [==============================] - 128s - loss: 7.0807 - dense_2_loss_1: 0.7698 - dense_2_loss_2: 1.0902 - dense_2_loss_3: 1.1631 - dense_2_loss_4: 1.4472 - dense_2_loss_5: 1.4191 - dense_2_loss_6: 1.1912 - dense_2_acc_1: 0.7680 - dense_2_acc_2: 0.6947 - dense_2_acc_3: 0.6683 - dense_2_acc_4: 0.6192 - dense_2_acc_5: 0.6235 - dense_2_acc_6: 0.7032 - val_loss: 41.4185 - val_dense_2_loss_1: 3.2501 - val_dense_2_loss_2: 7.9194 - val_dense_2_loss_3: 7.8562 - val_dense_2_loss_4: 8.2711 - val_dense_2_loss_5: 7.3192 - val_dense_2_loss_6: 6.8024 - val_dense_2_acc_1: 0.5439 - val_dense_2_acc_2: 0.2281 - val_dense_2_acc_3: 0.1754 - val_dense_2_acc_4: 0.1754 - val_dense_2_acc_5: 0.2456 - val_dense_2_acc_6: 0.3333\n",
      "Epoch 166/200\n",
      "2800/2810 [============================>.] - ETA: 0s - loss: 6.9038 - dense_2_loss_1: 0.7406 - dense_2_loss_2: 1.0994 - dense_2_loss_3: 1.1770 - dense_2_loss_4: 1.3781 - dense_2_loss_5: 1.3359 - dense_2_loss_6: 1.1728 - dense_2_acc_1: 0.7768 - dense_2_acc_2: 0.7068 - dense_2_acc_3: 0.6661 - dense_2_acc_4: 0.6179 - dense_2_acc_5: 0.6500 - dense_2_acc_6: 0.7050Epoch 00165: saving model to main_model_weights_new.h5\n",
      "2810/2810 [==============================] - 128s - loss: 6.9078 - dense_2_loss_1: 0.7416 - dense_2_loss_2: 1.0996 - dense_2_loss_3: 1.1773 - dense_2_loss_4: 1.3778 - dense_2_loss_5: 1.3374 - dense_2_loss_6: 1.1742 - dense_2_acc_1: 0.7769 - dense_2_acc_2: 0.7068 - dense_2_acc_3: 0.6658 - dense_2_acc_4: 0.6174 - dense_2_acc_5: 0.6498 - dense_2_acc_6: 0.7046 - val_loss: 39.6882 - val_dense_2_loss_1: 3.0881 - val_dense_2_loss_2: 7.6388 - val_dense_2_loss_3: 7.5994 - val_dense_2_loss_4: 8.0619 - val_dense_2_loss_5: 7.2498 - val_dense_2_loss_6: 6.0502 - val_dense_2_acc_1: 0.5789 - val_dense_2_acc_2: 0.2105 - val_dense_2_acc_3: 0.2456 - val_dense_2_acc_4: 0.1404 - val_dense_2_acc_5: 0.2456 - val_dense_2_acc_6: 0.2982\n",
      "Epoch 167/200\n",
      "2800/2810 [============================>.] - ETA: 0s - loss: 6.7423 - dense_2_loss_1: 0.7531 - dense_2_loss_2: 1.0204 - dense_2_loss_3: 1.1439 - dense_2_loss_4: 1.3466 - dense_2_loss_5: 1.3176 - dense_2_loss_6: 1.1608 - dense_2_acc_1: 0.7721 - dense_2_acc_2: 0.7107 - dense_2_acc_3: 0.6829 - dense_2_acc_4: 0.6332 - dense_2_acc_5: 0.6382 - dense_2_acc_6: 0.7079Epoch 00166: saving model to main_model_weights_new.h5\n",
      "2810/2810 [==============================] - 128s - loss: 6.7508 - dense_2_loss_1: 0.7532 - dense_2_loss_2: 1.0215 - dense_2_loss_3: 1.1434 - dense_2_loss_4: 1.3531 - dense_2_loss_5: 1.3166 - dense_2_loss_6: 1.1628 - dense_2_acc_1: 0.7719 - dense_2_acc_2: 0.7107 - dense_2_acc_3: 0.6826 - dense_2_acc_4: 0.6324 - dense_2_acc_5: 0.6384 - dense_2_acc_6: 0.7075 - val_loss: 41.8421 - val_dense_2_loss_1: 3.5252 - val_dense_2_loss_2: 8.1319 - val_dense_2_loss_3: 7.8446 - val_dense_2_loss_4: 8.5374 - val_dense_2_loss_5: 7.2340 - val_dense_2_loss_6: 6.5689 - val_dense_2_acc_1: 0.5088 - val_dense_2_acc_2: 0.2105 - val_dense_2_acc_3: 0.2105 - val_dense_2_acc_4: 0.1930 - val_dense_2_acc_5: 0.2456 - val_dense_2_acc_6: 0.3509\n",
      "Epoch 168/200\n",
      "2800/2810 [============================>.] - ETA: 0s - loss: 6.9153 - dense_2_loss_1: 0.7561 - dense_2_loss_2: 1.0369 - dense_2_loss_3: 1.1995 - dense_2_loss_4: 1.3639 - dense_2_loss_5: 1.3776 - dense_2_loss_6: 1.1813 - dense_2_acc_1: 0.7700 - dense_2_acc_2: 0.7025 - dense_2_acc_3: 0.6718 - dense_2_acc_4: 0.6064 - dense_2_acc_5: 0.6400 - dense_2_acc_6: 0.6932Epoch 00167: saving model to main_model_weights_new.h5\n",
      "2810/2810 [==============================] - 128s - loss: 6.9220 - dense_2_loss_1: 0.7564 - dense_2_loss_2: 1.0389 - dense_2_loss_3: 1.1998 - dense_2_loss_4: 1.3646 - dense_2_loss_5: 1.3780 - dense_2_loss_6: 1.1843 - dense_2_acc_1: 0.7701 - dense_2_acc_2: 0.7018 - dense_2_acc_3: 0.6719 - dense_2_acc_4: 0.6057 - dense_2_acc_5: 0.6399 - dense_2_acc_6: 0.6932 - val_loss: 41.5104 - val_dense_2_loss_1: 3.1618 - val_dense_2_loss_2: 7.8585 - val_dense_2_loss_3: 7.8328 - val_dense_2_loss_4: 8.2050 - val_dense_2_loss_5: 7.7934 - val_dense_2_loss_6: 6.6589 - val_dense_2_acc_1: 0.4737 - val_dense_2_acc_2: 0.2281 - val_dense_2_acc_3: 0.1579 - val_dense_2_acc_4: 0.1228 - val_dense_2_acc_5: 0.2281 - val_dense_2_acc_6: 0.3158\n",
      "Epoch 169/200\n",
      "2800/2810 [============================>.] - ETA: 0s - loss: 6.7899 - dense_2_loss_1: 0.7648 - dense_2_loss_2: 1.0478 - dense_2_loss_3: 1.1621 - dense_2_loss_4: 1.3387 - dense_2_loss_5: 1.3660 - dense_2_loss_6: 1.1105 - dense_2_acc_1: 0.7736 - dense_2_acc_2: 0.6964 - dense_2_acc_3: 0.6707 - dense_2_acc_4: 0.6261 - dense_2_acc_5: 0.6307 - dense_2_acc_6: 0.7229Epoch 00168: saving model to main_model_weights_new.h5\n",
      "2810/2810 [==============================] - 128s - loss: 6.8114 - dense_2_loss_1: 0.7646 - dense_2_loss_2: 1.0480 - dense_2_loss_3: 1.1640 - dense_2_loss_4: 1.3458 - dense_2_loss_5: 1.3760 - dense_2_loss_6: 1.1129 - dense_2_acc_1: 0.7740 - dense_2_acc_2: 0.6961 - dense_2_acc_3: 0.6701 - dense_2_acc_4: 0.6253 - dense_2_acc_5: 0.6299 - dense_2_acc_6: 0.7228 - val_loss: 41.0296 - val_dense_2_loss_1: 3.1736 - val_dense_2_loss_2: 7.7765 - val_dense_2_loss_3: 7.9113 - val_dense_2_loss_4: 8.0954 - val_dense_2_loss_5: 7.4406 - val_dense_2_loss_6: 6.6322 - val_dense_2_acc_1: 0.5614 - val_dense_2_acc_2: 0.1930 - val_dense_2_acc_3: 0.1930 - val_dense_2_acc_4: 0.1930 - val_dense_2_acc_5: 0.2105 - val_dense_2_acc_6: 0.2632\n",
      "Epoch 170/200\n",
      "2800/2810 [============================>.] - ETA: 0s - loss: 6.6755 - dense_2_loss_1: 0.7110 - dense_2_loss_2: 1.0382 - dense_2_loss_3: 1.1336 - dense_2_loss_4: 1.3382 - dense_2_loss_5: 1.3472 - dense_2_loss_6: 1.1073 - dense_2_acc_1: 0.7971 - dense_2_acc_2: 0.7068 - dense_2_acc_3: 0.6875 - dense_2_acc_4: 0.6325 - dense_2_acc_5: 0.6518 - dense_2_acc_6: 0.7229Epoch 00169: saving model to main_model_weights_new.h5\n",
      "2810/2810 [==============================] - 128s - loss: 6.6726 - dense_2_loss_1: 0.7101 - dense_2_loss_2: 1.0380 - dense_2_loss_3: 1.1340 - dense_2_loss_4: 1.3375 - dense_2_loss_5: 1.3472 - dense_2_loss_6: 1.1059 - dense_2_acc_1: 0.7975 - dense_2_acc_2: 0.7068 - dense_2_acc_3: 0.6872 - dense_2_acc_4: 0.6327 - dense_2_acc_5: 0.6512 - dense_2_acc_6: 0.7231 - val_loss: 40.0246 - val_dense_2_loss_1: 3.2046 - val_dense_2_loss_2: 7.6884 - val_dense_2_loss_3: 7.7167 - val_dense_2_loss_4: 7.8038 - val_dense_2_loss_5: 7.1721 - val_dense_2_loss_6: 6.4389 - val_dense_2_acc_1: 0.5439 - val_dense_2_acc_2: 0.1930 - val_dense_2_acc_3: 0.1754 - val_dense_2_acc_4: 0.1404 - val_dense_2_acc_5: 0.2632 - val_dense_2_acc_6: 0.3509\n",
      "Epoch 171/200\n",
      "2800/2810 [============================>.] - ETA: 0s - loss: 6.6648 - dense_2_loss_1: 0.7500 - dense_2_loss_2: 1.0146 - dense_2_loss_3: 1.1375 - dense_2_loss_4: 1.2940 - dense_2_loss_5: 1.3073 - dense_2_loss_6: 1.1615 - dense_2_acc_1: 0.7839 - dense_2_acc_2: 0.7068 - dense_2_acc_3: 0.6761 - dense_2_acc_4: 0.6529 - dense_2_acc_5: 0.6561 - dense_2_acc_6: 0.7132Epoch 00170: saving model to main_model_weights_new.h5\n",
      "2810/2810 [==============================] - 128s - loss: 6.6658 - dense_2_loss_1: 0.7513 - dense_2_loss_2: 1.0153 - dense_2_loss_3: 1.1372 - dense_2_loss_4: 1.2926 - dense_2_loss_5: 1.3060 - dense_2_loss_6: 1.1635 - dense_2_acc_1: 0.7836 - dense_2_acc_2: 0.7071 - dense_2_acc_3: 0.6762 - dense_2_acc_4: 0.6530 - dense_2_acc_5: 0.6562 - dense_2_acc_6: 0.7125 - val_loss: 40.0073 - val_dense_2_loss_1: 3.1530 - val_dense_2_loss_2: 8.0312 - val_dense_2_loss_3: 7.5849 - val_dense_2_loss_4: 7.6624 - val_dense_2_loss_5: 6.9847 - val_dense_2_loss_6: 6.5911 - val_dense_2_acc_1: 0.5263 - val_dense_2_acc_2: 0.1930 - val_dense_2_acc_3: 0.1754 - val_dense_2_acc_4: 0.1754 - val_dense_2_acc_5: 0.2105 - val_dense_2_acc_6: 0.3684\n",
      "Epoch 172/200\n",
      "2800/2810 [============================>.] - ETA: 0s - loss: 6.6262 - dense_2_loss_1: 0.7323 - dense_2_loss_2: 0.9863 - dense_2_loss_3: 1.1618 - dense_2_loss_4: 1.3062 - dense_2_loss_5: 1.3268 - dense_2_loss_6: 1.1128 - dense_2_acc_1: 0.7704 - dense_2_acc_2: 0.7168 - dense_2_acc_3: 0.6818 - dense_2_acc_4: 0.6289 - dense_2_acc_5: 0.6525 - dense_2_acc_6: 0.7068Epoch 00171: saving model to main_model_weights_new.h5\n",
      "2810/2810 [==============================] - 129s - loss: 6.6128 - dense_2_loss_1: 0.7317 - dense_2_loss_2: 0.9859 - dense_2_loss_3: 1.1583 - dense_2_loss_4: 1.3030 - dense_2_loss_5: 1.3237 - dense_2_loss_6: 1.1103 - dense_2_acc_1: 0.7705 - dense_2_acc_2: 0.7167 - dense_2_acc_3: 0.6829 - dense_2_acc_4: 0.6299 - dense_2_acc_5: 0.6534 - dense_2_acc_6: 0.7075 - val_loss: 40.7684 - val_dense_2_loss_1: 3.3436 - val_dense_2_loss_2: 7.9331 - val_dense_2_loss_3: 7.5127 - val_dense_2_loss_4: 8.3543 - val_dense_2_loss_5: 7.0958 - val_dense_2_loss_6: 6.5289 - val_dense_2_acc_1: 0.5439 - val_dense_2_acc_2: 0.2105 - val_dense_2_acc_3: 0.1754 - val_dense_2_acc_4: 0.1754 - val_dense_2_acc_5: 0.2281 - val_dense_2_acc_6: 0.3333\n",
      "Epoch 173/200\n",
      "2800/2810 [============================>.] - ETA: 0s - loss: 6.5808 - dense_2_loss_1: 0.7157 - dense_2_loss_2: 1.0506 - dense_2_loss_3: 1.1674 - dense_2_loss_4: 1.3044 - dense_2_loss_5: 1.2304 - dense_2_loss_6: 1.1123 - dense_2_acc_1: 0.7889 - dense_2_acc_2: 0.6993 - dense_2_acc_3: 0.6764 - dense_2_acc_4: 0.6496 - dense_2_acc_5: 0.6664 - dense_2_acc_6: 0.7289Epoch 00172: saving model to main_model_weights_new.h5\n",
      "2810/2810 [==============================] - 129s - loss: 6.5689 - dense_2_loss_1: 0.7139 - dense_2_loss_2: 1.0492 - dense_2_loss_3: 1.1650 - dense_2_loss_4: 1.3020 - dense_2_loss_5: 1.2270 - dense_2_loss_6: 1.1118 - dense_2_acc_1: 0.7893 - dense_2_acc_2: 0.6993 - dense_2_acc_3: 0.6772 - dense_2_acc_4: 0.6502 - dense_2_acc_5: 0.6676 - dense_2_acc_6: 0.7281 - val_loss: 40.6123 - val_dense_2_loss_1: 3.2250 - val_dense_2_loss_2: 7.4254 - val_dense_2_loss_3: 7.6080 - val_dense_2_loss_4: 8.4661 - val_dense_2_loss_5: 7.5058 - val_dense_2_loss_6: 6.3820 - val_dense_2_acc_1: 0.5088 - val_dense_2_acc_2: 0.2105 - val_dense_2_acc_3: 0.1930 - val_dense_2_acc_4: 0.1754 - val_dense_2_acc_5: 0.2281 - val_dense_2_acc_6: 0.3684\n",
      "Epoch 174/200\n",
      "2800/2810 [============================>.] - ETA: 0s - loss: 6.4473 - dense_2_loss_1: 0.7163 - dense_2_loss_2: 0.9925 - dense_2_loss_3: 1.1231 - dense_2_loss_4: 1.2955 - dense_2_loss_5: 1.2511 - dense_2_loss_6: 1.0686 - dense_2_acc_1: 0.7857 - dense_2_acc_2: 0.7161 - dense_2_acc_3: 0.6939 - dense_2_acc_4: 0.6532 - dense_2_acc_5: 0.6614 - dense_2_acc_6: 0.7321Epoch 00173: saving model to main_model_weights_new.h5\n",
      "2810/2810 [==============================] - 129s - loss: 6.4468 - dense_2_loss_1: 0.7168 - dense_2_loss_2: 0.9936 - dense_2_loss_3: 1.1213 - dense_2_loss_4: 1.2933 - dense_2_loss_5: 1.2537 - dense_2_loss_6: 1.0682 - dense_2_acc_1: 0.7858 - dense_2_acc_2: 0.7153 - dense_2_acc_3: 0.6943 - dense_2_acc_4: 0.6537 - dense_2_acc_5: 0.6616 - dense_2_acc_6: 0.7324 - val_loss: 42.7749 - val_dense_2_loss_1: 3.3857 - val_dense_2_loss_2: 8.3688 - val_dense_2_loss_3: 7.9817 - val_dense_2_loss_4: 8.6373 - val_dense_2_loss_5: 7.8836 - val_dense_2_loss_6: 6.5178 - val_dense_2_acc_1: 0.5614 - val_dense_2_acc_2: 0.1930 - val_dense_2_acc_3: 0.1930 - val_dense_2_acc_4: 0.1404 - val_dense_2_acc_5: 0.2281 - val_dense_2_acc_6: 0.3333\n",
      "Epoch 175/200\n",
      "2800/2810 [============================>.] - ETA: 0s - loss: 6.4249 - dense_2_loss_1: 0.7002 - dense_2_loss_2: 0.9622 - dense_2_loss_3: 1.1266 - dense_2_loss_4: 1.2480 - dense_2_loss_5: 1.2937 - dense_2_loss_6: 1.0942 - dense_2_acc_1: 0.7932 - dense_2_acc_2: 0.7214 - dense_2_acc_3: 0.6836 - dense_2_acc_4: 0.6632 - dense_2_acc_5: 0.6614 - dense_2_acc_6: 0.7311Epoch 00174: saving model to main_model_weights_new.h5\n",
      "2810/2810 [==============================] - 128s - loss: 6.4350 - dense_2_loss_1: 0.6982 - dense_2_loss_2: 0.9629 - dense_2_loss_3: 1.1314 - dense_2_loss_4: 1.2505 - dense_2_loss_5: 1.2958 - dense_2_loss_6: 1.0961 - dense_2_acc_1: 0.7940 - dense_2_acc_2: 0.7214 - dense_2_acc_3: 0.6822 - dense_2_acc_4: 0.6630 - dense_2_acc_5: 0.6609 - dense_2_acc_6: 0.7310 - val_loss: 39.6313 - val_dense_2_loss_1: 3.0391 - val_dense_2_loss_2: 7.5839 - val_dense_2_loss_3: 7.4650 - val_dense_2_loss_4: 8.2446 - val_dense_2_loss_5: 6.8490 - val_dense_2_loss_6: 6.4496 - val_dense_2_acc_1: 0.5263 - val_dense_2_acc_2: 0.2105 - val_dense_2_acc_3: 0.2105 - val_dense_2_acc_4: 0.1404 - val_dense_2_acc_5: 0.2281 - val_dense_2_acc_6: 0.3860\n",
      "Epoch 176/200\n",
      "2800/2810 [============================>.] - ETA: 0s - loss: 6.3882 - dense_2_loss_1: 0.7192 - dense_2_loss_2: 0.9781 - dense_2_loss_3: 1.0787 - dense_2_loss_4: 1.2830 - dense_2_loss_5: 1.2890 - dense_2_loss_6: 1.0403 - dense_2_acc_1: 0.7889 - dense_2_acc_2: 0.7179 - dense_2_acc_3: 0.6914 - dense_2_acc_4: 0.6507 - dense_2_acc_5: 0.6650 - dense_2_acc_6: 0.7339Epoch 00175: saving model to main_model_weights_new.h5\n",
      "2810/2810 [==============================] - 129s - loss: 6.3845 - dense_2_loss_1: 0.7190 - dense_2_loss_2: 0.9766 - dense_2_loss_3: 1.0802 - dense_2_loss_4: 1.2839 - dense_2_loss_5: 1.2871 - dense_2_loss_6: 1.0377 - dense_2_acc_1: 0.7883 - dense_2_acc_2: 0.7185 - dense_2_acc_3: 0.6907 - dense_2_acc_4: 0.6505 - dense_2_acc_5: 0.6655 - dense_2_acc_6: 0.7345 - val_loss: 40.5186 - val_dense_2_loss_1: 2.9966 - val_dense_2_loss_2: 7.7258 - val_dense_2_loss_3: 7.6734 - val_dense_2_loss_4: 8.3209 - val_dense_2_loss_5: 7.3009 - val_dense_2_loss_6: 6.5009 - val_dense_2_acc_1: 0.5088 - val_dense_2_acc_2: 0.1930 - val_dense_2_acc_3: 0.1579 - val_dense_2_acc_4: 0.1579 - val_dense_2_acc_5: 0.1930 - val_dense_2_acc_6: 0.2982\n",
      "Epoch 177/200\n",
      "2800/2810 [============================>.] - ETA: 0s - loss: 6.3766 - dense_2_loss_1: 0.7133 - dense_2_loss_2: 0.9540 - dense_2_loss_3: 1.0647 - dense_2_loss_4: 1.2747 - dense_2_loss_5: 1.2745 - dense_2_loss_6: 1.0955 - dense_2_acc_1: 0.7900 - dense_2_acc_2: 0.7268 - dense_2_acc_3: 0.7089 - dense_2_acc_4: 0.6707 - dense_2_acc_5: 0.6689 - dense_2_acc_6: 0.7268Epoch 00176: saving model to main_model_weights_new.h5\n",
      "2810/2810 [==============================] - 129s - loss: 6.3809 - dense_2_loss_1: 0.7161 - dense_2_loss_2: 0.9559 - dense_2_loss_3: 1.0653 - dense_2_loss_4: 1.2756 - dense_2_loss_5: 1.2740 - dense_2_loss_6: 1.0939 - dense_2_acc_1: 0.7890 - dense_2_acc_2: 0.7263 - dense_2_acc_3: 0.7085 - dense_2_acc_4: 0.6701 - dense_2_acc_5: 0.6687 - dense_2_acc_6: 0.7267 - val_loss: 42.5943 - val_dense_2_loss_1: 3.2998 - val_dense_2_loss_2: 7.7707 - val_dense_2_loss_3: 7.7570 - val_dense_2_loss_4: 9.0224 - val_dense_2_loss_5: 7.8811 - val_dense_2_loss_6: 6.8633 - val_dense_2_acc_1: 0.4737 - val_dense_2_acc_2: 0.1754 - val_dense_2_acc_3: 0.1754 - val_dense_2_acc_4: 0.1228 - val_dense_2_acc_5: 0.2807 - val_dense_2_acc_6: 0.3509\n",
      "Epoch 178/200\n",
      "2800/2810 [============================>.] - ETA: 0s - loss: 6.4956 - dense_2_loss_1: 0.7043 - dense_2_loss_2: 0.9790 - dense_2_loss_3: 1.1090 - dense_2_loss_4: 1.3078 - dense_2_loss_5: 1.2801 - dense_2_loss_6: 1.1155 - dense_2_acc_1: 0.7879 - dense_2_acc_2: 0.7300 - dense_2_acc_3: 0.6836 - dense_2_acc_4: 0.6432 - dense_2_acc_5: 0.6571 - dense_2_acc_6: 0.7114Epoch 00177: saving model to main_model_weights_new.h5\n",
      "2810/2810 [==============================] - 128s - loss: 6.5038 - dense_2_loss_1: 0.7071 - dense_2_loss_2: 0.9809 - dense_2_loss_3: 1.1094 - dense_2_loss_4: 1.3076 - dense_2_loss_5: 1.2801 - dense_2_loss_6: 1.1188 - dense_2_acc_1: 0.7875 - dense_2_acc_2: 0.7299 - dense_2_acc_3: 0.6833 - dense_2_acc_4: 0.6434 - dense_2_acc_5: 0.6569 - dense_2_acc_6: 0.7107 - val_loss: 41.5784 - val_dense_2_loss_1: 3.1171 - val_dense_2_loss_2: 7.9122 - val_dense_2_loss_3: 8.0193 - val_dense_2_loss_4: 8.4580 - val_dense_2_loss_5: 7.4209 - val_dense_2_loss_6: 6.6508 - val_dense_2_acc_1: 0.5088 - val_dense_2_acc_2: 0.2105 - val_dense_2_acc_3: 0.1579 - val_dense_2_acc_4: 0.1404 - val_dense_2_acc_5: 0.2632 - val_dense_2_acc_6: 0.3684\n",
      "Epoch 179/200\n",
      "2800/2810 [============================>.] - ETA: 0s - loss: 6.2588 - dense_2_loss_1: 0.6928 - dense_2_loss_2: 0.9514 - dense_2_loss_3: 1.0454 - dense_2_loss_4: 1.2709 - dense_2_loss_5: 1.2497 - dense_2_loss_6: 1.0486 - dense_2_acc_1: 0.8018 - dense_2_acc_2: 0.7325 - dense_2_acc_3: 0.7157 - dense_2_acc_4: 0.6568 - dense_2_acc_5: 0.6679 - dense_2_acc_6: 0.7318Epoch 00178: saving model to main_model_weights_new.h5\n",
      "2810/2810 [==============================] - 129s - loss: 6.2572 - dense_2_loss_1: 0.6932 - dense_2_loss_2: 0.9495 - dense_2_loss_3: 1.0465 - dense_2_loss_4: 1.2717 - dense_2_loss_5: 1.2466 - dense_2_loss_6: 1.0497 - dense_2_acc_1: 0.8011 - dense_2_acc_2: 0.7331 - dense_2_acc_3: 0.7146 - dense_2_acc_4: 0.6562 - dense_2_acc_5: 0.6687 - dense_2_acc_6: 0.7317 - val_loss: 40.9049 - val_dense_2_loss_1: 3.1694 - val_dense_2_loss_2: 7.8784 - val_dense_2_loss_3: 7.8401 - val_dense_2_loss_4: 8.2276 - val_dense_2_loss_5: 7.3395 - val_dense_2_loss_6: 6.4499 - val_dense_2_acc_1: 0.5439 - val_dense_2_acc_2: 0.2105 - val_dense_2_acc_3: 0.2281 - val_dense_2_acc_4: 0.1754 - val_dense_2_acc_5: 0.2456 - val_dense_2_acc_6: 0.3684\n",
      "Epoch 180/200\n",
      "2800/2810 [============================>.] - ETA: 0s - loss: 6.2002 - dense_2_loss_1: 0.6689 - dense_2_loss_2: 0.9485 - dense_2_loss_3: 1.1045 - dense_2_loss_4: 1.2553 - dense_2_loss_5: 1.2180 - dense_2_loss_6: 1.0051 - dense_2_acc_1: 0.7961 - dense_2_acc_2: 0.7218 - dense_2_acc_3: 0.6986 - dense_2_acc_4: 0.6571 - dense_2_acc_5: 0.6682 - dense_2_acc_6: 0.7396Epoch 00179: saving model to main_model_weights_new.h5\n",
      "2810/2810 [==============================] - 127s - loss: 6.2054 - dense_2_loss_1: 0.6696 - dense_2_loss_2: 0.9504 - dense_2_loss_3: 1.1023 - dense_2_loss_4: 1.2571 - dense_2_loss_5: 1.2197 - dense_2_loss_6: 1.0062 - dense_2_acc_1: 0.7957 - dense_2_acc_2: 0.7206 - dense_2_acc_3: 0.6993 - dense_2_acc_4: 0.6566 - dense_2_acc_5: 0.6673 - dense_2_acc_6: 0.7391 - val_loss: 41.9253 - val_dense_2_loss_1: 3.3222 - val_dense_2_loss_2: 7.7675 - val_dense_2_loss_3: 7.7738 - val_dense_2_loss_4: 8.4969 - val_dense_2_loss_5: 7.8213 - val_dense_2_loss_6: 6.7436 - val_dense_2_acc_1: 0.5439 - val_dense_2_acc_2: 0.2105 - val_dense_2_acc_3: 0.1754 - val_dense_2_acc_4: 0.1579 - val_dense_2_acc_5: 0.2456 - val_dense_2_acc_6: 0.2982\n",
      "Epoch 181/200\n",
      "2800/2810 [============================>.] - ETA: 0s - loss: 6.2378 - dense_2_loss_1: 0.7133 - dense_2_loss_2: 0.9408 - dense_2_loss_3: 1.1194 - dense_2_loss_4: 1.1914 - dense_2_loss_5: 1.2110 - dense_2_loss_6: 1.0619 - dense_2_acc_1: 0.7818 - dense_2_acc_2: 0.7282 - dense_2_acc_3: 0.6861 - dense_2_acc_4: 0.6686 - dense_2_acc_5: 0.6750 - dense_2_acc_6: 0.7346Epoch 00180: saving model to main_model_weights_new.h5\n",
      "2810/2810 [==============================] - 127s - loss: 6.2323 - dense_2_loss_1: 0.7118 - dense_2_loss_2: 0.9406 - dense_2_loss_3: 1.1199 - dense_2_loss_4: 1.1912 - dense_2_loss_5: 1.2083 - dense_2_loss_6: 1.0604 - dense_2_acc_1: 0.7819 - dense_2_acc_2: 0.7278 - dense_2_acc_3: 0.6858 - dense_2_acc_4: 0.6683 - dense_2_acc_5: 0.6758 - dense_2_acc_6: 0.7349 - val_loss: 42.4865 - val_dense_2_loss_1: 3.3153 - val_dense_2_loss_2: 8.1733 - val_dense_2_loss_3: 7.8257 - val_dense_2_loss_4: 8.4754 - val_dense_2_loss_5: 7.5288 - val_dense_2_loss_6: 7.1680 - val_dense_2_acc_1: 0.5088 - val_dense_2_acc_2: 0.1930 - val_dense_2_acc_3: 0.1754 - val_dense_2_acc_4: 0.1754 - val_dense_2_acc_5: 0.2807 - val_dense_2_acc_6: 0.4211\n",
      "Epoch 182/200\n",
      "2800/2810 [============================>.] - ETA: 0s - loss: 6.2642 - dense_2_loss_1: 0.6909 - dense_2_loss_2: 0.9524 - dense_2_loss_3: 1.1515 - dense_2_loss_4: 1.2073 - dense_2_loss_5: 1.1924 - dense_2_loss_6: 1.0697 - dense_2_acc_1: 0.7993 - dense_2_acc_2: 0.7354 - dense_2_acc_3: 0.6904 - dense_2_acc_4: 0.6550 - dense_2_acc_5: 0.6811 - dense_2_acc_6: 0.7339Epoch 00181: saving model to main_model_weights_new.h5\n",
      "2810/2810 [==============================] - 128s - loss: 6.2628 - dense_2_loss_1: 0.6891 - dense_2_loss_2: 0.9514 - dense_2_loss_3: 1.1512 - dense_2_loss_4: 1.2077 - dense_2_loss_5: 1.1938 - dense_2_loss_6: 1.0696 - dense_2_acc_1: 0.8000 - dense_2_acc_2: 0.7356 - dense_2_acc_3: 0.6907 - dense_2_acc_4: 0.6544 - dense_2_acc_5: 0.6804 - dense_2_acc_6: 0.7342 - val_loss: 43.3680 - val_dense_2_loss_1: 3.3774 - val_dense_2_loss_2: 8.4411 - val_dense_2_loss_3: 8.2513 - val_dense_2_loss_4: 8.9858 - val_dense_2_loss_5: 7.5863 - val_dense_2_loss_6: 6.7261 - val_dense_2_acc_1: 0.5263 - val_dense_2_acc_2: 0.1930 - val_dense_2_acc_3: 0.1754 - val_dense_2_acc_4: 0.1404 - val_dense_2_acc_5: 0.2456 - val_dense_2_acc_6: 0.3509\n",
      "Epoch 183/200\n",
      "2800/2810 [============================>.] - ETA: 0s - loss: 6.0236 - dense_2_loss_1: 0.6674 - dense_2_loss_2: 0.8972 - dense_2_loss_3: 1.0450 - dense_2_loss_4: 1.2064 - dense_2_loss_5: 1.1797 - dense_2_loss_6: 1.0278 - dense_2_acc_1: 0.8018 - dense_2_acc_2: 0.7389 - dense_2_acc_3: 0.7082 - dense_2_acc_4: 0.6571 - dense_2_acc_5: 0.6868 - dense_2_acc_6: 0.7407Epoch 00182: saving model to main_model_weights_new.h5\n",
      "2810/2810 [==============================] - 127s - loss: 6.0250 - dense_2_loss_1: 0.6690 - dense_2_loss_2: 0.8964 - dense_2_loss_3: 1.0441 - dense_2_loss_4: 1.2088 - dense_2_loss_5: 1.1782 - dense_2_loss_6: 1.0284 - dense_2_acc_1: 0.8014 - dense_2_acc_2: 0.7388 - dense_2_acc_3: 0.7082 - dense_2_acc_4: 0.6562 - dense_2_acc_5: 0.6872 - dense_2_acc_6: 0.7402 - val_loss: 42.5806 - val_dense_2_loss_1: 3.2494 - val_dense_2_loss_2: 8.2409 - val_dense_2_loss_3: 7.9461 - val_dense_2_loss_4: 8.6919 - val_dense_2_loss_5: 7.3586 - val_dense_2_loss_6: 7.0937 - val_dense_2_acc_1: 0.4912 - val_dense_2_acc_2: 0.1930 - val_dense_2_acc_3: 0.1754 - val_dense_2_acc_4: 0.1404 - val_dense_2_acc_5: 0.2456 - val_dense_2_acc_6: 0.2807\n",
      "Epoch 184/200\n",
      "2800/2810 [============================>.] - ETA: 0s - loss: 6.2037 - dense_2_loss_1: 0.6944 - dense_2_loss_2: 0.9309 - dense_2_loss_3: 1.0279 - dense_2_loss_4: 1.2398 - dense_2_loss_5: 1.2654 - dense_2_loss_6: 1.0452 - dense_2_acc_1: 0.7993 - dense_2_acc_2: 0.7257 - dense_2_acc_3: 0.7104 - dense_2_acc_4: 0.6536 - dense_2_acc_5: 0.6764 - dense_2_acc_6: 0.7368Epoch 00183: saving model to main_model_weights_new.h5\n",
      "2810/2810 [==============================] - 127s - loss: 6.2062 - dense_2_loss_1: 0.6970 - dense_2_loss_2: 0.9325 - dense_2_loss_3: 1.0300 - dense_2_loss_4: 1.2399 - dense_2_loss_5: 1.2644 - dense_2_loss_6: 1.0423 - dense_2_acc_1: 0.7986 - dense_2_acc_2: 0.7253 - dense_2_acc_3: 0.7100 - dense_2_acc_4: 0.6534 - dense_2_acc_5: 0.6765 - dense_2_acc_6: 0.7377 - val_loss: 41.2577 - val_dense_2_loss_1: 3.1969 - val_dense_2_loss_2: 8.2197 - val_dense_2_loss_3: 7.4754 - val_dense_2_loss_4: 8.2016 - val_dense_2_loss_5: 7.4185 - val_dense_2_loss_6: 6.7454 - val_dense_2_acc_1: 0.5439 - val_dense_2_acc_2: 0.1930 - val_dense_2_acc_3: 0.1754 - val_dense_2_acc_4: 0.1754 - val_dense_2_acc_5: 0.2456 - val_dense_2_acc_6: 0.3860\n",
      "Epoch 185/200\n",
      "2800/2810 [============================>.] - ETA: 0s - loss: 6.0374 - dense_2_loss_1: 0.6611 - dense_2_loss_2: 0.9049 - dense_2_loss_3: 1.0520 - dense_2_loss_4: 1.2218 - dense_2_loss_5: 1.1738 - dense_2_loss_6: 1.0238 - dense_2_acc_1: 0.7964 - dense_2_acc_2: 0.7464 - dense_2_acc_3: 0.6957 - dense_2_acc_4: 0.6696 - dense_2_acc_5: 0.6768 - dense_2_acc_6: 0.7439Epoch 00184: saving model to main_model_weights_new.h5\n",
      "2810/2810 [==============================] - 129s - loss: 6.0452 - dense_2_loss_1: 0.6607 - dense_2_loss_2: 0.9054 - dense_2_loss_3: 1.0538 - dense_2_loss_4: 1.2257 - dense_2_loss_5: 1.1755 - dense_2_loss_6: 1.0241 - dense_2_acc_1: 0.7964 - dense_2_acc_2: 0.7470 - dense_2_acc_3: 0.6954 - dense_2_acc_4: 0.6694 - dense_2_acc_5: 0.6762 - dense_2_acc_6: 0.7438 - val_loss: 40.2910 - val_dense_2_loss_1: 3.0864 - val_dense_2_loss_2: 7.5761 - val_dense_2_loss_3: 7.6678 - val_dense_2_loss_4: 8.3247 - val_dense_2_loss_5: 7.1271 - val_dense_2_loss_6: 6.5089 - val_dense_2_acc_1: 0.5439 - val_dense_2_acc_2: 0.2105 - val_dense_2_acc_3: 0.1930 - val_dense_2_acc_4: 0.1579 - val_dense_2_acc_5: 0.2105 - val_dense_2_acc_6: 0.3158\n",
      "Epoch 186/200\n",
      "2800/2810 [============================>.] - ETA: 0s - loss: 5.9265 - dense_2_loss_1: 0.6452 - dense_2_loss_2: 0.8852 - dense_2_loss_3: 1.0091 - dense_2_loss_4: 1.1710 - dense_2_loss_5: 1.1948 - dense_2_loss_6: 1.0212 - dense_2_acc_1: 0.8107 - dense_2_acc_2: 0.7457 - dense_2_acc_3: 0.7107 - dense_2_acc_4: 0.6779 - dense_2_acc_5: 0.6839 - dense_2_acc_6: 0.7361Epoch 00185: saving model to main_model_weights_new.h5\n",
      "2810/2810 [==============================] - 128s - loss: 5.9229 - dense_2_loss_1: 0.6436 - dense_2_loss_2: 0.8832 - dense_2_loss_3: 1.0096 - dense_2_loss_4: 1.1688 - dense_2_loss_5: 1.1953 - dense_2_loss_6: 1.0224 - dense_2_acc_1: 0.8110 - dense_2_acc_2: 0.7463 - dense_2_acc_3: 0.7110 - dense_2_acc_4: 0.6779 - dense_2_acc_5: 0.6836 - dense_2_acc_6: 0.7359 - val_loss: 41.0300 - val_dense_2_loss_1: 2.9624 - val_dense_2_loss_2: 7.6903 - val_dense_2_loss_3: 7.6279 - val_dense_2_loss_4: 8.5635 - val_dense_2_loss_5: 7.6647 - val_dense_2_loss_6: 6.5213 - val_dense_2_acc_1: 0.5439 - val_dense_2_acc_2: 0.2105 - val_dense_2_acc_3: 0.1754 - val_dense_2_acc_4: 0.1579 - val_dense_2_acc_5: 0.2456 - val_dense_2_acc_6: 0.3158\n",
      "Epoch 187/200\n",
      "2800/2810 [============================>.] - ETA: 0s - loss: 5.8443 - dense_2_loss_1: 0.6694 - dense_2_loss_2: 0.8488 - dense_2_loss_3: 1.0264 - dense_2_loss_4: 1.1346 - dense_2_loss_5: 1.1539 - dense_2_loss_6: 1.0113 - dense_2_acc_1: 0.7996 - dense_2_acc_2: 0.7557 - dense_2_acc_3: 0.7182 - dense_2_acc_4: 0.6793 - dense_2_acc_5: 0.6875 - dense_2_acc_6: 0.7529Epoch 00186: saving model to main_model_weights_new.h5\n",
      "2810/2810 [==============================] - 128s - loss: 5.8435 - dense_2_loss_1: 0.6692 - dense_2_loss_2: 0.8511 - dense_2_loss_3: 1.0275 - dense_2_loss_4: 1.1325 - dense_2_loss_5: 1.1523 - dense_2_loss_6: 1.0109 - dense_2_acc_1: 0.7996 - dense_2_acc_2: 0.7552 - dense_2_acc_3: 0.7185 - dense_2_acc_4: 0.6797 - dense_2_acc_5: 0.6879 - dense_2_acc_6: 0.7530 - val_loss: 40.7517 - val_dense_2_loss_1: 2.9916 - val_dense_2_loss_2: 7.7780 - val_dense_2_loss_3: 7.6561 - val_dense_2_loss_4: 8.4495 - val_dense_2_loss_5: 7.2829 - val_dense_2_loss_6: 6.5936 - val_dense_2_acc_1: 0.5263 - val_dense_2_acc_2: 0.2105 - val_dense_2_acc_3: 0.2105 - val_dense_2_acc_4: 0.1754 - val_dense_2_acc_5: 0.2456 - val_dense_2_acc_6: 0.3333\n",
      "Epoch 188/200\n",
      "2800/2810 [============================>.] - ETA: 0s - loss: 6.0493 - dense_2_loss_1: 0.6786 - dense_2_loss_2: 0.9104 - dense_2_loss_3: 1.0234 - dense_2_loss_4: 1.2094 - dense_2_loss_5: 1.1414 - dense_2_loss_6: 1.0861 - dense_2_acc_1: 0.8043 - dense_2_acc_2: 0.7325 - dense_2_acc_3: 0.7121 - dense_2_acc_4: 0.6618 - dense_2_acc_5: 0.7004 - dense_2_acc_6: 0.7343Epoch 00187: saving model to main_model_weights_new.h5\n",
      "2810/2810 [==============================] - 128s - loss: 6.0543 - dense_2_loss_1: 0.6810 - dense_2_loss_2: 0.9111 - dense_2_loss_3: 1.0240 - dense_2_loss_4: 1.2092 - dense_2_loss_5: 1.1426 - dense_2_loss_6: 1.0865 - dense_2_acc_1: 0.8043 - dense_2_acc_2: 0.7320 - dense_2_acc_3: 0.7117 - dense_2_acc_4: 0.6619 - dense_2_acc_5: 0.7004 - dense_2_acc_6: 0.7331 - val_loss: 41.3463 - val_dense_2_loss_1: 3.1178 - val_dense_2_loss_2: 7.9404 - val_dense_2_loss_3: 7.7771 - val_dense_2_loss_4: 8.2857 - val_dense_2_loss_5: 7.3968 - val_dense_2_loss_6: 6.8285 - val_dense_2_acc_1: 0.5789 - val_dense_2_acc_2: 0.1930 - val_dense_2_acc_3: 0.2105 - val_dense_2_acc_4: 0.1930 - val_dense_2_acc_5: 0.2456 - val_dense_2_acc_6: 0.3860\n",
      "Epoch 189/200\n",
      "2800/2810 [============================>.] - ETA: 0s - loss: 5.8292 - dense_2_loss_1: 0.6473 - dense_2_loss_2: 0.8544 - dense_2_loss_3: 0.9705 - dense_2_loss_4: 1.1737 - dense_2_loss_5: 1.1680 - dense_2_loss_6: 1.0153 - dense_2_acc_1: 0.8082 - dense_2_acc_2: 0.7543 - dense_2_acc_3: 0.7196 - dense_2_acc_4: 0.6679 - dense_2_acc_5: 0.6846 - dense_2_acc_6: 0.7457Epoch 00188: saving model to main_model_weights_new.h5\n",
      "2810/2810 [==============================] - 129s - loss: 5.8282 - dense_2_loss_1: 0.6475 - dense_2_loss_2: 0.8545 - dense_2_loss_3: 0.9717 - dense_2_loss_4: 1.1715 - dense_2_loss_5: 1.1670 - dense_2_loss_6: 1.0161 - dense_2_acc_1: 0.8082 - dense_2_acc_2: 0.7544 - dense_2_acc_3: 0.7192 - dense_2_acc_4: 0.6687 - dense_2_acc_5: 0.6847 - dense_2_acc_6: 0.7459 - val_loss: 42.6004 - val_dense_2_loss_1: 3.5319 - val_dense_2_loss_2: 7.8099 - val_dense_2_loss_3: 8.2658 - val_dense_2_loss_4: 8.6685 - val_dense_2_loss_5: 7.4797 - val_dense_2_loss_6: 6.8445 - val_dense_2_acc_1: 0.5439 - val_dense_2_acc_2: 0.2281 - val_dense_2_acc_3: 0.1754 - val_dense_2_acc_4: 0.1754 - val_dense_2_acc_5: 0.2456 - val_dense_2_acc_6: 0.3684\n",
      "Epoch 190/200\n",
      "2800/2810 [============================>.] - ETA: 0s - loss: 5.7249 - dense_2_loss_1: 0.6515 - dense_2_loss_2: 0.8460 - dense_2_loss_3: 0.9746 - dense_2_loss_4: 1.1438 - dense_2_loss_5: 1.1181 - dense_2_loss_6: 0.9909 - dense_2_acc_1: 0.8064 - dense_2_acc_2: 0.7529 - dense_2_acc_3: 0.7207 - dense_2_acc_4: 0.6779 - dense_2_acc_5: 0.6861 - dense_2_acc_6: 0.7568Epoch 00189: saving model to main_model_weights_new.h5\n",
      "2810/2810 [==============================] - 128s - loss: 5.7365 - dense_2_loss_1: 0.6540 - dense_2_loss_2: 0.8480 - dense_2_loss_3: 0.9767 - dense_2_loss_4: 1.1453 - dense_2_loss_5: 1.1209 - dense_2_loss_6: 0.9916 - dense_2_acc_1: 0.8060 - dense_2_acc_2: 0.7523 - dense_2_acc_3: 0.7206 - dense_2_acc_4: 0.6776 - dense_2_acc_5: 0.6854 - dense_2_acc_6: 0.7566 - val_loss: 44.9268 - val_dense_2_loss_1: 3.4045 - val_dense_2_loss_2: 8.2870 - val_dense_2_loss_3: 8.3124 - val_dense_2_loss_4: 9.0362 - val_dense_2_loss_5: 8.5619 - val_dense_2_loss_6: 7.3248 - val_dense_2_acc_1: 0.5263 - val_dense_2_acc_2: 0.2105 - val_dense_2_acc_3: 0.1579 - val_dense_2_acc_4: 0.1930 - val_dense_2_acc_5: 0.2456 - val_dense_2_acc_6: 0.4035\n",
      "Epoch 191/200\n",
      "2800/2810 [============================>.] - ETA: 0s - loss: 5.7614 - dense_2_loss_1: 0.6697 - dense_2_loss_2: 0.8703 - dense_2_loss_3: 0.9619 - dense_2_loss_4: 1.1158 - dense_2_loss_5: 1.1106 - dense_2_loss_6: 1.0330 - dense_2_acc_1: 0.8011 - dense_2_acc_2: 0.7486 - dense_2_acc_3: 0.7264 - dense_2_acc_4: 0.6921 - dense_2_acc_5: 0.7029 - dense_2_acc_6: 0.7461Epoch 00190: saving model to main_model_weights_new.h5\n",
      "2810/2810 [==============================] - 128s - loss: 5.7694 - dense_2_loss_1: 0.6702 - dense_2_loss_2: 0.8708 - dense_2_loss_3: 0.9635 - dense_2_loss_4: 1.1183 - dense_2_loss_5: 1.1154 - dense_2_loss_6: 1.0313 - dense_2_acc_1: 0.8007 - dense_2_acc_2: 0.7488 - dense_2_acc_3: 0.7263 - dense_2_acc_4: 0.6911 - dense_2_acc_5: 0.7032 - dense_2_acc_6: 0.7463 - val_loss: 41.0289 - val_dense_2_loss_1: 2.8658 - val_dense_2_loss_2: 7.6676 - val_dense_2_loss_3: 8.1224 - val_dense_2_loss_4: 8.5192 - val_dense_2_loss_5: 7.3212 - val_dense_2_loss_6: 6.5327 - val_dense_2_acc_1: 0.5088 - val_dense_2_acc_2: 0.1930 - val_dense_2_acc_3: 0.1404 - val_dense_2_acc_4: 0.1579 - val_dense_2_acc_5: 0.2807 - val_dense_2_acc_6: 0.3509\n",
      "Epoch 192/200\n",
      "2800/2810 [============================>.] - ETA: 0s - loss: 5.6308 - dense_2_loss_1: 0.6308 - dense_2_loss_2: 0.8477 - dense_2_loss_3: 0.9914 - dense_2_loss_4: 1.0949 - dense_2_loss_5: 1.1128 - dense_2_loss_6: 0.9531 - dense_2_acc_1: 0.8132 - dense_2_acc_2: 0.7546 - dense_2_acc_3: 0.7236 - dense_2_acc_4: 0.6932 - dense_2_acc_5: 0.6993 - dense_2_acc_6: 0.7507Epoch 00191: saving model to main_model_weights_new.h5\n",
      "2810/2810 [==============================] - 129s - loss: 5.6292 - dense_2_loss_1: 0.6325 - dense_2_loss_2: 0.8489 - dense_2_loss_3: 0.9898 - dense_2_loss_4: 1.0948 - dense_2_loss_5: 1.1117 - dense_2_loss_6: 0.9516 - dense_2_acc_1: 0.8121 - dense_2_acc_2: 0.7544 - dense_2_acc_3: 0.7242 - dense_2_acc_4: 0.6936 - dense_2_acc_5: 0.6993 - dense_2_acc_6: 0.7509 - val_loss: 42.0903 - val_dense_2_loss_1: 3.3971 - val_dense_2_loss_2: 7.7813 - val_dense_2_loss_3: 8.0304 - val_dense_2_loss_4: 8.3639 - val_dense_2_loss_5: 7.4866 - val_dense_2_loss_6: 7.0310 - val_dense_2_acc_1: 0.5263 - val_dense_2_acc_2: 0.2105 - val_dense_2_acc_3: 0.1930 - val_dense_2_acc_4: 0.1228 - val_dense_2_acc_5: 0.2456 - val_dense_2_acc_6: 0.3684\n",
      "Epoch 193/200\n",
      "2800/2810 [============================>.] - ETA: 0s - loss: 5.7554 - dense_2_loss_1: 0.6377 - dense_2_loss_2: 0.8690 - dense_2_loss_3: 0.9450 - dense_2_loss_4: 1.1502 - dense_2_loss_5: 1.1766 - dense_2_loss_6: 0.9770 - dense_2_acc_1: 0.8150 - dense_2_acc_2: 0.7507 - dense_2_acc_3: 0.7332 - dense_2_acc_4: 0.6807 - dense_2_acc_5: 0.6861 - dense_2_acc_6: 0.7596Epoch 00192: saving model to main_model_weights_new.h5\n",
      "2810/2810 [==============================] - 129s - loss: 5.7494 - dense_2_loss_1: 0.6373 - dense_2_loss_2: 0.8674 - dense_2_loss_3: 0.9437 - dense_2_loss_4: 1.1485 - dense_2_loss_5: 1.1763 - dense_2_loss_6: 0.9762 - dense_2_acc_1: 0.8153 - dense_2_acc_2: 0.7509 - dense_2_acc_3: 0.7331 - dense_2_acc_4: 0.6808 - dense_2_acc_5: 0.6861 - dense_2_acc_6: 0.7594 - val_loss: 42.4923 - val_dense_2_loss_1: 3.2853 - val_dense_2_loss_2: 8.1231 - val_dense_2_loss_3: 8.2176 - val_dense_2_loss_4: 8.4951 - val_dense_2_loss_5: 7.6560 - val_dense_2_loss_6: 6.7153 - val_dense_2_acc_1: 0.5439 - val_dense_2_acc_2: 0.1930 - val_dense_2_acc_3: 0.1579 - val_dense_2_acc_4: 0.1754 - val_dense_2_acc_5: 0.2105 - val_dense_2_acc_6: 0.3333\n",
      "Epoch 194/200\n",
      "2800/2810 [============================>.] - ETA: 0s - loss: 5.6000 - dense_2_loss_1: 0.6075 - dense_2_loss_2: 0.8073 - dense_2_loss_3: 0.9454 - dense_2_loss_4: 1.1435 - dense_2_loss_5: 1.1075 - dense_2_loss_6: 0.9887 - dense_2_acc_1: 0.8075 - dense_2_acc_2: 0.7661 - dense_2_acc_3: 0.7264 - dense_2_acc_4: 0.6929 - dense_2_acc_5: 0.6971 - dense_2_acc_6: 0.7536Epoch 00193: saving model to main_model_weights_new.h5\n",
      "2810/2810 [==============================] - 128s - loss: 5.6003 - dense_2_loss_1: 0.6077 - dense_2_loss_2: 0.8076 - dense_2_loss_3: 0.9486 - dense_2_loss_4: 1.1398 - dense_2_loss_5: 1.1051 - dense_2_loss_6: 0.9914 - dense_2_acc_1: 0.8075 - dense_2_acc_2: 0.7658 - dense_2_acc_3: 0.7260 - dense_2_acc_4: 0.6940 - dense_2_acc_5: 0.6972 - dense_2_acc_6: 0.7530 - val_loss: 42.1485 - val_dense_2_loss_1: 3.4232 - val_dense_2_loss_2: 7.2736 - val_dense_2_loss_3: 8.1642 - val_dense_2_loss_4: 8.3248 - val_dense_2_loss_5: 7.7829 - val_dense_2_loss_6: 7.1798 - val_dense_2_acc_1: 0.5088 - val_dense_2_acc_2: 0.2456 - val_dense_2_acc_3: 0.1579 - val_dense_2_acc_4: 0.1930 - val_dense_2_acc_5: 0.1754 - val_dense_2_acc_6: 0.2982\n",
      "Epoch 195/200\n",
      "2800/2810 [============================>.] - ETA: 0s - loss: 5.7658 - dense_2_loss_1: 0.6529 - dense_2_loss_2: 0.8695 - dense_2_loss_3: 1.0174 - dense_2_loss_4: 1.1178 - dense_2_loss_5: 1.1117 - dense_2_loss_6: 0.9963 - dense_2_acc_1: 0.8061 - dense_2_acc_2: 0.7579 - dense_2_acc_3: 0.7171 - dense_2_acc_4: 0.6786 - dense_2_acc_5: 0.7018 - dense_2_acc_6: 0.7471Epoch 00194: saving model to main_model_weights_new.h5\n",
      "2810/2810 [==============================] - 129s - loss: 5.7601 - dense_2_loss_1: 0.6516 - dense_2_loss_2: 0.8682 - dense_2_loss_3: 1.0155 - dense_2_loss_4: 1.1171 - dense_2_loss_5: 1.1130 - dense_2_loss_6: 0.9948 - dense_2_acc_1: 0.8068 - dense_2_acc_2: 0.7584 - dense_2_acc_3: 0.7178 - dense_2_acc_4: 0.6790 - dense_2_acc_5: 0.7018 - dense_2_acc_6: 0.7477 - val_loss: 41.9143 - val_dense_2_loss_1: 3.3507 - val_dense_2_loss_2: 7.8646 - val_dense_2_loss_3: 7.6850 - val_dense_2_loss_4: 8.7060 - val_dense_2_loss_5: 7.7749 - val_dense_2_loss_6: 6.5330 - val_dense_2_acc_1: 0.5088 - val_dense_2_acc_2: 0.2281 - val_dense_2_acc_3: 0.1930 - val_dense_2_acc_4: 0.1754 - val_dense_2_acc_5: 0.2281 - val_dense_2_acc_6: 0.3509\n",
      "Epoch 196/200\n",
      "2800/2810 [============================>.] - ETA: 0s - loss: 5.5523 - dense_2_loss_1: 0.5932 - dense_2_loss_2: 0.8579 - dense_2_loss_3: 0.9370 - dense_2_loss_4: 1.1456 - dense_2_loss_5: 1.0969 - dense_2_loss_6: 0.9217 - dense_2_acc_1: 0.8143 - dense_2_acc_2: 0.7496 - dense_2_acc_3: 0.7354 - dense_2_acc_4: 0.6975 - dense_2_acc_5: 0.6968 - dense_2_acc_6: 0.7671Epoch 00195: saving model to main_model_weights_new.h5\n",
      "2810/2810 [==============================] - 129s - loss: 5.5540 - dense_2_loss_1: 0.5929 - dense_2_loss_2: 0.8552 - dense_2_loss_3: 0.9345 - dense_2_loss_4: 1.1445 - dense_2_loss_5: 1.1020 - dense_2_loss_6: 0.9249 - dense_2_acc_1: 0.8142 - dense_2_acc_2: 0.7505 - dense_2_acc_3: 0.7363 - dense_2_acc_4: 0.6975 - dense_2_acc_5: 0.6957 - dense_2_acc_6: 0.7669 - val_loss: 41.8333 - val_dense_2_loss_1: 3.4111 - val_dense_2_loss_2: 7.7624 - val_dense_2_loss_3: 7.8629 - val_dense_2_loss_4: 8.4520 - val_dense_2_loss_5: 7.1765 - val_dense_2_loss_6: 7.1684 - val_dense_2_acc_1: 0.4737 - val_dense_2_acc_2: 0.1930 - val_dense_2_acc_3: 0.1930 - val_dense_2_acc_4: 0.1579 - val_dense_2_acc_5: 0.2807 - val_dense_2_acc_6: 0.3684\n",
      "Epoch 197/200\n",
      "2800/2810 [============================>.] - ETA: 0s - loss: 5.4712 - dense_2_loss_1: 0.5989 - dense_2_loss_2: 0.7967 - dense_2_loss_3: 0.9286 - dense_2_loss_4: 1.1138 - dense_2_loss_5: 1.0723 - dense_2_loss_6: 0.9610 - dense_2_acc_1: 0.8186 - dense_2_acc_2: 0.7675 - dense_2_acc_3: 0.7411 - dense_2_acc_4: 0.7000 - dense_2_acc_5: 0.7214 - dense_2_acc_6: 0.7621Epoch 00196: saving model to main_model_weights_new.h5\n",
      "2810/2810 [==============================] - 127s - loss: 5.4764 - dense_2_loss_1: 0.6021 - dense_2_loss_2: 0.7971 - dense_2_loss_3: 0.9305 - dense_2_loss_4: 1.1125 - dense_2_loss_5: 1.0740 - dense_2_loss_6: 0.9601 - dense_2_acc_1: 0.8178 - dense_2_acc_2: 0.7673 - dense_2_acc_3: 0.7402 - dense_2_acc_4: 0.7000 - dense_2_acc_5: 0.7206 - dense_2_acc_6: 0.7626 - val_loss: 42.7837 - val_dense_2_loss_1: 3.7671 - val_dense_2_loss_2: 8.2183 - val_dense_2_loss_3: 7.8847 - val_dense_2_loss_4: 8.4245 - val_dense_2_loss_5: 7.5701 - val_dense_2_loss_6: 6.9189 - val_dense_2_acc_1: 0.5088 - val_dense_2_acc_2: 0.1930 - val_dense_2_acc_3: 0.2281 - val_dense_2_acc_4: 0.1930 - val_dense_2_acc_5: 0.2807 - val_dense_2_acc_6: 0.3158\n",
      "Epoch 198/200\n",
      "2800/2810 [============================>.] - ETA: 0s - loss: 5.6422 - dense_2_loss_1: 0.6370 - dense_2_loss_2: 0.8457 - dense_2_loss_3: 0.9841 - dense_2_loss_4: 1.1031 - dense_2_loss_5: 1.0926 - dense_2_loss_6: 0.9797 - dense_2_acc_1: 0.8082 - dense_2_acc_2: 0.7525 - dense_2_acc_3: 0.7296 - dense_2_acc_4: 0.6943 - dense_2_acc_5: 0.7086 - dense_2_acc_6: 0.7571Epoch 00197: saving model to main_model_weights_new.h5\n",
      "2810/2810 [==============================] - 128s - loss: 5.6423 - dense_2_loss_1: 0.6387 - dense_2_loss_2: 0.8469 - dense_2_loss_3: 0.9839 - dense_2_loss_4: 1.1026 - dense_2_loss_5: 1.0900 - dense_2_loss_6: 0.9802 - dense_2_acc_1: 0.8071 - dense_2_acc_2: 0.7523 - dense_2_acc_3: 0.7299 - dense_2_acc_4: 0.6943 - dense_2_acc_5: 0.7089 - dense_2_acc_6: 0.7569 - val_loss: 41.9202 - val_dense_2_loss_1: 3.3029 - val_dense_2_loss_2: 7.6772 - val_dense_2_loss_3: 7.9577 - val_dense_2_loss_4: 8.5192 - val_dense_2_loss_5: 7.4976 - val_dense_2_loss_6: 6.9657 - val_dense_2_acc_1: 0.5263 - val_dense_2_acc_2: 0.2105 - val_dense_2_acc_3: 0.1754 - val_dense_2_acc_4: 0.1754 - val_dense_2_acc_5: 0.2456 - val_dense_2_acc_6: 0.3509\n",
      "Epoch 199/200\n",
      "2800/2810 [============================>.] - ETA: 0s - loss: 5.5424 - dense_2_loss_1: 0.5770 - dense_2_loss_2: 0.8121 - dense_2_loss_3: 0.9659 - dense_2_loss_4: 1.0584 - dense_2_loss_5: 1.1103 - dense_2_loss_6: 1.0187 - dense_2_acc_1: 0.8179 - dense_2_acc_2: 0.7532 - dense_2_acc_3: 0.7311 - dense_2_acc_4: 0.6929 - dense_2_acc_5: 0.7075 - dense_2_acc_6: 0.7482Epoch 00198: saving model to main_model_weights_new.h5\n",
      "2810/2810 [==============================] - 129s - loss: 5.5466 - dense_2_loss_1: 0.5782 - dense_2_loss_2: 0.8115 - dense_2_loss_3: 0.9692 - dense_2_loss_4: 1.0585 - dense_2_loss_5: 1.1117 - dense_2_loss_6: 1.0175 - dense_2_acc_1: 0.8174 - dense_2_acc_2: 0.7534 - dense_2_acc_3: 0.7299 - dense_2_acc_4: 0.6929 - dense_2_acc_5: 0.7068 - dense_2_acc_6: 0.7488 - val_loss: 42.1803 - val_dense_2_loss_1: 3.1867 - val_dense_2_loss_2: 7.9890 - val_dense_2_loss_3: 7.7226 - val_dense_2_loss_4: 8.7343 - val_dense_2_loss_5: 7.7392 - val_dense_2_loss_6: 6.8085 - val_dense_2_acc_1: 0.5088 - val_dense_2_acc_2: 0.1754 - val_dense_2_acc_3: 0.1930 - val_dense_2_acc_4: 0.2105 - val_dense_2_acc_5: 0.2456 - val_dense_2_acc_6: 0.3684\n",
      "Epoch 200/200\n",
      "2800/2810 [============================>.] - ETA: 0s - loss: 5.6078 - dense_2_loss_1: 0.6212 - dense_2_loss_2: 0.8481 - dense_2_loss_3: 0.9243 - dense_2_loss_4: 1.0855 - dense_2_loss_5: 1.1008 - dense_2_loss_6: 1.0280 - dense_2_acc_1: 0.8168 - dense_2_acc_2: 0.7682 - dense_2_acc_3: 0.7371 - dense_2_acc_4: 0.6950 - dense_2_acc_5: 0.7011 - dense_2_acc_6: 0.7539Epoch 00199: saving model to main_model_weights_new.h5\n",
      "2810/2810 [==============================] - 130s - loss: 5.6031 - dense_2_loss_1: 0.6193 - dense_2_loss_2: 0.8478 - dense_2_loss_3: 0.9223 - dense_2_loss_4: 1.0839 - dense_2_loss_5: 1.1026 - dense_2_loss_6: 1.0273 - dense_2_acc_1: 0.8174 - dense_2_acc_2: 0.7683 - dense_2_acc_3: 0.7377 - dense_2_acc_4: 0.6954 - dense_2_acc_5: 0.7011 - dense_2_acc_6: 0.7541 - val_loss: 44.1031 - val_dense_2_loss_1: 3.5353 - val_dense_2_loss_2: 7.8698 - val_dense_2_loss_3: 8.1190 - val_dense_2_loss_4: 8.9073 - val_dense_2_loss_5: 8.0639 - val_dense_2_loss_6: 7.6078 - val_dense_2_acc_1: 0.5439 - val_dense_2_acc_2: 0.1754 - val_dense_2_acc_3: 0.1930 - val_dense_2_acc_4: 0.1404 - val_dense_2_acc_5: 0.2281 - val_dense_2_acc_6: 0.2456\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'mask_ib-0'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-71-42526472c58c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainY\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m200\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtestX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtestY\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcheckpoint\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m#Hoping to save model without errors.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'main_model.h5'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'final_model_weights.h5'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/src/conda3_runtime/home/envs/DSX-Python35-Spark/lib/python3.5/site-packages/keras/engine/topology.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(self, filepath, overwrite, include_optimizer)\u001b[0m\n\u001b[1;32m   2504\u001b[0m         \"\"\"\n\u001b[1;32m   2505\u001b[0m         \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msave_model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2506\u001b[0;31m         \u001b[0msave_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moverwrite\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minclude_optimizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2507\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2508\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0msave_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moverwrite\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/src/conda3_runtime/home/envs/DSX-Python35-Spark/lib/python3.5/site-packages/keras/models.py\u001b[0m in \u001b[0;36msave_model\u001b[0;34m(model, filepath, overwrite, include_optimizer)\u001b[0m\n\u001b[1;32m    104\u001b[0m     f.attrs['model_config'] = json.dumps({\n\u001b[1;32m    105\u001b[0m         \u001b[0;34m'class_name'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 106\u001b[0;31m         \u001b[0;34m'config'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_config\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    107\u001b[0m     }, default=get_json_type).encode('utf8')\n\u001b[1;32m    108\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/src/conda3_runtime/home/envs/DSX-Python35-Spark/lib/python3.5/site-packages/keras/engine/topology.py\u001b[0m in \u001b[0;36mget_config\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   2368\u001b[0m             \u001b[0mnode_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_layers_node_indices\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2369\u001b[0m             \u001b[0mnode_key\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'_ib-'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2370\u001b[0;31m             \u001b[0mnew_node_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnode_conversion_map\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnode_key\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2371\u001b[0m             \u001b[0mtensor_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_layers_tensor_indices\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2372\u001b[0m             \u001b[0mmodel_inputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlayer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_node_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'mask_ib-0'"
     ]
    }
   ],
   "source": [
    "#Run the model.fit. If only best validation model needs to be saved, then change save_best_only=True in checkpoint.\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "checkpoint = ModelCheckpoint('main_model_weights_new.h5', monitor='val_loss', verbose=1, save_best_only=False, mode='auto',save_weights_only=True)\n",
    "model.fit(trainX, trainY, epochs=200, batch_size=20, validation_data=(testX, testY), callbacks=[checkpoint])\n",
    "#Hoping to save model without errors.\n",
    "model.save('main_model.h5')\n",
    "model.save_weights('final_model_weights.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Quantitative Analysis\n",
    "While training you can see the loss as well as the accuracy on each of the positions of the output. The output snapshot below gives you an example of what the accuracies could be at 100th iteration in above settings: \n",
    "\n",
    "Epoch 100/100\n",
    "2800/2810 [============================>.] - ETA: 0s - loss: 5.1898 - dense_2_loss_1: 0.7850 - dense_2_loss_2: 0.8572 - dense_2_loss_3: 0.8971 - dense_2_loss_4: 0.8881 - dense_2_loss_5: 0.9539 - dense_2_loss_6: 0.8085 - dense_2_acc_1: 0.7875 - dense_2_acc_2: 0.7571 - dense_2_acc_3: 0.7443 - dense_2_acc_4: 0.7479 - dense_2_acc_5: 0.7379 - dense_2_acc_6: 0.7975Epoch 00099: saving model to main_model_weights.h5\n",
    "2810/2810 [==============================] - 17s - loss: 5.1868 - dense_2_loss_1: 0.7848 - dense_2_loss_2: 0.8570 - dense_2_loss_3: 0.8973 - dense_2_loss_4: 0.8891 - dense_2_loss_5: 0.9526 - dense_2_loss_6: 0.8061 - dense_2_acc_1: 0.7872 - dense_2_acc_2: 0.7569 - dense_2_acc_3: 0.7438 - dense_2_acc_4: 0.7480 - dense_2_acc_5: 0.7384 - dense_2_acc_6: 0.7982 - val_loss: 40.8334 - val_dense_2_loss_1: 4.3684 - val_dense_2_loss_2: 7.5746 - val_dense_2_loss_3: 6.9904 - val_dense_2_loss_4: 8.9484 - val_dense_2_loss_5: 6.6466 - val_dense_2_loss_6: 6.3049 - val_dense_2_acc_1: 0.4561 - val_dense_2_acc_2: 0.2807 - val_dense_2_acc_3: 0.1754 - val_dense_2_acc_4: 0.1579 - val_dense_2_acc_5: 0.2456 - val_dense_2_acc_6: 0.3684\n",
    "\n",
    "######################################################################################################\n",
    "\n",
    "<caption><left> Thus at 100-th iteration with unaltered settings above, `dense_2_acc_6: 0.7975` means that you are predicting the 6th word of the output correctly 79% of the time in the current batch of data. Also val_dense_2_acc_6: 0.3684 means the 6th digit prediction accuracy is 36%  </left></caption>\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Qualitative Analysis\n",
    "\n",
    "Following code will load the saved weights which will be used to do a qualitative analysis. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "loaded_model.load_weights('main_model_weights_new.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now see the results on new examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EXAMPLES=hindi_sentences_list[0:1]\n",
    "true_test=\"हमने खरीदी\"\n",
    "EXAMPLES.append(true_test)\n",
    "EXAMPLES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EXAMPLES_CODED=get_padded_encoding(EXAMPLES,hindi_dictionary,6)\n",
    "print(EXAMPLES_CODED,EXAMPLES_CODED.shape,hindi_encoding.shape)\n",
    "#h_mask_examples=h_mask\n",
    "#for train_numbers in range(0,testXoh.shape[0]-1):\n",
    "#    h_mask_test=np.concatenate((h_mask_test,h_mask),axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i=0\n",
    "for example in EXAMPLES_CODED:\n",
    "    iteration=i+1\n",
    "    source = example\n",
    "    source = np.array(list(map(lambda x: to_categorical(x, num_classes=len(hindi_dictionary)), source))).swapaxes(0,1)\n",
    "    prediction = model.predict([source,train_s0, train_c0,h_mask_train])\n",
    "    #print (\"Prediction, Type & Shape:\",prediction,type(prediction),len(prediction))\n",
    "    prediction = np.argmax(prediction, axis = -1)\n",
    "    #print (\"Prediction, After Argmax:\",prediction)\n",
    "    output = [revere_dictionary_english[int(i)] for i in prediction]\n",
    "    print(\"\\n ##### \\n\")\n",
    "    print(\"Hindi\",EXAMPLES[i])\n",
    "    if (iteration!=EXAMPLES_CODED.shape[0]):\n",
    "        print(\"Expected:\",english_sentences_list[i])\n",
    "    print(\"Predicted output:\", ' '.join(output))\n",
    "    #print (\"Prediction:\",list(prediction))\n",
    "    i=i+1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We should be able to see following results. We have first sentence from training example and another one from true test. We can see that there is a pretty good translation for the data from training and for true test it was able to predict first place pretty accurately but failed in following portions.\n",
    "\n",
    " ##### \n",
    "\n",
    "Hindi इसको अपना घर ही समझो\n",
    "Expected: Please make yourself at home\n",
    "Predicted output: Please yourself yourself home <pad> <pad>\n",
    "\n",
    " ##### \n",
    "\n",
    "Hindi हमने खरीदी\n",
    "Predicted output: We leave up <pad> <pad> <pad>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5 References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Neural Machine Translation by Jointly Learning to Align and Translate: Dzmitry Bahdanau, Kyunghyun Cho, Yoshua Bengio https://arxiv.org/pdf/1409.0473.pdf\n",
    "\n",
    "https://machinelearningmastery.com\n",
    "\n",
    "https://www.coursera.org/\n",
    "\n",
    "https://www.udemy.com/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Appendix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One thing we can do to improve the model is instead of one hot encodings of words of length vocabulary, get the word2vec vectors for each word with fixed length.\n",
    "\n",
    "Another thing that can be done is train only short sentences.\n",
    "\n",
    "In below section we will provide the functions to help to do the tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Converting input to word2vec.\n",
    "def sentences_to_word2vec_input_format(language_sentences_list):\n",
    "    word2vec_sentence_feed=list()\n",
    "    for sentence in language_sentences_list:\n",
    "        word2vec_sentence_feed.append(sentence.split())\n",
    "    return(word2vec_sentence_feed)\n",
    "english_sentences_w2v_format=sentences_to_word2vec_input_format(english_sentences_list)\n",
    "hindi_sentences_w2v_format=sentences_to_word2vec_input_format(hindi_sentences_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "# train model\n",
    "english_model = Word2Vec(english_sentences_w2v_format, min_count=1)\n",
    "english_words_vocab = list(english_model.wv.vocab)\n",
    "hindi_model = Word2Vec(hindi_sentences_w2v_format, min_count=1)\n",
    "english_words_vocab = list(hindi_model.wv.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentences_to_w2vec(language_encoding,revere_dictionary_language,language_model):\n",
    "    import numpy as np\n",
    "    sentence_level_w2vec_list=[]\n",
    "    #arr = np.empty((2,), float)\n",
    "    number_of_sentences=language_encoding.shape[0]\n",
    "    for i in range(0,number_of_sentences):\n",
    "        language_list_padded=[]\n",
    "        #print (english_encoding[i])\n",
    "        for key in language_encoding[i]:\n",
    "            #print(revere_dictionary_english[key])\n",
    "            word=(revere_dictionary_language[key])\n",
    "            try:\n",
    "                #print(\"Found word Shape of word vector\",(english_model[word]).shape,arr.shape)\n",
    "                language_list_padded.append(language_model[word])\n",
    "            except KeyError:\n",
    "                unk='<unk>'\n",
    "                #print(\"not found! Assigning Unknown Vector\",  (english_model[unk]).shape)\n",
    "                language_list_padded.append(language_model[unk])\n",
    "        #print(np.array(language_list_padded))\n",
    "        sentence_level_w2vec_list.append((np.array(language_list_padded)))\n",
    "    sentence_level_w2vec=np.array(sentence_level_w2vec_list)\n",
    "    return(sentence_level_w2vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=hindi_encoding\n",
    "Y=english_encoding\n",
    "#Y will remain the same.\n",
    "Yoh=np.array(list(map(lambda x: to_categorical(x, num_classes=len(english_dictionary)), Y)))\n",
    "print(\"X.shape:\", X.shape)\n",
    "print(\"Y.shape:\", Y.shape)\n",
    "print(\"Yoh.shape:\", Yoh.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Run this if you want word2vec instead of One hot encoding\n",
    "#Naming it still as X0h and Yoh to avoid changes in too many places further.\n",
    "#Yoh \n",
    "Xoh=sentences_to_w2vec(hindi_encoding,revere_dictionary_hindi,hindi_model)\n",
    "print(\"Xoh.shape:\", Xoh.shape)\n",
    "print(\"Yoh.shape:\", Yoh.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One might also like to get the sentences of only specific length from source as well as target, for example get all sentences which has maximum 5 words and in hindi maximum 8 words. Use below function and feed the length you need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataset=Ndarray with following dimentions (sentence_length, 2)\n",
    "#source_len is the length of language in dataset[0][1]\n",
    "#target_len is the length of language in dataset[0][0]\n",
    "def get_sentences_subset(dataset,source_len,target_len):\n",
    "    limited=dataset\n",
    "    indexes_list=[]\n",
    "    for indexes in range(0,limited.shape[0]):\n",
    "        #print(len(limited[i][0].split()),len(limited[i][1].split()))\n",
    "        eng_len=len(limited[indexes][0].split()) \n",
    "        hin_len=len(limited[indexes][1].split())\n",
    "        state1=(eng_len<target_len)\n",
    "        state2=(hin_len<source_len)\n",
    "        final=state2&state1\n",
    "        #print(eng_len,hin_len,final)\n",
    "        #print(state1,state2,final)\n",
    "        if (final):\n",
    "            indexes_list.append(indexes)\n",
    "    #print(indexes_list,type(indexes_list))\n",
    "    return(limited[indexes_list])"
   ]
  }
 ],
 "metadata": {
  "coursera": {
   "course_slug": "nlp-sequence-models",
   "graded_item_id": "n16CQ",
   "launcher_item_id": "npjGi"
  },
  "kernelspec": {
   "display_name": "Python 3.5 with Spark 1.6 (Unsupported)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
