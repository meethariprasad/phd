{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NMT_Project_V3_Attention_decoder.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3.5 with Spark 1.6 (Unsupported)",
      "language": "python",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "[View in Colaboratory](https://colab.research.google.com/github/meethariprasad/phd/blob/master/NMT_Project_V3_Attention_decoder.ipynb)"
      ]
    },
    {
      "metadata": {
        "id": "MudlkWl8ppfA",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Neural Machine Translation from Hindi to English\n",
        "\n",
        "\n",
        "Assignment is to build a Neural Machine Translation (NMT) model to translate Hindi Sentences into machine English. \n",
        "\n",
        "We will do this using by creating attention model as in Neural Machine Translation by Jointly Learning to Align and Translate: Dzmitry Bahdanau, Kyunghyun Cho, Yoshua Bengio https://arxiv.org/pdf/1409.0473.pdf\n",
        "\n",
        "We will be using following small parallel corpus \"http://www.manythings.org/anki/hin-eng.zip\"\n",
        "\n",
        "Notes: In Appendix section at end we have given additional helper functions which can be used to improve model as future improvement effort."
      ]
    },
    {
      "metadata": {
        "id": "aEplXA7DPasr",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 340
        },
        "outputId": "e6d76a9a-0a3a-43f5-f0c1-8d9b62a2f4dc"
      },
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.listdir()"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['datalab',\n",
              " '.config',\n",
              " '.keras',\n",
              " '.local',\n",
              " 'dataset.pkl',\n",
              " '.forever',\n",
              " '.rnd',\n",
              " '.cache',\n",
              " '.gdfuse',\n",
              " '__pycache__',\n",
              " 'drive',\n",
              " 'clean_pairs.pkl',\n",
              " 'best_model_new.h5',\n",
              " 'attention_decoder.py',\n",
              " 'hin.txt',\n",
              " 'enghindi.txt',\n",
              " '.nv',\n",
              " '.ipython',\n",
              " '_about.txt']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "metadata": {
        "id": "ikjC2-A6ppfE",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Loading Libraries."
      ]
    },
    {
      "metadata": {
        "id": "olaxCdoioGy-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "875712d6-4845-42e4-b1df-08401218df9f"
      },
      "cell_type": "code",
      "source": [
        "# https://pypi.python.org/pypi/pydot\n",
        "#Setting up graphviz (2.38.0-16ubuntu2) ...\n",
        "#Processing triggers for libc-bin (2.26-0ubuntu2.1) ...\n",
        "try:\n",
        "  !apt-get -qq install -y graphviz && pip install -q pydot\n",
        "  import pydot\n",
        "  import graphviz\n",
        "except ModuleNotFoundError:\n",
        "  print (\"ModuleNotFoundError\")"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "ModuleNotFoundError\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "BqctMUGtGUtQ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        },
        "outputId": "2bd9f7cb-cb25-46dd-9d09-61bd8f08e334"
      },
      "cell_type": "code",
      "source": [
        "!apt-get install -y -qq software-properties-common python-software-properties module-init-tools\n",
        "!add-apt-repository -y ppa:alessandro-strada/ppa 2>&1 > /dev/null\n",
        "!apt-get update -qq 2>&1 > /dev/null\n",
        "!apt-get -y install -qq google-drive-ocamlfuse fuse\n",
        "from google.colab import auth\n",
        "auth.authenticate_user()\n",
        "from oauth2client.client import GoogleCredentials\n",
        "creds = GoogleCredentials.get_application_default()\n",
        "import getpass\n",
        "!google-drive-ocamlfuse -headless -id={creds.client_id} -secret={creds.client_secret} < /dev/null 2>&1 | grep URL\n",
        "vcode = getpass.getpass()\n",
        "!echo {vcode} | google-drive-ocamlfuse -headless -id={creds.client_id} -secret={creds.client_secret}"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "gpg: keybox '/tmp/tmp37tzq9fw/pubring.gpg' created\n",
            "gpg: /tmp/tmp37tzq9fw/trustdb.gpg: trustdb created\n",
            "gpg: key AD5F235DF639B041: public key \"Launchpad PPA for Alessandro Strada\" imported\n",
            "gpg: Total number processed: 1\n",
            "gpg:               imported: 1\n",
            "Warning: apt-key output should not be parsed (stdout is not a terminal)\n",
            "··········\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "lUG5RD6HK3Ez",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "8c254f3e-8ff1-4938-bdea-bab11eaaac1b"
      },
      "cell_type": "code",
      "source": [
        "!mkdir -p drive\n",
        "!google-drive-ocamlfuse drive"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "fuse: mountpoint is not empty\r\n",
            "fuse: if you are sure this is safe, use the 'nonempty' mount option\r\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "xMloGe3bOOAF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 340
        },
        "outputId": "6e220d01-51c1-4b98-e34f-613670927b4b"
      },
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.listdir()"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['datalab',\n",
              " '.config',\n",
              " '.keras',\n",
              " '.local',\n",
              " 'dataset.pkl',\n",
              " '.forever',\n",
              " '.rnd',\n",
              " '.cache',\n",
              " '.gdfuse',\n",
              " '__pycache__',\n",
              " 'drive',\n",
              " 'clean_pairs.pkl',\n",
              " 'best_model_new.h5',\n",
              " 'attention_decoder.py',\n",
              " 'hin.txt',\n",
              " 'enghindi.txt',\n",
              " '.nv',\n",
              " '.ipython',\n",
              " '_about.txt']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "metadata": {
        "id": "e_DoANY8lv1v",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Install the PyDrive wrapper & import libraries. This is to upload model & weights to your GDrive.\n",
        "# This only needs to be done once in a notebook.\n",
        "!pip install -U -q PyDrive\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "\n",
        "# Authenticate and create the PyDrive client.\n",
        "# This only needs to be done once in a notebook.\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "CgFXOwK1Fhs0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 340
        },
        "outputId": "989f085b-6b24-45a7-84c7-ea87d6d91d0a"
      },
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.getcwd()\n",
        "os.listdir()"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['datalab',\n",
              " '.config',\n",
              " '.keras',\n",
              " '.local',\n",
              " 'dataset.pkl',\n",
              " '.forever',\n",
              " '.rnd',\n",
              " '.cache',\n",
              " '.gdfuse',\n",
              " '__pycache__',\n",
              " 'drive',\n",
              " 'clean_pairs.pkl',\n",
              " 'best_model_new.h5',\n",
              " 'attention_decoder.py',\n",
              " 'hin.txt',\n",
              " 'enghindi.txt',\n",
              " '.nv',\n",
              " '.ipython',\n",
              " '_about.txt']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "metadata": {
        "id": "ooG6sMrA1uDD",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        },
        "outputId": "cedf309c-2b23-44c3-9b17-aa3f5e6e37fb"
      },
      "cell_type": "code",
      "source": [
        "!pip install keras==2.0.5"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: keras==2.0.5 in /usr/local/lib/python3.6/dist-packages (2.0.5)\r\n",
            "Requirement already satisfied: theano in /usr/local/lib/python3.6/dist-packages (from keras==2.0.5) (1.0.1)\r\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from keras==2.0.5) (3.12)\r\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from keras==2.0.5) (1.11.0)\r\n",
            "Requirement already satisfied: scipy>=0.14 in /usr/local/lib/python3.6/dist-packages (from theano->keras==2.0.5) (0.19.1)\r\n",
            "Requirement already satisfied: numpy>=1.9.1 in /usr/local/lib/python3.6/dist-packages (from theano->keras==2.0.5) (1.14.3)\r\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "VZyjHexqppfK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "7bdda163-5982-42bc-f643-9853b1879a16"
      },
      "cell_type": "code",
      "source": [
        "from keras.layers import Bidirectional, Concatenate, Permute, Dot, Input, LSTM, Multiply\n",
        "from keras.layers import RepeatVector, Dense, Activation, Lambda\n",
        "from keras.optimizers import Adam\n",
        "import random\n",
        "from keras.utils import to_categorical\n",
        "from keras.models import load_model, Model\n",
        "import keras.backend as K\n",
        "import numpy as np\n",
        "import gc\n",
        "import os.path\n",
        "#import pydot \n",
        "#import graphviz\n",
        " \n",
        "\n",
        "#from faker import Faker\n",
        "#import random\n",
        "#from tqdm import tqdm\n",
        "#from babel.dates import format_date\n",
        "#from nmt_utils import *\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "I5GL6sJsppfj",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## 1 - Translating Source Language to Target Language\n",
        "\n",
        "The model we will build here could be used to translate from from English to Hindi or any other parallel corpus. "
      ]
    },
    {
      "metadata": {
        "id": "Z0vsi9Hzppfm",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### 1.1 - Dataset\n",
        "\n",
        "In this section we are going to download the dataset and prepare the dataset with Padding & Integer Encoding & One hot encoding."
      ]
    },
    {
      "metadata": {
        "id": "OR89gTm0ppfp",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "if (os.path.isfile(\"hin-eng.zip\")==False & os.path.isfile(\"enghindi.zip\")==False ):\n",
        "  import requests, zipfile, io, os\n",
        "  #https://github.com/meethariprasad/phd/raw/master/assignments/NLP/Translation/hin.zip\n",
        "  r = requests.get(\"http://www.manythings.org/anki/hin-eng.zip\")\n",
        "  z = zipfile.ZipFile(io.BytesIO(r.content))\n",
        "  z.extractall()\n",
        "  os.listdir()\n",
        "  #https://github.com/meethariprasad/phd/raw/master/assignments/NLP/Translation/enghindi.zip\n",
        "  r = requests.get(\"https://github.com/meethariprasad/phd/raw/master/assignments/NLP/Translation/enghindi.zip\")\n",
        "  z = zipfile.ZipFile(io.BytesIO(r.content))\n",
        "  z.extractall()\n",
        "  os.listdir()\n",
        "#import requests, zipfile, io,os\n",
        "#r = requests.get(\"http://www.manythings.org/anki/hin-eng.zip\")\n",
        "#z = zipfile.ZipFile(io.BytesIO(r.content))\n",
        "#z.extractall()\n",
        "#Verifying if the file hin.txt are downloaded properly.\n",
        "#if os.path.isfile(\"hin.txt\"):\n",
        "#    print('hin.txt exists')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "N0tCHfs0qyxZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "8bbfc28d-917b-4295-ea1f-1907740887b9"
      },
      "cell_type": "code",
      "source": [
        "print(\"If you want dataset to be reconstructed using new sentence length, enter 1\")\n",
        "a = input()\n",
        "print(type(int(a)))\n",
        "a=int(a)\n",
        "if((a==1)& os.path.isfile(\"dataset.pkl\")==True):\n",
        "  print(\"yeah. I will delete dataset.pkl\")\n",
        "  print(os.remove(\"dataset.pkl\"))\n",
        "else:\n",
        "  print(\"cool. Either it is already deleted or I Will proceed with already created dataset.pkl\")"
      ],
      "execution_count": 89,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "If you want dataset to be reconstructed using new sentence length, enter 1\n",
            "1\n",
            "<class 'int'>\n",
            "yeah. I will delete dataset.pkl\n",
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "DkFNFCCDppf6",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "if (os.path.isfile(\"dataset.pkl\")==False):\n",
        "  file=open(\"hin.txt\",'r',encoding='utf-8')\n",
        "  content=file.read()\n",
        "  file.close()\n",
        "#Reading the file\n",
        "#file=open(\"hin.txt\",'r',encoding='utf-8')\n",
        "#content=file.read()\n",
        "#file.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "UUcRUeheru4C",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "if (os.path.isfile(\"dataset.pkl\")==False):\n",
        "  file=open(\"enghindi.txt\",'r',encoding='utf-16')\n",
        "  content2=file.read()\n",
        "  file.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "McnR0gFIuux0",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#End of Sentence.\n",
        "eos=\" \"+\"</s>\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "PqleDsISppgF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 292
        },
        "outputId": "68eb681b-7056-4722-a5f0-cc0d53636ed9"
      },
      "cell_type": "code",
      "source": [
        "def save_clean_data(sentences, filename):\n",
        "  dump(sentences, open(filename, 'wb'))\n",
        "  print('Saved: %s' % filename)\n",
        "  \n",
        "def load_clean_sentences(filename):\n",
        "\treturn load(open(filename, 'rb'))\n",
        "\n",
        "import string\n",
        "import re\n",
        "from pickle import dump\n",
        "from unicodedata import normalize\n",
        "from numpy import array\n",
        "  \n",
        "if (os.path.isfile(\"dataset.pkl\")==False):\n",
        "  # load doc into memory\n",
        "  def load_doc(filename,encode_format):\n",
        "    # open the file as read only\n",
        "    file = open(filename, mode='rt', encoding=encode_format)\n",
        "    # read all text\n",
        "    text = file.read()\n",
        "    # close the file\n",
        "    file.close()\n",
        "    return text\n",
        "\n",
        "  # split a loaded document into sentences\n",
        "  def to_pairs(doc):\n",
        "    lines = doc.strip().split('\\n')\n",
        "    pairs = [line.split('\\t') for line in  lines]\n",
        "    return pairs\n",
        "\n",
        "  # clean a list of lines\n",
        "  def clean_pairs(lines,):\n",
        "    cleaned = list()\n",
        "    # prepare regex for char filtering\n",
        "    re_punc = re.compile('[।%s]' % re.escape(string.punctuation))\n",
        "    re_print = re.compile('[^%s]' % re.escape(string.printable))\n",
        "    for pair in lines:\n",
        "      clean_pair = list()\n",
        "      for line in pair:\n",
        "        # tokenize on white space\n",
        "        line = line.split()\n",
        "        # remove punctuation from each token\n",
        "        line = [re_punc.sub('', w) for w in line]\n",
        "        # remove tokens with numbers in them\n",
        "        #line = [word for word in line if word.isalpha()]\n",
        "        #line=re.sub('[।]', '', line)\n",
        "        # store as string\n",
        "        line=(' '.join(line))\n",
        "        line=line+eos\n",
        "        clean_pair.append(line.strip())\n",
        "      cleaned.append(clean_pair)\n",
        "    return array(cleaned)\n",
        "\n",
        "  # save a list of clean sentences to file\n",
        "  def save_clean_data(sentences, filename):\n",
        "    dump(sentences, open(filename, 'wb'))\n",
        "    print('Saved: %s' % filename)\n",
        "\n",
        "  # load dataset\n",
        "  filename = 'hin.txt'\n",
        "  doc = load_doc(filename,encode_format='utf-8')\n",
        "  # split into english-german pairs\n",
        "  pairs = to_pairs(doc)\n",
        "  del doc\n",
        "\n",
        "  filename2 = 'enghindi.txt'\n",
        "  doc2 = load_doc(filename2,encode_format='utf-16')\n",
        "  # split into english-german pairs\n",
        "  pairs2 = to_pairs(doc2)\n",
        "  del doc2\n",
        "\n",
        "  print(type(pairs),pairs[0:2],pairs2[0:2])\n",
        "  # clean sentences\n",
        "  clean_pairs = clean_pairs(pairs)\n",
        "  lines=pairs2\n",
        "  cleaned = list()\n",
        "  re_punc = re.compile('[।%s]' % re.escape(string.punctuation))\n",
        "  re_print = re.compile('[^%s]' % re.escape(string.printable))\n",
        "  for pair in lines:\n",
        "    clean_pair = list()\n",
        "    for line in pair:\n",
        "      line=line.strip()\n",
        "      # tokenize on white space\n",
        "      line = line.split()\n",
        "      # remove punctuation from each token\n",
        "      line = [re_punc.sub('', w) for w in line]\n",
        "      # remove tokens with numbers in them\n",
        "      #line = [word for word in line if word.isalpha()]\n",
        "      #line=re.sub('[।]', '', line)\n",
        "      # store as string\n",
        "      line=(' '.join(line))\n",
        "      line=line+eos\n",
        "      clean_pair.append(line.strip())\n",
        "    cleaned.append(clean_pair)\n",
        "  clean_pairs2=(array(cleaned))\n",
        "  print(type(clean_pairs2),clean_pairs2.shape,type(clean_pairs),clean_pairs.shape,(np.concatenate((clean_pairs2, clean_pairs))).shape)\n",
        "  # save clean pairs to file\n",
        "  print (\"Number of clean pairs\",clean_pairs.shape[0])\n",
        "  # spot check\n",
        "  for i in range(10):\n",
        "    print('[%s] => [%s]' % (clean_pairs[i,0], clean_pairs[i,1]))\n",
        "  clean_pairs=(np.concatenate((clean_pairs2, clean_pairs)))\n",
        "  print(clean_pairs[1][1],len(((clean_pairs[1][1].split()))))\n",
        "  save_clean_data(clean_pairs, 'clean_pairs.pkl')\n",
        "  del clean_pairs,clean_pairs2,lines,re_punc,re_print,cleaned,pairs,pairs2"
      ],
      "execution_count": 93,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'list'> [['Help!', 'बचाओ!'], ['Jump.', 'उछलो.']] [['fresh breath and shining teeth enhance your personality .', 'ताजा साँसें और चमचमाते दाँत आपके व्यक्तित्व को निखारते हैं ।'], ['your self-confidence also increases with teeth .', 'दाँतों से आपका आत्मविश्\\u200dवास भी बढ़ता है ।']]\n",
            "<class 'numpy.ndarray'> (77145, 2) <class 'numpy.ndarray'> (2867, 2) (80012, 2)\n",
            "Number of clean pairs 2867\n",
            "[Help </s>] => [बचाओ </s>]\n",
            "[Jump </s>] => [उछलो </s>]\n",
            "[Jump </s>] => [कूदो </s>]\n",
            "[Jump </s>] => [छलांग </s>]\n",
            "[Hello </s>] => [नमस्ते </s>]\n",
            "[Hello </s>] => [नमस्कार </s>]\n",
            "[Cheers </s>] => [वाहवाह </s>]\n",
            "[Cheers </s>] => [चियर्स </s>]\n",
            "[Got it </s>] => [समझे कि नहीं </s>]\n",
            "[Im OK </s>] => [मैं ठीक हूँ </s>]\n",
            "दाँतों से आपका आत्मविश्‍वास भी बढ़ता है  </s> 8\n",
            "Saved: clean_pairs.pkl\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "b4Wrq1UFppgU",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "1b0d63ff-a187-4eca-fb4c-6ace31b6a8de"
      },
      "cell_type": "code",
      "source": [
        "from pickle import load\n",
        "from pickle import dump\n",
        "from numpy.random import shuffle\n",
        "\n",
        "if (os.path.isfile(\"dataset.pkl\")==False):\n",
        "  # load a clean dataset\n",
        "  def load_clean_sentences(filename):\n",
        "    return load(open(filename, 'rb'))\n",
        "\n",
        "  # save a list of clean sentences to file\n",
        "  def save_clean_data(sentences, filename):\n",
        "    dump(sentences, open(filename, 'wb'))\n",
        "    print('Saved: %s' % filename)\n",
        "\n",
        "  # load dataset\n",
        "  raw_dataset = load_clean_sentences('clean_pairs.pkl')\n",
        "  #del os.remove(\"clean_pairs.pkl\")\n",
        "  # reduce dataset size\n",
        "  n_sentences = raw_dataset.shape[0]\n",
        "  print (n_sentences)\n",
        "  #Subsetting to 10000 records.\n",
        "  #n_sentences=dataset.shape[0]\n",
        "  # random shuffle\n",
        "  random.Random(4).shuffle(raw_dataset)\n",
        "  dataset = raw_dataset[:n_sentences, :]\n",
        "  print(dataset.shape)\n",
        "  random.Random(4).shuffle(dataset)\n",
        "  del raw_dataset\n",
        "# split into train/test\n",
        "#train, test = dataset[:2800], dataset[2800:]\n",
        "# save\n",
        "#save_clean_data(dataset, 'dataset.pkl')\n",
        "#save_clean_data(train, 'english-german-train.pkl')\n",
        "#save_clean_data(test, 'english-german-test.pkl')"
      ],
      "execution_count": 94,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "80012\n",
            "(80012, 2)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "xO7qN5hbsmLY",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#Trying to get sentences of length say hindi=12 words or less also with English words 10 or less.\n",
        "#limited=dataset[:10]\n",
        "#print((len(limited[2][0].split()) < 100))\n",
        "#print((len(limited[2][1].split()) < 100))\n",
        "#state1=(len(limited[2][0].split()) < 100)\n",
        "#state2=(len(limited[2][1].split())<100)\n",
        "#print(state1,state2,state1&state2)\n",
        "#print((len(limited[2][0].split()) < 100 & len(limited[2][1].split())<100))\n",
        "\n",
        "def get_sentences_subset(limited,source_len,target_len):\n",
        "    indexes_list=[]\n",
        "    for indexes in range(0,limited.shape[0]):\n",
        "        #print(len(limited[i][0].split()),len(limited[i][1].split()))\n",
        "        eng_len=len(limited[indexes][0].split()) \n",
        "        hin_len=len(limited[indexes][1].split())\n",
        "        state1=(eng_len<=target_len)\n",
        "        state2=(hin_len<=source_len)\n",
        "        final=state2&state1\n",
        "        #print(eng_len,hin_len,final)\n",
        "        #print(state1,state2,final)\n",
        "        if (final):\n",
        "            indexes_list.append(indexes)\n",
        "    #print(indexes_list,type(indexes_list))\n",
        "    return(limited[indexes_list])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "SsCKkEHjspAS",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 14586
        },
        "outputId": "b2023254-c406-4b39-a702-d1018f4b693b"
      },
      "cell_type": "code",
      "source": [
        "if (os.path.isfile(\"dataset.pkl\")==False):\n",
        "  print(\"Total sentences\", dataset.shape[0])\n",
        "  print(\"How many words long sentence in both language you need?\")\n",
        "  sentence_words=input()\n",
        "  sentence_words=int(sentence_words)\n",
        "  #print(\"How many words long sentence in Target Language you need?\")\n",
        "  #target_sentence_words=input()\n",
        "  #target_sentence_words=int(target_sentence_words)\n",
        "  raw_dataset_subset=get_sentences_subset(dataset,sentence_words,sentence_words)\n",
        "  print(\"There are Number of Sentences are matching above criteria\",raw_dataset_subset.shape)\n",
        "  print(\"########################################################################\")\n",
        "  print(\"How many sentences you need?\")\n",
        "  n_sentences=input()\n",
        "  n_sentences=int(n_sentences)\n",
        "  #Subsetting to n sentences.\n",
        "  #n_sentences=5000\n",
        "  random.Random(4).shuffle(raw_dataset_subset)\n",
        "  dataset = raw_dataset_subset[:n_sentences, :]\n",
        "  random.Random(4).shuffle(dataset)\n",
        "  print(raw_dataset_subset.shape,dataset,dataset.shape)\n",
        "  save_clean_data(dataset, 'dataset.pkl')\n",
        "#i=[0,2,3]\n",
        "#print(\"Limited\",limited,i)\n",
        "#print(\"\\n Indexed\",limited[i],i)"
      ],
      "execution_count": 96,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total sentences 80012\n",
            "How many words long sentence in both language you need?\n",
            "10\n",
            "There are Number of Sentences are matching above criteria (14211, 2)\n",
            "########################################################################\n",
            "How many sentences you need?\n",
            "500\n",
            "(14211, 2) [['your selfconfidence also increases with teeth  </s>'\n",
            "  'दाँतों से आपका आत्मविश्\\u200dवास भी बढ़ता है  </s>']\n",
            " ['your selfconfidence also increases with teeth  </s>'\n",
            "  'दाँतों से आपका आत्मविश्\\u200dवास भी बढ़ता है  </s>']\n",
            " ['your selfconfidence also increases with teeth  </s>'\n",
            "  'दाँतों से आपका आत्मविश्\\u200dवास भी बढ़ता है  </s>']\n",
            " ['your selfconfidence also increases with teeth  </s>'\n",
            "  'दाँतों से आपका आत्मविश्\\u200dवास भी बढ़ता है  </s>']\n",
            " ['your selfconfidence also increases with teeth  </s>'\n",
            "  'दाँतों से आपका आत्मविश्\\u200dवास भी बढ़ता है  </s>']\n",
            " ['your selfconfidence also increases with teeth  </s>'\n",
            "  'दाँतों से आपका आत्मविश्\\u200dवास भी बढ़ता है  </s>']\n",
            " ['your selfconfidence also increases with teeth  </s>'\n",
            "  'दाँतों से आपका आत्मविश्\\u200dवास भी बढ़ता है  </s>']\n",
            " ['your selfconfidence also increases with teeth  </s>'\n",
            "  'दाँतों से आपका आत्मविश्\\u200dवास भी बढ़ता है  </s>']\n",
            " ['clean the mouth after meal  </s>' 'खाने के बाद मुँह साफ करें  </s>']\n",
            " ['your selfconfidence also increases with teeth  </s>'\n",
            "  'दाँतों से आपका आत्मविश्\\u200dवास भी बढ़ता है  </s>']\n",
            " ['with this stink comes from breath  </s>'\n",
            "  'इससे साँसों से बदबू आने लगती है  </s>']\n",
            " ['your selfconfidence also increases with teeth  </s>'\n",
            "  'दाँतों से आपका आत्मविश्\\u200dवास भी बढ़ता है  </s>']\n",
            " ['your selfconfidence also increases with teeth  </s>'\n",
            "  'दाँतों से आपका आत्मविश्\\u200dवास भी बढ़ता है  </s>']\n",
            " ['your selfconfidence also increases with teeth  </s>'\n",
            "  'दाँतों से आपका आत्मविश्\\u200dवास भी बढ़ता है  </s>']\n",
            " ['your selfconfidence also increases with teeth  </s>'\n",
            "  'दाँतों से आपका आत्मविश्\\u200dवास भी बढ़ता है  </s>']\n",
            " ['your selfconfidence also increases with teeth  </s>'\n",
            "  'दाँतों से आपका आत्मविश्\\u200dवास भी बढ़ता है  </s>']\n",
            " ['your selfconfidence also increases with teeth  </s>'\n",
            "  'दाँतों से आपका आत्मविश्\\u200dवास भी बढ़ता है  </s>']\n",
            " ['your selfconfidence also increases with teeth  </s>'\n",
            "  'दाँतों से आपका आत्मविश्\\u200dवास भी बढ़ता है  </s>']\n",
            " ['your selfconfidence also increases with teeth  </s>'\n",
            "  'दाँतों से आपका आत्मविश्\\u200dवास भी बढ़ता है  </s>']\n",
            " ['your selfconfidence also increases with teeth  </s>'\n",
            "  'दाँतों से आपका आत्मविश्\\u200dवास भी बढ़ता है  </s>']\n",
            " ['your selfconfidence also increases with teeth  </s>'\n",
            "  'दाँतों से आपका आत्मविश्\\u200dवास भी बढ़ता है  </s>']\n",
            " ['your selfconfidence also increases with teeth  </s>'\n",
            "  'दाँतों से आपका आत्मविश्\\u200dवास भी बढ़ता है  </s>']\n",
            " ['clean the mouth after meal  </s>' 'खाने के बाद मुँह साफ करें  </s>']\n",
            " ['your selfconfidence also increases with teeth  </s>'\n",
            "  'दाँतों से आपका आत्मविश्\\u200dवास भी बढ़ता है  </s>']\n",
            " ['your selfconfidence also increases with teeth  </s>'\n",
            "  'दाँतों से आपका आत्मविश्\\u200dवास भी बढ़ता है  </s>']\n",
            " ['clean your teeth properly  </s>' 'दाँतों को ठीक से साफ करें  </s>']\n",
            " ['your selfconfidence also increases with teeth  </s>'\n",
            "  'दाँतों से आपका आत्मविश्\\u200dवास भी बढ़ता है  </s>']\n",
            " ['get the teeth checkedup regularly  </s>'\n",
            "  'नियमित रूप से कराएँ दाँतों की जाँच  </s>']\n",
            " ['with this stink comes from breath  </s>'\n",
            "  'इससे साँसों से बदबू आने लगती है  </s>']\n",
            " ['your selfconfidence also increases with teeth  </s>'\n",
            "  'दाँतों से आपका आत्मविश्\\u200dवास भी बढ़ता है  </s>']\n",
            " ['your selfconfidence also increases with teeth  </s>'\n",
            "  'दाँतों से आपका आत्मविश्\\u200dवास भी बढ़ता है  </s>']\n",
            " ['your selfconfidence also increases with teeth  </s>'\n",
            "  'दाँतों से आपका आत्मविश्\\u200dवास भी बढ़ता है  </s>']\n",
            " ['your selfconfidence also increases with teeth  </s>'\n",
            "  'दाँतों से आपका आत्मविश्\\u200dवास भी बढ़ता है  </s>']\n",
            " ['clean your teeth properly  </s>' 'दाँतों को ठीक से साफ करें  </s>']\n",
            " ['with this stink comes from breath  </s>'\n",
            "  'इससे साँसों से बदबू आने लगती है  </s>']\n",
            " ['with this stink comes from breath  </s>'\n",
            "  'इससे साँसों से बदबू आने लगती है  </s>']\n",
            " ['your selfconfidence also increases with teeth  </s>'\n",
            "  'दाँतों से आपका आत्मविश्\\u200dवास भी बढ़ता है  </s>']\n",
            " ['clean your teeth properly  </s>' 'दाँतों को ठीक से साफ करें  </s>']\n",
            " ['your selfconfidence also increases with teeth  </s>'\n",
            "  'दाँतों से आपका आत्मविश्\\u200dवास भी बढ़ता है  </s>']\n",
            " ['your selfconfidence also increases with teeth  </s>'\n",
            "  'दाँतों से आपका आत्मविश्\\u200dवास भी बढ़ता है  </s>']\n",
            " ['your selfconfidence also increases with teeth  </s>'\n",
            "  'दाँतों से आपका आत्मविश्\\u200dवास भी बढ़ता है  </s>']\n",
            " ['your selfconfidence also increases with teeth  </s>'\n",
            "  'दाँतों से आपका आत्मविश्\\u200dवास भी बढ़ता है  </s>']\n",
            " ['your selfconfidence also increases with teeth  </s>'\n",
            "  'दाँतों से आपका आत्मविश्\\u200dवास भी बढ़ता है  </s>']\n",
            " ['if there is health there is everything  </s>' 'सेहत है तो सब है  </s>']\n",
            " ['if there is health there is everything  </s>' 'सेहत है तो सब है  </s>']\n",
            " ['clean your teeth properly  </s>' 'दाँतों को ठीक से साफ करें  </s>']\n",
            " ['get the teeth checkedup regularly  </s>'\n",
            "  'नियमित रूप से कराएँ दाँतों की जाँच  </s>']\n",
            " ['with this stink comes from breath  </s>'\n",
            "  'इससे साँसों से बदबू आने लगती है  </s>']\n",
            " ['your selfconfidence also increases with teeth  </s>'\n",
            "  'दाँतों से आपका आत्मविश्\\u200dवास भी बढ़ता है  </s>']\n",
            " ['with this stink comes from breath  </s>'\n",
            "  'इससे साँसों से बदबू आने लगती है  </s>']\n",
            " ['if there is health there is everything  </s>' 'सेहत है तो सब है  </s>']\n",
            " ['clean your teeth properly  </s>' 'दाँतों को ठीक से साफ करें  </s>']\n",
            " ['with this stink comes from breath  </s>'\n",
            "  'इससे साँसों से बदबू आने लगती है  </s>']\n",
            " ['your selfconfidence also increases with teeth  </s>'\n",
            "  'दाँतों से आपका आत्मविश्\\u200dवास भी बढ़ता है  </s>']\n",
            " ['get the teeth checkedup regularly  </s>'\n",
            "  'नियमित रूप से कराएँ दाँतों की जाँच  </s>']\n",
            " ['your selfconfidence also increases with teeth  </s>'\n",
            "  'दाँतों से आपका आत्मविश्\\u200dवास भी बढ़ता है  </s>']\n",
            " ['with this stink comes from breath  </s>'\n",
            "  'इससे साँसों से बदबू आने लगती है  </s>']\n",
            " ['with this stink comes from breath  </s>'\n",
            "  'इससे साँसों से बदबू आने लगती है  </s>']\n",
            " ['clean your teeth properly  </s>' 'दाँतों को ठीक से साफ करें  </s>']\n",
            " ['with this stink comes from breath  </s>'\n",
            "  'इससे साँसों से बदबू आने लगती है  </s>']\n",
            " ['saliva is formed by chewing the chewing gum  </s>'\n",
            "  'चुइंग गम चबाने से लार बनती है  </s>']\n",
            " ['with this stink comes from breath  </s>'\n",
            "  'इससे साँसों से बदबू आने लगती है  </s>']\n",
            " ['bacteria attack fast if the mouth dries up  </s>'\n",
            "  'मुँह सूखने पर बैक्टीरिया हमला तेज कर देते हैं  </s>']\n",
            " ['your selfconfidence also increases with teeth  </s>'\n",
            "  'दाँतों से आपका आत्मविश्\\u200dवास भी बढ़ता है  </s>']\n",
            " ['with this stink comes from breath  </s>'\n",
            "  'इससे साँसों से बदबू आने लगती है  </s>']\n",
            " ['your selfconfidence also increases with teeth  </s>'\n",
            "  'दाँतों से आपका आत्मविश्\\u200dवास भी बढ़ता है  </s>']\n",
            " ['if there is health there is everything  </s>' 'सेहत है तो सब है  </s>']\n",
            " ['if there is health there is everything  </s>' 'सेहत है तो सब है  </s>']\n",
            " ['with this stink comes from breath  </s>'\n",
            "  'इससे साँसों से बदबू आने लगती है  </s>']\n",
            " ['with this stink comes from breath  </s>'\n",
            "  'इससे साँसों से बदबू आने लगती है  </s>']\n",
            " ['your selfconfidence also increases with teeth  </s>'\n",
            "  'दाँतों से आपका आत्मविश्\\u200dवास भी बढ़ता है  </s>']\n",
            " ['clean your teeth properly  </s>' 'दाँतों को ठीक से साफ करें  </s>']\n",
            " ['with this stink comes from breath  </s>'\n",
            "  'इससे साँसों से बदबू आने लगती है  </s>']\n",
            " ['get the teeth checkedup regularly  </s>'\n",
            "  'नियमित रूप से कराएँ दाँतों की जाँच  </s>']\n",
            " ['with this stink comes from breath  </s>'\n",
            "  'इससे साँसों से बदबू आने लगती है  </s>']\n",
            " ['saliva is formed by chewing the chewing gum  </s>'\n",
            "  'चुइंग गम चबाने से लार बनती है  </s>']\n",
            " ['your selfconfidence also increases with teeth  </s>'\n",
            "  'दाँतों से आपका आत्मविश्\\u200dवास भी बढ़ता है  </s>']\n",
            " ['clean the teeth every week with this mixture  </s>'\n",
            "  'हर हफ्ते इस मिश्रण से दाँतों को साफ करें  </s>']\n",
            " ['your selfconfidence also increases with teeth  </s>'\n",
            "  'दाँतों से आपका आत्मविश्\\u200dवास भी बढ़ता है  </s>']\n",
            " ['clean your teeth properly  </s>' 'दाँतों को ठीक से साफ करें  </s>']\n",
            " ['your selfconfidence also increases with teeth  </s>'\n",
            "  'दाँतों से आपका आत्मविश्\\u200dवास भी बढ़ता है  </s>']\n",
            " ['your selfconfidence also increases with teeth  </s>'\n",
            "  'दाँतों से आपका आत्मविश्\\u200dवास भी बढ़ता है  </s>']\n",
            " ['with this stink comes from breath  </s>'\n",
            "  'इससे साँसों से बदबू आने लगती है  </s>']\n",
            " ['clean your teeth properly  </s>' 'दाँतों को ठीक से साफ करें  </s>']\n",
            " ['your selfconfidence also increases with teeth  </s>'\n",
            "  'दाँतों से आपका आत्मविश्\\u200dवास भी बढ़ता है  </s>']\n",
            " ['clean your teeth properly  </s>' 'दाँतों को ठीक से साफ करें  </s>']\n",
            " ['your selfconfidence also increases with teeth  </s>'\n",
            "  'दाँतों से आपका आत्मविश्\\u200dवास भी बढ़ता है  </s>']\n",
            " ['with this stink comes from breath  </s>'\n",
            "  'इससे साँसों से बदबू आने लगती है  </s>']\n",
            " ['your selfconfidence also increases with teeth  </s>'\n",
            "  'दाँतों से आपका आत्मविश्\\u200dवास भी बढ़ता है  </s>']\n",
            " ['with this stink comes from breath  </s>'\n",
            "  'इससे साँसों से बदबू आने लगती है  </s>']\n",
            " ['your selfconfidence also increases with teeth  </s>'\n",
            "  'दाँतों से आपका आत्मविश्\\u200dवास भी बढ़ता है  </s>']\n",
            " ['your selfconfidence also increases with teeth  </s>'\n",
            "  'दाँतों से आपका आत्मविश्\\u200dवास भी बढ़ता है  </s>']\n",
            " ['does its influence increase in summer  </s>'\n",
            "  'क्या गर्मी में इसका प्रकोप बढ़ जाता है  </s>']\n",
            " ['i suffer from fever continuously  </s>'\n",
            "  'लगातार बुखार से पीड़ित हो  </s>']\n",
            " ['your selfconfidence also increases with teeth  </s>'\n",
            "  'दाँतों से आपका आत्मविश्\\u200dवास भी बढ़ता है  </s>']\n",
            " ['with this stink comes from breath  </s>'\n",
            "  'इससे साँसों से बदबू आने लगती है  </s>']\n",
            " ['with this stink comes from breath  </s>'\n",
            "  'इससे साँसों से बदबू आने लगती है  </s>']\n",
            " ['your selfconfidence also increases with teeth  </s>'\n",
            "  'दाँतों से आपका आत्मविश्\\u200dवास भी बढ़ता है  </s>']\n",
            " ['your selfconfidence also increases with teeth  </s>'\n",
            "  'दाँतों से आपका आत्मविश्\\u200dवास भी बढ़ता है  </s>']\n",
            " ['with this stink comes from breath  </s>'\n",
            "  'इससे साँसों से बदबू आने लगती है  </s>']\n",
            " ['does its influence increase in summer  </s>'\n",
            "  'क्या गर्मी में इसका प्रकोप बढ़ जाता है  </s>']\n",
            " ['i suffer from fever continuously  </s>'\n",
            "  'लगातार बुखार से पीड़ित हो  </s>']\n",
            " ['clean your teeth properly  </s>' 'दाँतों को ठीक से साफ करें  </s>']\n",
            " ['clean your teeth properly  </s>' 'दाँतों को ठीक से साफ करें  </s>']\n",
            " ['your selfconfidence also increases with teeth  </s>'\n",
            "  'दाँतों से आपका आत्मविश्\\u200dवास भी बढ़ता है  </s>']\n",
            " ['with this stink comes from breath  </s>'\n",
            "  'इससे साँसों से बदबू आने लगती है  </s>']\n",
            " ['with this stink comes from breath  </s>'\n",
            "  'इससे साँसों से बदबू आने लगती है  </s>']\n",
            " ['bacteria attack fast if the mouth dries up  </s>'\n",
            "  'मुँह सूखने पर बैक्टीरिया हमला तेज कर देते हैं  </s>']\n",
            " ['clean the mouth after meal  </s>' 'खाने के बाद मुँह साफ करें  </s>']\n",
            " ['your selfconfidence also increases with teeth  </s>'\n",
            "  'दाँतों से आपका आत्मविश्\\u200dवास भी बढ़ता है  </s>']\n",
            " ['take less salt and alcohol  </s>' 'नमक और शराब का सेवन कम करें  </s>']\n",
            " ['get the teeth checkedup regularly  </s>'\n",
            "  'नियमित रूप से कराएँ दाँतों की जाँच  </s>']\n",
            " ['your selfconfidence also increases with teeth  </s>'\n",
            "  'दाँतों से आपका आत्मविश्\\u200dवास भी बढ़ता है  </s>']\n",
            " ['clean your teeth properly  </s>' 'दाँतों को ठीक से साफ करें  </s>']\n",
            " ['with this stink comes from breath  </s>'\n",
            "  'इससे साँसों से बदबू आने लगती है  </s>']\n",
            " ['clean the mouth after meal  </s>' 'खाने के बाद मुँह साफ करें  </s>']\n",
            " ['does its influence increase in summer  </s>'\n",
            "  'क्या गर्मी में इसका प्रकोप बढ़ जाता है  </s>']\n",
            " ['your selfconfidence also increases with teeth  </s>'\n",
            "  'दाँतों से आपका आत्मविश्\\u200dवास भी बढ़ता है  </s>']\n",
            " ['clean the mouth after meal  </s>' 'खाने के बाद मुँह साफ करें  </s>']\n",
            " ['state the solutions of prevention  </s>' 'बचाव के उपाय बताएँ  </s>']\n",
            " ['your selfconfidence also increases with teeth  </s>'\n",
            "  'दाँतों से आपका आत्मविश्\\u200dवास भी बढ़ता है  </s>']\n",
            " ['avoid citrus food item  </s>' 'खट्टी चीजों के सेवन से बचें  </s>']\n",
            " ['clean the mouth after meal  </s>' 'खाने के बाद मुँह साफ करें  </s>']\n",
            " ['with this stink comes from breath  </s>'\n",
            "  'इससे साँसों से बदबू आने लगती है  </s>']\n",
            " ['if there is health there is everything  </s>' 'सेहत है तो सब है  </s>']\n",
            " ['chew the sugarfree chewing gum  </s>' 'चबाएँ शुगर रहित चुइंग गम  </s>']\n",
            " ['state the solutions of prevention  </s>' 'बचाव के उपाय बताएँ  </s>']\n",
            " ['clean the mouth after meal  </s>' 'खाने के बाद मुँह साफ करें  </s>']\n",
            " ['your selfconfidence also increases with teeth  </s>'\n",
            "  'दाँतों से आपका आत्मविश्\\u200dवास भी बढ़ता है  </s>']\n",
            " ['get the teeth checkedup regularly  </s>'\n",
            "  'नियमित रूप से कराएँ दाँतों की जाँच  </s>']\n",
            " ['your selfconfidence also increases with teeth  </s>'\n",
            "  'दाँतों से आपका आत्मविश्\\u200dवास भी बढ़ता है  </s>']\n",
            " ['your selfconfidence also increases with teeth  </s>'\n",
            "  'दाँतों से आपका आत्मविश्\\u200dवास भी बढ़ता है  </s>']\n",
            " ['chew the sugarfree chewing gum  </s>' 'चबाएँ शुगर रहित चुइंग गम  </s>']\n",
            " ['clean the mouth after meal  </s>' 'खाने के बाद मुँह साफ करें  </s>']\n",
            " ['clean your teeth properly  </s>' 'दाँतों को ठीक से साफ करें  </s>']\n",
            " ['your selfconfidence also increases with teeth  </s>'\n",
            "  'दाँतों से आपका आत्मविश्\\u200dवास भी बढ़ता है  </s>']\n",
            " ['my child is suffering from both diarrhoea and malaria  </s>'\n",
            "  'मेरा बच्चा दस्त और बुखार दोनों पीड़ित है  </s>']\n",
            " ['kalajar malaria or high fiver may have happened  </s>'\n",
            "  'कालाजार मलेरिया या फिर हाई फीवर हो सकता है  </s>']\n",
            " ['clean your teeth properly  </s>' 'दाँतों को ठीक से साफ करें  </s>']\n",
            " ['does kalajar occur because of sun  </s>'\n",
            "  'धूप के कारण क्या कालाजार रोग होता है  </s>']\n",
            " ['these retain the flexibility in the body  </s>'\n",
            "  'इनसे शरीर में लचीलापन बना रहता है  </s>']\n",
            " ['chew the sugarfree chewing gum  </s>' 'चबाएँ शुगर रहित चुइंग गम  </s>']\n",
            " ['if there is health there is everything  </s>' 'सेहत है तो सब है  </s>']\n",
            " ['your selfconfidence also increases with teeth  </s>'\n",
            "  'दाँतों से आपका आत्मविश्\\u200dवास भी बढ़ता है  </s>']\n",
            " ['your selfconfidence also increases with teeth  </s>'\n",
            "  'दाँतों से आपका आत्मविश्\\u200dवास भी बढ़ता है  </s>']\n",
            " ['does its influence increase in summer  </s>'\n",
            "  'क्या गर्मी में इसका प्रकोप बढ़ जाता है  </s>']\n",
            " ['your selfconfidence also increases with teeth  </s>'\n",
            "  'दाँतों से आपका आत्मविश्\\u200dवास भी बढ़ता है  </s>']\n",
            " ['bacteria attack fast if the mouth dries up  </s>'\n",
            "  'मुँह सूखने पर बैक्टीरिया हमला तेज कर देते हैं  </s>']\n",
            " ['your selfconfidence also increases with teeth  </s>'\n",
            "  'दाँतों से आपका आत्मविश्\\u200dवास भी बढ़ता है  </s>']\n",
            " ['saliva is formed by chewing the chewing gum  </s>'\n",
            "  'चुइंग गम चबाने से लार बनती है  </s>']\n",
            " ['chew the sugarfree chewing gum  </s>' 'चबाएँ शुगर रहित चुइंग गम  </s>']\n",
            " ['i suffer from fever continuously  </s>'\n",
            "  'लगातार बुखार से पीड़ित हो  </s>']\n",
            " ['not only teeth starts shining with this  </s>'\n",
            "  'इससे न केवल दाँत चमचमाने लगते हैं  </s>']\n",
            " ['clean your teeth properly  </s>' 'दाँतों को ठीक से साफ करें  </s>']\n",
            " ['your selfconfidence also increases with teeth  </s>'\n",
            "  'दाँतों से आपका आत्मविश्\\u200dवास भी बढ़ता है  </s>']\n",
            " ['saliva is formed by chewing the chewing gum  </s>'\n",
            "  'चुइंग गम चबाने से लार बनती है  </s>']\n",
            " ['clean your teeth properly  </s>' 'दाँतों को ठीक से साफ करें  </s>']\n",
            " ['with this stink comes from breath  </s>'\n",
            "  'इससे साँसों से बदबू आने लगती है  </s>']\n",
            " ['your selfconfidence also increases with teeth  </s>'\n",
            "  'दाँतों से आपका आत्मविश्\\u200dवास भी बढ़ता है  </s>']\n",
            " ['avoid citrus food item  </s>' 'खट्टी चीजों के सेवन से बचें  </s>']\n",
            " ['your selfconfidence also increases with teeth  </s>'\n",
            "  'दाँतों से आपका आत्मविश्\\u200dवास भी बढ़ता है  </s>']\n",
            " ['clean the mouth after meal  </s>' 'खाने के बाद मुँह साफ करें  </s>']\n",
            " ['avoid citrus food item  </s>' 'खट्टी चीजों के सेवन से बचें  </s>']\n",
            " ['clean your teeth properly  </s>' 'दाँतों को ठीक से साफ करें  </s>']\n",
            " ['your selfconfidence also increases with teeth  </s>'\n",
            "  'दाँतों से आपका आत्मविश्\\u200dवास भी बढ़ता है  </s>']\n",
            " ['with this stink comes from breath  </s>'\n",
            "  'इससे साँसों से बदबू आने लगती है  </s>']\n",
            " ['clean the mouth after meal  </s>' 'खाने के बाद मुँह साफ करें  </s>']\n",
            " ['with this stink comes from breath  </s>'\n",
            "  'इससे साँसों से बदबू आने लगती है  </s>']\n",
            " ['your selfconfidence also increases with teeth  </s>'\n",
            "  'दाँतों से आपका आत्मविश्\\u200dवास भी बढ़ता है  </s>']\n",
            " ['administer the dpt vaccine to the child  </s>'\n",
            "  'बच्चे को डीपीटी का टीका लगायें  </s>']\n",
            " ['avoid citrus food item  </s>' 'खट्टी चीजों के सेवन से बचें  </s>']\n",
            " ['chew the sugarfree chewing gum  </s>' 'चबाएँ शुगर रहित चुइंग गम  </s>']\n",
            " ['with this stink comes from breath  </s>'\n",
            "  'इससे साँसों से बदबू आने लगती है  </s>']\n",
            " ['xray and tcdc are normal  </s>' 'एक्सरे टीसीडीसी नार्मल है  </s>']\n",
            " ['clean your teeth properly  </s>' 'दाँतों को ठीक से साफ करें  </s>']\n",
            " ['clean your teeth properly  </s>' 'दाँतों को ठीक से साफ करें  </s>']\n",
            " ['clean your teeth properly  </s>' 'दाँतों को ठीक से साफ करें  </s>']\n",
            " ['state the solutions of prevention  </s>' 'बचाव के उपाय बताएँ  </s>']\n",
            " ['chew the sugarfree chewing gum  </s>' 'चबाएँ शुगर रहित चुइंग गम  </s>']\n",
            " ['your selfconfidence also increases with teeth  </s>'\n",
            "  'दाँतों से आपका आत्मविश्\\u200dवास भी बढ़ता है  </s>']\n",
            " ['state the solutions of prevention  </s>' 'बचाव के उपाय बताएँ  </s>']\n",
            " ['does its influence increase in summer  </s>'\n",
            "  'क्या गर्मी में इसका प्रकोप बढ़ जाता है  </s>']\n",
            " ['with this stink comes from breath  </s>'\n",
            "  'इससे साँसों से बदबू आने लगती है  </s>']\n",
            " ['your selfconfidence also increases with teeth  </s>'\n",
            "  'दाँतों से आपका आत्मविश्\\u200dवास भी बढ़ता है  </s>']\n",
            " ['chew the sugarfree chewing gum  </s>' 'चबाएँ शुगर रहित चुइंग गम  </s>']\n",
            " ['your selfconfidence also increases with teeth  </s>'\n",
            "  'दाँतों से आपका आत्मविश्\\u200dवास भी बढ़ता है  </s>']\n",
            " ['your selfconfidence also increases with teeth  </s>'\n",
            "  'दाँतों से आपका आत्मविश्\\u200dवास भी बढ़ता है  </s>']\n",
            " ['clean your teeth properly  </s>' 'दाँतों को ठीक से साफ करें  </s>']\n",
            " ['keep doing light physical activities  </s>'\n",
            "  'करते रहें हल्की फुल्की शारीरिक गतिविधियाँ  </s>']\n",
            " ['take less salt and alcohol  </s>' 'नमक और शराब का सेवन कम करें  </s>']\n",
            " ['your selfconfidence also increases with teeth  </s>'\n",
            "  'दाँतों से आपका आत्मविश्\\u200dवास भी बढ़ता है  </s>']\n",
            " ['kalajar malaria or high fiver may have happened  </s>'\n",
            "  'कालाजार मलेरिया या फिर हाई फीवर हो सकता है  </s>']\n",
            " ['get the teeth checkedup regularly  </s>'\n",
            "  'नियमित रूप से कराएँ दाँतों की जाँच  </s>']\n",
            " ['clean your teeth properly  </s>' 'दाँतों को ठीक से साफ करें  </s>']\n",
            " ['your selfconfidence also increases with teeth  </s>'\n",
            "  'दाँतों से आपका आत्मविश्\\u200dवास भी बढ़ता है  </s>']\n",
            " ['your selfconfidence also increases with teeth  </s>'\n",
            "  'दाँतों से आपका आत्मविश्\\u200dवास भी बढ़ता है  </s>']\n",
            " ['i suffer from fever continuously  </s>'\n",
            "  'लगातार बुखार से पीड़ित हो  </s>']\n",
            " ['chew the sugarfree chewing gum  </s>' 'चबाएँ शुगर रहित चुइंग गम  </s>']\n",
            " ['with this stink comes from breath  </s>'\n",
            "  'इससे साँसों से बदबू आने लगती है  </s>']\n",
            " ['chew the sugarfree chewing gum  </s>' 'चबाएँ शुगर रहित चुइंग गम  </s>']\n",
            " ['if there is health there is everything  </s>' 'सेहत है तो सब है  </s>']\n",
            " ['chew the sugarfree chewing gum  </s>' 'चबाएँ शुगर रहित चुइंग गम  </s>']\n",
            " ['bacteria attack fast if the mouth dries up  </s>'\n",
            "  'मुँह सूखने पर बैक्टीरिया हमला तेज कर देते हैं  </s>']\n",
            " ['clean the mouth after meal  </s>' 'खाने के बाद मुँह साफ करें  </s>']\n",
            " ['get the teeth checkedup regularly  </s>'\n",
            "  'नियमित रूप से कराएँ दाँतों की जाँच  </s>']\n",
            " ['with this stink comes from breath  </s>'\n",
            "  'इससे साँसों से बदबू आने लगती है  </s>']\n",
            " ['clean the teeth every week with this mixture  </s>'\n",
            "  'हर हफ्ते इस मिश्रण से दाँतों को साफ करें  </s>']\n",
            " ['does kalajar occur because of sun  </s>'\n",
            "  'धूप के कारण क्या कालाजार रोग होता है  </s>']\n",
            " ['stop smoking  </s>' 'बंद करें धूम्रपान  </s>']\n",
            " ['your selfconfidence also increases with teeth  </s>'\n",
            "  'दाँतों से आपका आत्मविश्\\u200dवास भी बढ़ता है  </s>']\n",
            " ['if somebody suffers from black cataract in the family  </s>'\n",
            "  'यदि परिवार में किसी को काला मोतियाबिंद हो  </s>']\n",
            " ['with this stink comes from breath  </s>'\n",
            "  'इससे साँसों से बदबू आने लगती है  </s>']\n",
            " ['your selfconfidence also increases with teeth  </s>'\n",
            "  'दाँतों से आपका आत्मविश्\\u200dवास भी बढ़ता है  </s>']\n",
            " ['clean the mouth after meal  </s>' 'खाने के बाद मुँह साफ करें  </s>']\n",
            " ['i suffer from fever continuously  </s>'\n",
            "  'लगातार बुखार से पीड़ित हो  </s>']\n",
            " ['drink plenty of water  </s>' 'खूब पानी पीएँ  </s>']\n",
            " ['clean your teeth properly  </s>' 'दाँतों को ठीक से साफ करें  </s>']\n",
            " ['with this stink comes from breath  </s>'\n",
            "  'इससे साँसों से बदबू आने लगती है  </s>']\n",
            " ['get the teeth checkedup regularly  </s>'\n",
            "  'नियमित रूप से कराएँ दाँतों की जाँच  </s>']\n",
            " ['get the teeth checkedup regularly  </s>'\n",
            "  'नियमित रूप से कराएँ दाँतों की जाँच  </s>']\n",
            " ['clean the mouth after meal  </s>' 'खाने के बाद मुँह साफ करें  </s>']\n",
            " ['your selfconfidence also increases with teeth  </s>'\n",
            "  'दाँतों से आपका आत्मविश्\\u200dवास भी बढ़ता है  </s>']\n",
            " ['your selfconfidence also increases with teeth  </s>'\n",
            "  'दाँतों से आपका आत्मविश्\\u200dवास भी बढ़ता है  </s>']\n",
            " ['clean the teeth every week with this mixture  </s>'\n",
            "  'हर हफ्ते इस मिश्रण से दाँतों को साफ करें  </s>']\n",
            " ['clean the teeth every week with this mixture  </s>'\n",
            "  'हर हफ्ते इस मिश्रण से दाँतों को साफ करें  </s>']\n",
            " ['not only teeth starts shining with this  </s>'\n",
            "  'इससे न केवल दाँत चमचमाने लगते हैं  </s>']\n",
            " ['clean your teeth properly  </s>' 'दाँतों को ठीक से साफ करें  </s>']\n",
            " ['chew the sugarfree chewing gum  </s>' 'चबाएँ शुगर रहित चुइंग गम  </s>']\n",
            " ['surgery depends on the stage of the cancer  </s>'\n",
            "  'शल्यचिकित्सा कैंसर के चरण पर निर्भर करता है  </s>']\n",
            " ['take less salt and alcohol  </s>' 'नमक और शराब का सेवन कम करें  </s>']\n",
            " ['your selfconfidence also increases with teeth  </s>'\n",
            "  'दाँतों से आपका आत्मविश्\\u200dवास भी बढ़ता है  </s>']\n",
            " ['with this stink comes from breath  </s>'\n",
            "  'इससे साँसों से बदबू आने लगती है  </s>']\n",
            " ['clean the mouth after meal  </s>' 'खाने के बाद मुँह साफ करें  </s>']\n",
            " ['your selfconfidence also increases with teeth  </s>'\n",
            "  'दाँतों से आपका आत्मविश्\\u200dवास भी बढ़ता है  </s>']\n",
            " ['with this stink comes from breath  </s>'\n",
            "  'इससे साँसों से बदबू आने लगती है  </s>']\n",
            " ['with this stink comes from breath  </s>'\n",
            "  'इससे साँसों से बदबू आने लगती है  </s>']\n",
            " ['state the solutions of prevention  </s>' 'बचाव के उपाय बताएँ  </s>']\n",
            " ['if there is health there is everything  </s>' 'सेहत है तो सब है  </s>']\n",
            " ['state the solutions of prevention  </s>' 'बचाव के उपाय बताएँ  </s>']\n",
            " ['stop smoking  </s>' 'बंद करें धूम्रपान  </s>']\n",
            " ['these retain the flexibility in the body  </s>'\n",
            "  'इनसे शरीर में लचीलापन बना रहता है  </s>']\n",
            " ['with this stink comes from breath  </s>'\n",
            "  'इससे साँसों से बदबू आने लगती है  </s>']\n",
            " ['your selfconfidence also increases with teeth  </s>'\n",
            "  'दाँतों से आपका आत्मविश्\\u200dवास भी बढ़ता है  </s>']\n",
            " ['if there is health there is everything  </s>' 'सेहत है तो सब है  </s>']\n",
            " ['your selfconfidence also increases with teeth  </s>'\n",
            "  'दाँतों से आपका आत्मविश्\\u200dवास भी बढ़ता है  </s>']\n",
            " ['drink plenty of water  </s>' 'खूब पानी पीएँ  </s>']\n",
            " ['your selfconfidence also increases with teeth  </s>'\n",
            "  'दाँतों से आपका आत्मविश्\\u200dवास भी बढ़ता है  </s>']\n",
            " ['state the solutions of prevention  </s>' 'बचाव के उपाय बताएँ  </s>']\n",
            " ['paralysis may also attack in this situation  </s>'\n",
            "  'इस परिस्थिति में लकवा भी मार सकता है  </s>']\n",
            " ['xray and tcdc are normal  </s>' 'एक्सरे टीसीडीसी नार्मल है  </s>']\n",
            " ['your selfconfidence also increases with teeth  </s>'\n",
            "  'दाँतों से आपका आत्मविश्\\u200dवास भी बढ़ता है  </s>']\n",
            " ['bacteria attack fast if the mouth dries up  </s>'\n",
            "  'मुँह सूखने पर बैक्टीरिया हमला तेज कर देते हैं  </s>']\n",
            " ['your selfconfidence also increases with teeth  </s>'\n",
            "  'दाँतों से आपका आत्मविश्\\u200dवास भी बढ़ता है  </s>']\n",
            " ['xray and tcdc are normal  </s>' 'एक्सरे टीसीडीसी नार्मल है  </s>']\n",
            " ['your selfconfidence also increases with teeth  </s>'\n",
            "  'दाँतों से आपका आत्मविश्\\u200dवास भी बढ़ता है  </s>']\n",
            " ['clean the teeth every week with this mixture  </s>'\n",
            "  'हर हफ्ते इस मिश्रण से दाँतों को साफ करें  </s>']\n",
            " ['with this stink comes from breath  </s>'\n",
            "  'इससे साँसों से बदबू आने लगती है  </s>']\n",
            " ['your selfconfidence also increases with teeth  </s>'\n",
            "  'दाँतों से आपका आत्मविश्\\u200dवास भी बढ़ता है  </s>']\n",
            " ['with this stink comes from breath  </s>'\n",
            "  'इससे साँसों से बदबू आने लगती है  </s>']\n",
            " ['your selfconfidence also increases with teeth  </s>'\n",
            "  'दाँतों से आपका आत्मविश्\\u200dवास भी बढ़ता है  </s>']\n",
            " ['stop smoking  </s>' 'बंद करें धूम्रपान  </s>']\n",
            " ['your selfconfidence also increases with teeth  </s>'\n",
            "  'दाँतों से आपका आत्मविश्\\u200dवास भी बढ़ता है  </s>']\n",
            " ['get the teeth checkedup regularly  </s>'\n",
            "  'नियमित रूप से कराएँ दाँतों की जाँच  </s>']\n",
            " ['your selfconfidence also increases with teeth  </s>'\n",
            "  'दाँतों से आपका आत्मविश्\\u200dवास भी बढ़ता है  </s>']\n",
            " ['state the solutions of prevention  </s>' 'बचाव के उपाय बताएँ  </s>']\n",
            " ['your selfconfidence also increases with teeth  </s>'\n",
            "  'दाँतों से आपका आत्मविश्\\u200dवास भी बढ़ता है  </s>']\n",
            " ['chew the sugarfree chewing gum  </s>' 'चबाएँ शुगर रहित चुइंग गम  </s>']\n",
            " ['with this stink comes from breath  </s>'\n",
            "  'इससे साँसों से बदबू आने लगती है  </s>']\n",
            " ['with this stink comes from breath  </s>'\n",
            "  'इससे साँसों से बदबू आने लगती है  </s>']\n",
            " ['your selfconfidence also increases with teeth  </s>'\n",
            "  'दाँतों से आपका आत्मविश्\\u200dवास भी बढ़ता है  </s>']\n",
            " ['clean the mouth after meal  </s>' 'खाने के बाद मुँह साफ करें  </s>']\n",
            " ['your selfconfidence also increases with teeth  </s>'\n",
            "  'दाँतों से आपका आत्मविश्\\u200dवास भी बढ़ता है  </s>']\n",
            " ['if there is health there is everything  </s>' 'सेहत है तो सब है  </s>']\n",
            " ['your selfconfidence also increases with teeth  </s>'\n",
            "  'दाँतों से आपका आत्मविश्\\u200dवास भी बढ़ता है  </s>']\n",
            " ['if somebody suffers from black cataract in the family  </s>'\n",
            "  'यदि परिवार में किसी को काला मोतियाबिंद हो  </s>']\n",
            " ['children are suffering from prickly heat what to do  </s>'\n",
            "  'बच्चों को घमौंरी हो रही है क्या करें  </s>']\n",
            " ['get the teeth checkedup regularly  </s>'\n",
            "  'नियमित रूप से कराएँ दाँतों की जाँच  </s>']\n",
            " ['but you will also be safe from eye diseases  </s>'\n",
            "  'बल्कि आप नेत्ररोगों से भी बचे रहेंगे  </s>']\n",
            " ['your selfconfidence also increases with teeth  </s>'\n",
            "  'दाँतों से आपका आत्मविश्\\u200dवास भी बढ़ता है  </s>']\n",
            " ['chew the sugarfree chewing gum  </s>' 'चबाएँ शुगर रहित चुइंग गम  </s>']\n",
            " ['if there is health there is everything  </s>' 'सेहत है तो सब है  </s>']\n",
            " ['does its influence increase in summer  </s>'\n",
            "  'क्या गर्मी में इसका प्रकोप बढ़ जाता है  </s>']\n",
            " ['if there is health there is everything  </s>' 'सेहत है तो सब है  </s>']\n",
            " ['kalajar malaria or high fiver may have happened  </s>'\n",
            "  'कालाजार मलेरिया या फिर हाई फीवर हो सकता है  </s>']\n",
            " ['when the extra pressure is more in the eyes  </s>'\n",
            "  'जब आँखों में अतिरिक्त दबाव ज्यादा हो  </s>']\n",
            " ['with this stink comes from breath  </s>'\n",
            "  'इससे साँसों से बदबू आने लगती है  </s>']\n",
            " ['administer the dpt vaccine to the child  </s>'\n",
            "  'बच्चे को डीपीटी का टीका लगायें  </s>']\n",
            " ['clean your teeth properly  </s>' 'दाँतों को ठीक से साफ करें  </s>']\n",
            " ['chew the sugarfree chewing gum  </s>' 'चबाएँ शुगर रहित चुइंग गम  </s>']\n",
            " ['clean your teeth properly  </s>' 'दाँतों को ठीक से साफ करें  </s>']\n",
            " ['get the teeth checkedup with the dentists regularly  </s>'\n",
            "  'दंतचिकित्सक से दाँतों की जाँच नियमित रूप से कराएँ  </s>']\n",
            " ['clean your teeth properly  </s>' 'दाँतों को ठीक से साफ करें  </s>']\n",
            " ['your selfconfidence also increases with teeth  </s>'\n",
            "  'दाँतों से आपका आत्मविश्\\u200dवास भी बढ़ता है  </s>']\n",
            " ['administer the dpt vaccine to the child  </s>'\n",
            "  'बच्चे को डीपीटी का टीका लगायें  </s>']\n",
            " ['with this stink comes from breath  </s>'\n",
            "  'इससे साँसों से बदबू आने लगती है  </s>']\n",
            " ['state the solutions of prevention  </s>' 'बचाव के उपाय बताएँ  </s>']\n",
            " ['state the solutions of prevention  </s>' 'बचाव के उपाय बताएँ  </s>']\n",
            " ['does kalajar occur because of sun  </s>'\n",
            "  'धूप के कारण क्या कालाजार रोग होता है  </s>']\n",
            " ['measure the fever every four hours  </s>'\n",
            "  'चार चार घंटे पर फीवर को मापें  </s>']\n",
            " ['your selfconfidence also increases with teeth  </s>'\n",
            "  'दाँतों से आपका आत्मविश्\\u200dवास भी बढ़ता है  </s>']\n",
            " ['clean the teeth every week with this mixture  </s>'\n",
            "  'हर हफ्ते इस मिश्रण से दाँतों को साफ करें  </s>']\n",
            " ['clean your teeth properly  </s>' 'दाँतों को ठीक से साफ करें  </s>']\n",
            " ['these retain the flexibility in the body  </s>'\n",
            "  'इनसे शरीर में लचीलापन बना रहता है  </s>']\n",
            " ['xray and tcdc are normal  </s>' 'एक्सरे टीसीडीसी नार्मल है  </s>']\n",
            " ['surgery depends on the stage of the cancer  </s>'\n",
            "  'शल्यचिकित्सा कैंसर के चरण पर निर्भर करता है  </s>']\n",
            " ['these retain the flexibility in the body  </s>'\n",
            "  'इनसे शरीर में लचीलापन बना रहता है  </s>']\n",
            " ['if there is health there is everything  </s>' 'सेहत है तो सब है  </s>']\n",
            " ['your selfconfidence also increases with teeth  </s>'\n",
            "  'दाँतों से आपका आत्मविश्\\u200dवास भी बढ़ता है  </s>']\n",
            " ['state the solutions of prevention  </s>' 'बचाव के उपाय बताएँ  </s>']\n",
            " ['your selfconfidence also increases with teeth  </s>'\n",
            "  'दाँतों से आपका आत्मविश्\\u200dवास भी बढ़ता है  </s>']\n",
            " ['measure the fever every four hours  </s>'\n",
            "  'चार चार घंटे पर फीवर को मापें  </s>']\n",
            " ['does kalajar occur because of sun  </s>'\n",
            "  'धूप के कारण क्या कालाजार रोग होता है  </s>']\n",
            " ['my child is suffering from both diarrhoea and malaria  </s>'\n",
            "  'मेरा बच्चा दस्त और बुखार दोनों पीड़ित है  </s>']\n",
            " ['clean your teeth properly  </s>' 'दाँतों को ठीक से साफ करें  </s>']\n",
            " ['keep doing light physical activities  </s>'\n",
            "  'करते रहें हल्की फुल्की शारीरिक गतिविधियाँ  </s>']\n",
            " ['does kalajar occur because of sun  </s>'\n",
            "  'धूप के कारण क्या कालाजार रोग होता है  </s>']\n",
            " ['measure the fever every four hours  </s>'\n",
            "  'चार चार घंटे पर फीवर को मापें  </s>']\n",
            " ['these retain the flexibility in the body  </s>'\n",
            "  'इनसे शरीर में लचीलापन बना रहता है  </s>']\n",
            " ['xray and tcdc are normal  </s>' 'एक्सरे टीसीडीसी नार्मल है  </s>']\n",
            " ['get the teeth checkedup regularly  </s>'\n",
            "  'नियमित रूप से कराएँ दाँतों की जाँच  </s>']\n",
            " ['your selfconfidence also increases with teeth  </s>'\n",
            "  'दाँतों से आपका आत्मविश्\\u200dवास भी बढ़ता है  </s>']\n",
            " ['take less salt and alcohol  </s>' 'नमक और शराब का सेवन कम करें  </s>']\n",
            " ['if there is health there is everything  </s>' 'सेहत है तो सब है  </s>']\n",
            " ['i suffer from fever continuously  </s>'\n",
            "  'लगातार बुखार से पीड़ित हो  </s>']\n",
            " ['if there is health there is everything  </s>' 'सेहत है तो सब है  </s>']\n",
            " ['with this stink comes from breath  </s>'\n",
            "  'इससे साँसों से बदबू आने लगती है  </s>']\n",
            " ['your selfconfidence also increases with teeth  </s>'\n",
            "  'दाँतों से आपका आत्मविश्\\u200dवास भी बढ़ता है  </s>']\n",
            " ['chew the sugarfree chewing gum  </s>' 'चबाएँ शुगर रहित चुइंग गम  </s>']\n",
            " ['administer the dpt vaccine to the child  </s>'\n",
            "  'बच्चे को डीपीटी का टीका लगायें  </s>']\n",
            " ['your selfconfidence also increases with teeth  </s>'\n",
            "  'दाँतों से आपका आत्मविश्\\u200dवास भी बढ़ता है  </s>']\n",
            " ['administer the dpt vaccine to the child  </s>'\n",
            "  'बच्चे को डीपीटी का टीका लगायें  </s>']\n",
            " ['surgery depends on the stage of the cancer  </s>'\n",
            "  'शल्यचिकित्सा कैंसर के चरण पर निर्भर करता है  </s>']\n",
            " ['chew the sugarfree chewing gum  </s>' 'चबाएँ शुगर रहित चुइंग गम  </s>']\n",
            " ['state the solutions of prevention  </s>' 'बचाव के उपाय बताएँ  </s>']\n",
            " ['with this stink comes from breath  </s>'\n",
            "  'इससे साँसों से बदबू आने लगती है  </s>']\n",
            " ['stop smoking  </s>' 'बंद करें धूम्रपान  </s>']\n",
            " ['its treatment is available in all the hospitals  </s>'\n",
            "  'इसका उपचार सभी अस्पतालों में है  </s>']\n",
            " ['i suffer from fever continuously  </s>'\n",
            "  'लगातार बुखार से पीड़ित हो  </s>']\n",
            " ['clean the teeth every week with this mixture  </s>'\n",
            "  'हर हफ्ते इस मिश्रण से दाँतों को साफ करें  </s>']\n",
            " ['with this stink comes from breath  </s>'\n",
            "  'इससे साँसों से बदबू आने लगती है  </s>']\n",
            " ['its treatment is available in all the hospitals  </s>'\n",
            "  'इसका उपचार सभी अस्पतालों में है  </s>']\n",
            " ['your selfconfidence also increases with teeth  </s>'\n",
            "  'दाँतों से आपका आत्मविश्\\u200dवास भी बढ़ता है  </s>']\n",
            " ['i suffer from fever continuously  </s>'\n",
            "  'लगातार बुखार से पीड़ित हो  </s>']\n",
            " ['saliva is formed by chewing the chewing gum  </s>'\n",
            "  'चुइंग गम चबाने से लार बनती है  </s>']\n",
            " ['state the solutions of prevention  </s>' 'बचाव के उपाय बताएँ  </s>']\n",
            " ['chew the sugarfree chewing gum  </s>' 'चबाएँ शुगर रहित चुइंग गम  </s>']\n",
            " ['with this stink comes from breath  </s>'\n",
            "  'इससे साँसों से बदबू आने लगती है  </s>']\n",
            " ['not only teeth starts shining with this  </s>'\n",
            "  'इससे न केवल दाँत चमचमाने लगते हैं  </s>']\n",
            " ['your selfconfidence also increases with teeth  </s>'\n",
            "  'दाँतों से आपका आत्मविश्\\u200dवास भी बढ़ता है  </s>']\n",
            " ['measure the fever every four hours  </s>'\n",
            "  'चार चार घंटे पर फीवर को मापें  </s>']\n",
            " ['measure the fever every four hours  </s>'\n",
            "  'चार चार घंटे पर फीवर को मापें  </s>']\n",
            " ['get the teeth checkedup regularly  </s>'\n",
            "  'नियमित रूप से कराएँ दाँतों की जाँच  </s>']\n",
            " ['saliva is formed by chewing the chewing gum  </s>'\n",
            "  'चुइंग गम चबाने से लार बनती है  </s>']\n",
            " ['with this stink comes from breath  </s>'\n",
            "  'इससे साँसों से बदबू आने लगती है  </s>']\n",
            " ['with this stink comes from breath  </s>'\n",
            "  'इससे साँसों से बदबू आने लगती है  </s>']\n",
            " ['your selfconfidence also increases with teeth  </s>'\n",
            "  'दाँतों से आपका आत्मविश्\\u200dवास भी बढ़ता है  </s>']\n",
            " ['how does it happens  </s>' 'यह कैसे होता है  </s>']\n",
            " ['xray and tcdc are normal  </s>' 'एक्सरे टीसीडीसी नार्मल है  </s>']\n",
            " ['keep doing light physical activities  </s>'\n",
            "  'करते रहें हल्की फुल्की शारीरिक गतिविधियाँ  </s>']\n",
            " ['your selfconfidence also increases with teeth  </s>'\n",
            "  'दाँतों से आपका आत्मविश्\\u200dवास भी बढ़ता है  </s>']\n",
            " ['keep doing light physical activities  </s>'\n",
            "  'करते रहें हल्की फुल्की शारीरिक गतिविधियाँ  </s>']\n",
            " ['there is a tablet also available for this now  </s>'\n",
            "  'इसके लिए अब टेबलेट भी उपलब्ध हैं  </s>']\n",
            " ['state the solutions of prevention  </s>' 'बचाव के उपाय बताएँ  </s>']\n",
            " ['your selfconfidence also increases with teeth  </s>'\n",
            "  'दाँतों से आपका आत्मविश्\\u200dवास भी बढ़ता है  </s>']\n",
            " ['clean your teeth properly  </s>' 'दाँतों को ठीक से साफ करें  </s>']\n",
            " ['try your best to quit it  </s>' 'इसे छोड़ने की पूरी कोशिश करें  </s>']\n",
            " ['with this stink comes from breath  </s>'\n",
            "  'इससे साँसों से बदबू आने लगती है  </s>']\n",
            " ['your selfconfidence also increases with teeth  </s>'\n",
            "  'दाँतों से आपका आत्मविश्\\u200dवास भी बढ़ता है  </s>']\n",
            " ['its treatment is available in all the hospitals  </s>'\n",
            "  'इसका उपचार सभी अस्पतालों में है  </s>']\n",
            " ['state the solutions of prevention  </s>' 'बचाव के उपाय बताएँ  </s>']\n",
            " ['your selfconfidence also increases with teeth  </s>'\n",
            "  'दाँतों से आपका आत्मविश्\\u200dवास भी बढ़ता है  </s>']\n",
            " ['get the teeth checkedup regularly  </s>'\n",
            "  'नियमित रूप से कराएँ दाँतों की जाँच  </s>']\n",
            " ['with this stink comes from breath  </s>'\n",
            "  'इससे साँसों से बदबू आने लगती है  </s>']\n",
            " ['with this stink comes from breath  </s>'\n",
            "  'इससे साँसों से बदबू आने लगती है  </s>']\n",
            " ['your selfconfidence also increases with teeth  </s>'\n",
            "  'दाँतों से आपका आत्मविश्\\u200dवास भी बढ़ता है  </s>']\n",
            " ['saliva is formed by chewing the chewing gum  </s>'\n",
            "  'चुइंग गम चबाने से लार बनती है  </s>']\n",
            " ['my child is suffering from both diarrhoea and malaria  </s>'\n",
            "  'मेरा बच्चा दस्त और बुखार दोनों पीड़ित है  </s>']\n",
            " ['try your best to quit it  </s>' 'इसे छोड़ने की पूरी कोशिश करें  </s>']\n",
            " ['its treatment is available in all the hospitals  </s>'\n",
            "  'इसका उपचार सभी अस्पतालों में है  </s>']\n",
            " ['measure the fever every four hours  </s>'\n",
            "  'चार चार घंटे पर फीवर को मापें  </s>']\n",
            " ['avoid citrus food item  </s>' 'खट्टी चीजों के सेवन से बचें  </s>']\n",
            " ['your selfconfidence also increases with teeth  </s>'\n",
            "  'दाँतों से आपका आत्मविश्\\u200dवास भी बढ़ता है  </s>']\n",
            " ['clean the teeth every week with this mixture  </s>'\n",
            "  'हर हफ्ते इस मिश्रण से दाँतों को साफ करें  </s>']\n",
            " ['with this stink comes from breath  </s>'\n",
            "  'इससे साँसों से बदबू आने लगती है  </s>']\n",
            " ['with this stink comes from breath  </s>'\n",
            "  'इससे साँसों से बदबू आने लगती है  </s>']\n",
            " ['state the solutions of prevention  </s>' 'बचाव के उपाय बताएँ  </s>']\n",
            " ['not only teeth starts shining with this  </s>'\n",
            "  'इससे न केवल दाँत चमचमाने लगते हैं  </s>']\n",
            " ['drink plenty of water  </s>' 'खूब पानी पीएँ  </s>']\n",
            " ['does its influence increase in summer  </s>'\n",
            "  'क्या गर्मी में इसका प्रकोप बढ़ जाता है  </s>']\n",
            " ['keep doing light physical activities  </s>'\n",
            "  'करते रहें हल्की फुल्की शारीरिक गतिविधियाँ  </s>']\n",
            " ['drink pure water as much as you can  </s>'\n",
            "  'साफ पानी अधिक से अधिक पीयें  </s>']\n",
            " ['headache heaviness in the eyes problem in reading  </s>'\n",
            "  'सिर दर्द आँखों में भारीपन पढ़ने में परेशानी  </s>']\n",
            " ['state the solutions of prevention  </s>' 'बचाव के उपाय बताएँ  </s>']\n",
            " ['state the solutions of prevention  </s>' 'बचाव के उपाय बताएँ  </s>']\n",
            " ['drink plenty of water  </s>' 'खूब पानी पीएँ  </s>']\n",
            " ['state the solutions of prevention  </s>' 'बचाव के उपाय बताएँ  </s>']\n",
            " ['get the teeth checkedup regularly  </s>'\n",
            "  'नियमित रूप से कराएँ दाँतों की जाँच  </s>']\n",
            " ['a person becomes infected easily  </s>'\n",
            "  'व्यक्ति को बड़ी आसानी से संक्रमण होने लगते हैं  </s>']\n",
            " ['your selfconfidence also increases with teeth  </s>'\n",
            "  'दाँतों से आपका आत्मविश्\\u200dवास भी बढ़ता है  </s>']\n",
            " ['if there is health there is everything  </s>' 'सेहत है तो सब है  </s>']\n",
            " ['clean the teeth every week with this mixture  </s>'\n",
            "  'हर हफ्ते इस मिश्रण से दाँतों को साफ करें  </s>']\n",
            " ['your selfconfidence also increases with teeth  </s>'\n",
            "  'दाँतों से आपका आत्मविश्\\u200dवास भी बढ़ता है  </s>']\n",
            " ['state the solutions of prevention  </s>' 'बचाव के उपाय बताएँ  </s>']\n",
            " ['stiffness in body  </s>' 'शरीर में अकड़न  </s>']\n",
            " ['your selfconfidence also increases with teeth  </s>'\n",
            "  'दाँतों से आपका आत्मविश्\\u200dवास भी बढ़ता है  </s>']\n",
            " ['does its influence increase in summer  </s>'\n",
            "  'क्या गर्मी में इसका प्रकोप बढ़ जाता है  </s>']\n",
            " ['clean the teeth every week with this mixture  </s>'\n",
            "  'हर हफ्ते इस मिश्रण से दाँतों को साफ करें  </s>']\n",
            " ['with this stink comes from breath  </s>'\n",
            "  'इससे साँसों से बदबू आने लगती है  </s>']\n",
            " ['state the solutions of prevention  </s>' 'बचाव के उपाय बताएँ  </s>']\n",
            " ['its treatment is available in all the hospitals  </s>'\n",
            "  'इसका उपचार सभी अस्पतालों में है  </s>']\n",
            " ['saliva is formed by chewing the chewing gum  </s>'\n",
            "  'चुइंग गम चबाने से लार बनती है  </s>']\n",
            " ['with this stink comes from breath  </s>'\n",
            "  'इससे साँसों से बदबू आने लगती है  </s>']\n",
            " ['not only this white coat also forms on tongue  </s>'\n",
            "  'यही नहीं जीभ पर सफेद परत जम जाती है  </s>']\n",
            " ['with this stink comes from breath  </s>'\n",
            "  'इससे साँसों से बदबू आने लगती है  </s>']\n",
            " ['with this stink comes from breath  </s>'\n",
            "  'इससे साँसों से बदबू आने लगती है  </s>']\n",
            " ['clean your teeth properly  </s>' 'दाँतों को ठीक से साफ करें  </s>']\n",
            " ['with this stink comes from breath  </s>'\n",
            "  'इससे साँसों से बदबू आने लगती है  </s>']\n",
            " ['your selfconfidence also increases with teeth  </s>'\n",
            "  'दाँतों से आपका आत्मविश्\\u200dवास भी बढ़ता है  </s>']\n",
            " ['its treatment is available in all the hospitals  </s>'\n",
            "  'इसका उपचार सभी अस्पतालों में है  </s>']\n",
            " ['keep doing light physical activities  </s>'\n",
            "  'करते रहें हल्की फुल्की शारीरिक गतिविधियाँ  </s>']\n",
            " ['take less salt and alcohol  </s>' 'नमक और शराब का सेवन कम करें  </s>']\n",
            " ['clean the mouth after meal  </s>' 'खाने के बाद मुँह साफ करें  </s>']\n",
            " ['your selfconfidence also increases with teeth  </s>'\n",
            "  'दाँतों से आपका आत्मविश्\\u200dवास भी बढ़ता है  </s>']\n",
            " ['there is a tablet also available for this now  </s>'\n",
            "  'इसके लिए अब टेबलेट भी उपलब्ध हैं  </s>']\n",
            " ['if there is health there is everything  </s>' 'सेहत है तो सब है  </s>']\n",
            " ['there is a tablet also available for this now  </s>'\n",
            "  'इसके लिए अब टेबलेट भी उपलब्ध हैं  </s>']\n",
            " ['chew the sugarfree chewing gum  </s>' 'चबाएँ शुगर रहित चुइंग गम  </s>']\n",
            " ['these retain the flexibility in the body  </s>'\n",
            "  'इनसे शरीर में लचीलापन बना रहता है  </s>']\n",
            " ['clean the teeth every week with this mixture  </s>'\n",
            "  'हर हफ्ते इस मिश्रण से दाँतों को साफ करें  </s>']\n",
            " ['state the solutions of prevention  </s>' 'बचाव के उपाय बताएँ  </s>']\n",
            " ['avoid citrus food item  </s>' 'खट्टी चीजों के सेवन से बचें  </s>']\n",
            " ['cylindrical glasses have to be worn always  </s>'\n",
            "  'सिलिंडरिकल ग्लासेस को हमेशा पहनना है  </s>']\n",
            " ['with this stink comes from breath  </s>'\n",
            "  'इससे साँसों से बदबू आने लगती है  </s>']\n",
            " ['xray and tcdc are normal  </s>' 'एक्सरे टीसीडीसी नार्मल है  </s>']\n",
            " ['i suffer from fever continuously  </s>'\n",
            "  'लगातार बुखार से पीड़ित हो  </s>']\n",
            " ['measure the fever every four hours  </s>'\n",
            "  'चार चार घंटे पर फीवर को मापें  </s>']\n",
            " ['your selfconfidence also increases with teeth  </s>'\n",
            "  'दाँतों से आपका आत्मविश्\\u200dवास भी बढ़ता है  </s>']\n",
            " ['paralysis may also attack in this situation  </s>'\n",
            "  'इस परिस्थिति में लकवा भी मार सकता है  </s>']\n",
            " ['paralysis may also attack in this situation  </s>'\n",
            "  'इस परिस्थिति में लकवा भी मार सकता है  </s>']\n",
            " ['how does it happens  </s>' 'यह कैसे होता है  </s>']\n",
            " ['xray and tcdc are normal  </s>' 'एक्सरे टीसीडीसी नार्मल है  </s>']\n",
            " ['saliva is formed by chewing the chewing gum  </s>'\n",
            "  'चुइंग गम चबाने से लार बनती है  </s>']\n",
            " ['with this stink comes from breath  </s>'\n",
            "  'इससे साँसों से बदबू आने लगती है  </s>']\n",
            " ['i suffer from fever continuously  </s>'\n",
            "  'लगातार बुखार से पीड़ित हो  </s>']\n",
            " ['not only teeth starts shining with this  </s>'\n",
            "  'इससे न केवल दाँत चमचमाने लगते हैं  </s>']\n",
            " ['clean the mouth after meal  </s>' 'खाने के बाद मुँह साफ करें  </s>']\n",
            " ['does kalajar occur because of sun  </s>'\n",
            "  'धूप के कारण क्या कालाजार रोग होता है  </s>']\n",
            " ['chew the sugarfree chewing gum  </s>' 'चबाएँ शुगर रहित चुइंग गम  </s>']\n",
            " ['if somebody suffers from black cataract in the family  </s>'\n",
            "  'यदि परिवार में किसी को काला मोतियाबिंद हो  </s>']\n",
            " ['surgery depends on the stage of the cancer  </s>'\n",
            "  'शल्यचिकित्सा कैंसर के चरण पर निर्भर करता है  </s>']\n",
            " ['your selfconfidence also increases with teeth  </s>'\n",
            "  'दाँतों से आपका आत्मविश्\\u200dवास भी बढ़ता है  </s>']\n",
            " ['your selfconfidence also increases with teeth  </s>'\n",
            "  'दाँतों से आपका आत्मविश्\\u200dवास भी बढ़ता है  </s>']\n",
            " ['a person becomes infected easily  </s>'\n",
            "  'व्यक्ति को बड़ी आसानी से संक्रमण होने लगते हैं  </s>']\n",
            " ['your selfconfidence also increases with teeth  </s>'\n",
            "  'दाँतों से आपका आत्मविश्\\u200dवास भी बढ़ता है  </s>']\n",
            " ['take less salt and alcohol  </s>' 'नमक और शराब का सेवन कम करें  </s>']\n",
            " ['clean the teeth every week with this mixture  </s>'\n",
            "  'हर हफ्ते इस मिश्रण से दाँतों को साफ करें  </s>']\n",
            " ['get the teeth checkedup regularly  </s>'\n",
            "  'नियमित रूप से कराएँ दाँतों की जाँच  </s>']\n",
            " ['not only this white coat also forms on tongue  </s>'\n",
            "  'यही नहीं जीभ पर सफेद परत जम जाती है  </s>']\n",
            " ['bacteria attack fast if the mouth dries up  </s>'\n",
            "  'मुँह सूखने पर बैक्टीरिया हमला तेज कर देते हैं  </s>']\n",
            " ['here the main yogic diagnosis for paralysis are presented  </s>'\n",
            "  'यहाँ पर पक्षाघात हेतु प्रमुख यौगिक निदान प्रस्तुत हैं  </s>']\n",
            " ['clean the teeth every week with this mixture  </s>'\n",
            "  'हर हफ्ते इस मिश्रण से दाँतों को साफ करें  </s>']\n",
            " ['with this stink comes from breath  </s>'\n",
            "  'इससे साँसों से बदबू आने लगती है  </s>']\n",
            " ['xray and tcdc are normal  </s>' 'एक्सरे टीसीडीसी नार्मल है  </s>']\n",
            " ['clean the mouth after meal  </s>' 'खाने के बाद मुँह साफ करें  </s>']\n",
            " ['state the solutions of prevention  </s>' 'बचाव के उपाय बताएँ  </s>']\n",
            " ['your selfconfidence also increases with teeth  </s>'\n",
            "  'दाँतों से आपका आत्मविश्\\u200dवास भी बढ़ता है  </s>']\n",
            " ['if somebody suffers from black cataract in the family  </s>'\n",
            "  'यदि परिवार में किसी को काला मोतियाबिंद हो  </s>']\n",
            " ['kalajar malaria or high fiver may have happened  </s>'\n",
            "  'कालाजार मलेरिया या फिर हाई फीवर हो सकता है  </s>']\n",
            " ['kalajar malaria or high fiver may have happened  </s>'\n",
            "  'कालाजार मलेरिया या फिर हाई फीवर हो सकता है  </s>']\n",
            " ['every type of hepatitis is different  </s>'\n",
            "  'हर प्रकार का हेपेटाइटिस अलग है  </s>']\n",
            " ['h attacks the immune system of the body  </s>'\n",
            "  'एचआईवी शरीर की प्रतिरक्षण प्रणाली को आघात पहुँचाता है  </s>']\n",
            " ['get the teeth checkedup regularly  </s>'\n",
            "  'नियमित रूप से कराएँ दाँतों की जाँच  </s>']\n",
            " ['clean your teeth properly  </s>' 'दाँतों को ठीक से साफ करें  </s>']\n",
            " ['does kalajar occur because of sun  </s>'\n",
            "  'धूप के कारण क्या कालाजार रोग होता है  </s>']\n",
            " ['clean the mouth after meal  </s>' 'खाने के बाद मुँह साफ करें  </s>']\n",
            " ['chew the sugarfree chewing gum  </s>' 'चबाएँ शुगर रहित चुइंग गम  </s>']\n",
            " ['keep doing light physical activities  </s>'\n",
            "  'करते रहें हल्की फुल्की शारीरिक गतिविधियाँ  </s>']\n",
            " ['your selfconfidence also increases with teeth  </s>'\n",
            "  'दाँतों से आपका आत्मविश्\\u200dवास भी बढ़ता है  </s>']\n",
            " ['state the solutions of prevention  </s>' 'बचाव के उपाय बताएँ  </s>']\n",
            " ['h attacks the immune system of the body  </s>'\n",
            "  'एचआईवी शरीर की प्रतिरक्षण प्रणाली को आघात पहुँचाता है  </s>']\n",
            " ['if there is health there is everything  </s>' 'सेहत है तो सब है  </s>']\n",
            " ['pupil getting gray or white  </s>' 'भूरा या सफेद पुतली होना  </s>']\n",
            " ['your selfconfidence also increases with teeth  </s>'\n",
            "  'दाँतों से आपका आत्मविश्\\u200dवास भी बढ़ता है  </s>']\n",
            " ['your selfconfidence also increases with teeth  </s>'\n",
            "  'दाँतों से आपका आत्मविश्\\u200dवास भी बढ़ता है  </s>']\n",
            " ['cylindrical glasses have to be worn always  </s>'\n",
            "  'सिलिंडरिकल ग्लासेस को हमेशा पहनना है  </s>']\n",
            " ['if somebody suffers from black cataract in the family  </s>'\n",
            "  'यदि परिवार में किसी को काला मोतियाबिंद हो  </s>']\n",
            " ['with this stink comes from breath  </s>'\n",
            "  'इससे साँसों से बदबू आने लगती है  </s>']\n",
            " ['with this stink comes from breath  </s>'\n",
            "  'इससे साँसों से बदबू आने लगती है  </s>']\n",
            " ['frequent loose motion for a month  </s>'\n",
            "  'एक माह से लगातार दस्त आना  </s>']\n",
            " ['does its influence increase in summer  </s>'\n",
            "  'क्या गर्मी में इसका प्रकोप बढ़ जाता है  </s>']\n",
            " ['your selfconfidence also increases with teeth  </s>'\n",
            "  'दाँतों से आपका आत्मविश्\\u200dवास भी बढ़ता है  </s>']\n",
            " ['your selfconfidence also increases with teeth  </s>'\n",
            "  'दाँतों से आपका आत्मविश्\\u200dवास भी बढ़ता है  </s>']\n",
            " ['your selfconfidence also increases with teeth  </s>'\n",
            "  'दाँतों से आपका आत्मविश्\\u200dवास भी बढ़ता है  </s>']\n",
            " ['does its influence increase in summer  </s>'\n",
            "  'क्या गर्मी में इसका प्रकोप बढ़ जाता है  </s>']\n",
            " ['your selfconfidence also increases with teeth  </s>'\n",
            "  'दाँतों से आपका आत्मविश्\\u200dवास भी बढ़ता है  </s>']\n",
            " ['clean the teeth every week with this mixture  </s>'\n",
            "  'हर हफ्ते इस मिश्रण से दाँतों को साफ करें  </s>']\n",
            " ['if somebody suffers from black cataract in the family  </s>'\n",
            "  'यदि परिवार में किसी को काला मोतियाबिंद हो  </s>']\n",
            " ['with this stink comes from breath  </s>'\n",
            "  'इससे साँसों से बदबू आने लगती है  </s>']\n",
            " ['i suffer from fever continuously  </s>'\n",
            "  'लगातार बुखार से पीड़ित हो  </s>']\n",
            " ['stop smoking  </s>' 'बंद करें धूम्रपान  </s>']] (500, 2)\n",
            "Saved: dataset.pkl\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "xJOdtrDnsuYM",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def load_clean_sentences(filename):\n",
        "\treturn load(open(filename, 'rb'))\n",
        "dataset = load_clean_sentences('dataset.pkl')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "SZRfjr0Dsw46",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "raw_dataset_subset = None\n",
        "raw_dataset =None\n",
        "del raw_dataset_subset,raw_dataset"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "wpFEWgRTppgn",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "879f6c7f-3d13-40f9-fd1c-d618f9933470"
      },
      "cell_type": "code",
      "source": [
        "#Check the data sample\n",
        "dataset.shape"
      ],
      "execution_count": 99,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(500, 2)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 99
        }
      ]
    },
    {
      "metadata": {
        "id": "4jiw4GFrppg2",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#Converting it to tuples.\n",
        "dataset_list=(list(tuple(map(tuple, dataset))))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "kxgShonlpphH",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#English Sentence List\n",
        "english_sentences_list=list(dataset[:,0])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "0ImL8sf7pphP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "148e628f-d9cd-4dc1-fb83-28cb806d9fb6"
      },
      "cell_type": "code",
      "source": [
        "english_sentences_list[0]='Please make yourself at home'+eos\n",
        "english_sentences_list[0]"
      ],
      "execution_count": 102,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Please make yourself at home </s>'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 102
        }
      ]
    },
    {
      "metadata": {
        "id": "WX2jSTcbpphj",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#English Sentence Unique Word List and Length of Vocabulary\n",
        "english_unique_words=set((' '.join(english_sentences_list)).split())\n",
        "english_vocab_len=len(set((' '.join(english_sentences_list)).split()))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "2Ilvfe8Lppht",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#Hindi Sentence List\n",
        "hindi_sentences_list=list(dataset[:,1])\n",
        "hindi_sentences_list[0]='इसको अपना घर ही समझो'+eos"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "uohwpb-hpph7",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#Hindi Sentence Unique Word List and Length of Vocabulary\n",
        "hindi_unique_words=set((' '.join(hindi_sentences_list)).split())\n",
        "hindi_vocab_len=len(set((' '.join(hindi_sentences_list)).split()))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ChFZsTeYppiA",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#Creating Dictionary with Unknown and Pad elements\n",
        "english_dictionary=dict(zip(sorted(english_unique_words) + ['<unk>', '<pad>'], list(range(len(english_unique_words) + 2))))\n",
        "hindi_dictionary=dict(zip(sorted(hindi_unique_words) + ['<unk>', '<pad>'], list(range(len(hindi_unique_words) + 2))))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "eRiSaO6ALbyL",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#Making Sure that 0th Value is pad value to ensure masking.\n",
        "def return_adjusted_dictionary(english_dictionary):\n",
        "  zero_value=0\n",
        "  Key_pad='<pad>'\n",
        "  for Key, Value in english_dictionary.items():\n",
        "    if Value == zero_value:\n",
        "      Key_zero=Key\n",
        "      #print (Key)\n",
        "    if Key == Key_pad:\n",
        "      value_pad=Value\n",
        "      #print (Value)\n",
        "  english_dictionary.update({Key_pad: 0, Key_zero: value_pad})\n",
        "  return(english_dictionary)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "NN9SydyGTs_I",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "hindi_dictionary=return_adjusted_dictionary(hindi_dictionary)\n",
        "english_dictionary=return_adjusted_dictionary(english_dictionary)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "WKS48Rk6ppiH",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#Reverse Dictionary for both languages\n",
        "revere_dictionary_hindi=dict((v,k) for k,v in hindi_dictionary.items())\n",
        "revere_dictionary_english=dict((v,k) for k,v in english_dictionary.items())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "XixDvWS-ppiN",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "67bb90f6-75be-4c24-8575-0aa2d850ce52"
      },
      "cell_type": "code",
      "source": [
        "#Storing the index of padding value in variables to add it going ahead.\n",
        "english_padding_value=english_dictionary['<pad>']\n",
        "hindi_padding_value=hindi_dictionary['<pad>']\n",
        "print(english_padding_value,hindi_padding_value)"
      ],
      "execution_count": 110,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0 0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "S5vLHzZVppiZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "ce09212d-c938-400a-c47b-1527bc28f295"
      },
      "cell_type": "code",
      "source": [
        "#This going to be the global variable with maximum number of words found in a sentence\n",
        "max_english_words=max(len(line.split()) for line in english_sentences_list)\n",
        "max_hindi_words=max(len(line.split()) for line in hindi_sentences_list)\n",
        "print(max_english_words,max_hindi_words)"
      ],
      "execution_count": 111,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "10 10\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "073iRdhtppin",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def get_padded_encoding(sentences_list,language_dictionary,max_language_words):\n",
        "    padding_value=language_dictionary['<pad>']\n",
        "    language_array=[]\n",
        "    #Iterate over List.\n",
        "    for sentence in sentences_list:\n",
        "        #Replaces English words with English Vocabulary Indexes and Hindi with Hindi Vocabulary Indexes.\n",
        "        #logic: if a word not in dictionary enters, it will be replaced by unk key value.\n",
        "        single_sentence_array=[]\n",
        "        for word in sentence.split(): \n",
        "            try:\n",
        "                #single_sentence_array=([language_dictionary[word] for word in sentence.split()])\n",
        "                single_sentence_array.append(language_dictionary[word])\n",
        "            except KeyError:\n",
        "                unk='<unk>'\n",
        "                single_sentence_array.append(language_dictionary[unk])\n",
        "        #Find the length of english_single_sentence_array\n",
        "        length_single_sentence=(len(single_sentence_array))\n",
        "        #So how many times padding dictionary key needs to be appended, if we say maximum length of sentences to be considered is eng_max_len.\n",
        "        if (max_language_words>length_single_sentence):\n",
        "            padding_count=(max_language_words-length_single_sentence)\n",
        "        else:\n",
        "            padding_count=0\n",
        "        if (padding_count>0):\n",
        "            for pad in range(0,padding_count):\n",
        "                single_sentence_array.append(padding_value)\n",
        "        else:\n",
        "            single_sentence_array=single_sentence_array[0:max_language_words]\n",
        "        #Append to main array\n",
        "        language_array.append(single_sentence_array)\n",
        "    #Convert to Numpy array at the end\n",
        "    language_array=np.array(language_array)\n",
        "    return(language_array)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "oQ8MXMDmppi-",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Instead of doing a padding over large sentence size, emperically it is found that it is better to do for a short sentences considering the limitation we are having with respect to corpus size."
      ]
    },
    {
      "metadata": {
        "id": "MMbxxTKZppjS",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "599b10f7-d4da-481f-f879-77325f83e896"
      },
      "cell_type": "code",
      "source": [
        "#Get encoded sentences\n",
        "hindi_encoding=get_padded_encoding(hindi_sentences_list,hindi_dictionary,max_english_words)\n",
        "english_encoding=get_padded_encoding(english_sentences_list,english_dictionary,max_hindi_words)\n",
        "print(hindi_encoding.shape,english_encoding.shape)"
      ],
      "execution_count": 113,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(500, 10) (500, 10)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "7vawCinyppjh",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 408
        },
        "outputId": "8b5bb25b-ca46-4960-d1fb-241958813744"
      },
      "cell_type": "code",
      "source": [
        "#Verifying the encoding and decoding for a sample data.\n",
        "print(english_sentences_list[1],hindi_sentences_list[1])\n",
        "print(english_encoding[1],hindi_encoding[1])\n",
        "#Check if encoding gives back the same answer\n",
        "for key in english_encoding[1]:\n",
        "    print(revere_dictionary_english[key])\n",
        "for key in hindi_encoding[1]:\n",
        "    print(revere_dictionary_hindi[key])\n",
        "english_dictionary['<pad>']\n",
        "hindi_dictionary['<pad>']"
      ],
      "execution_count": 114,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "your selfconfidence also increases with teeth  </s> दाँतों से आपका आत्मविश्‍वास भी बढ़ता है  </s>\n",
            "[196 150   8  98 191 173 199   0   0   0] [ 86 191  14  10 142 131 203 210   0   0]\n",
            "your\n",
            "selfconfidence\n",
            "also\n",
            "increases\n",
            "with\n",
            "teeth\n",
            "</s>\n",
            "<pad>\n",
            "<pad>\n",
            "<pad>\n",
            "दाँतों\n",
            "से\n",
            "आपका\n",
            "आत्मविश्‍वास\n",
            "भी\n",
            "बढ़ता\n",
            "है\n",
            "</s>\n",
            "<pad>\n",
            "<pad>\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 114
        }
      ]
    },
    {
      "metadata": {
        "id": "7pX_5qX3ppj0",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#We will convert the english and hindi encodings to one hot encodings.\n",
        "#Please note Input is of the dimension (number of sentences,max_length_language(every column is a word))\n",
        "#Output is (number of sentences,Max_length_language(every row is a word),length of vocabulary)\n",
        "#Basically every row of the onehotcode matrix must be for one word.\n",
        "#How=1 => 1 0 0\n",
        "#Are=2 => 0 1 0\n",
        "#You=3 => 0 0 1\n",
        "#We are trying to translate hindi to english, so our X is Hindi and Y is English\n",
        "X=hindi_encoding\n",
        "Y=english_encoding\n",
        "hindi_encoding=None\n",
        "english_encoding=None\n",
        "dataset=None\n",
        "english_sentences_list=None\n",
        "hindi_sentences_list=None\n",
        "del hindi_encoding,english_encoding,dataset,english_sentences_list,hindi_sentences_list\n",
        "#Note: Instead of one hot we can use word embeddings for Xoh\n",
        "\n",
        "#Xoh=np.array(list(map(lambda x: to_categorical(x, num_classes=len(hindi_dictionary)), X)))\n",
        "#Yoh=np.array(list(map(lambda x: to_categorical(x, num_classes=len(english_dictionary)), Y)))\n",
        "#print(\"X.shape:\", X.shape)\n",
        "#print(\"Y.shape:\", Y.shape)\n",
        "#print(\"Xoh.shape:\", Xoh.shape)\n",
        "#print(\"Yoh.shape:\", Yoh.shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "sOkyyKPL9Hci",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#sample_size=X.shape[0]\n",
        "#test_start_index=(sample_size-100)\n",
        "#test_batch_size=sample_size-test_start_index-2\n",
        "#testXoh=get_single_onehot_array(X,test_start_index,test_batch_size,hindi_dictionary)\n",
        "#testYoh=get_single_onehot_array(Y,test_start_index,test_batch_size,english_dictionary)\n",
        "#print(sample_size,test_start_index,test_batch_size)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "3kPEqhEx9MJj",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#print(Y.shape,X.shape,len(english_dictionary),len(hindi_dictionary))\n",
        "#from keras.callbacks import ModelCheckpoint\n",
        "#import pandas\n",
        "\n",
        "#sample_size=X.shape[0]\n",
        "#test_start_index=(sample_size-100)\n",
        "#test_batch_size=sample_size-test_start_index-2\n",
        "#train_iteration_size=test_start_index\n",
        "#print(\"train_iteration_size\",train_iteration_size)\n",
        "#gc.collect()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Bfgy8zHB8YB5",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#Every call will return batch size sized Y or x\n",
        "#Intention generate the small dataset and train the model \n",
        "#and retrain the model again for next index small dataset and so on\n",
        "\n",
        "#start_index=0\n",
        "#iteration=0\n",
        "#if (iteration>0):\n",
        "#    start_index=iteration+batch_size\n",
        "def get_single_onehot_array(Y,start_index,batch_size,dictionary):\n",
        "    english_dictionary=dictionary\n",
        "    end_index=start_index+batch_size\n",
        "    if end_index>Y.shape[0]:\n",
        "        end_index=Y.shape[0]\n",
        "    if start_index>end_index:\n",
        "        start_index=(end_index-1)\n",
        "    result_array=[]\n",
        "    for i in range(start_index,end_index):\n",
        "        Yoh_single_sentence=np.array(list(map(lambda x: to_categorical(x, num_classes=len(english_dictionary)), Y[i])))\n",
        "        Yoh_single_sentence=np.swapaxes(Yoh_single_sentence,0,1)\n",
        "        Yoh_single_sentence=np.reshape(Yoh_single_sentence,(Yoh_single_sentence.shape[1],Yoh_single_sentence.shape[2]))\n",
        "        #Making fist column of timestep of one hot encoding as 0 as this column will have 1 only for the row with encoding 0, which is our pad value, which will be masked.\n",
        "        #if i % 1000 == 0:\n",
        "        #    print(i)\n",
        "        #print(type(Yoh_single_sentence),Yoh_single_sentence.shape)\n",
        "        result_array.append(Yoh_single_sentence)\n",
        "        #result_array = np.append(result_array,Yoh_single_sentence, axis=0)\n",
        "    Yoh = np.array(result_array)\n",
        "    Yoh[:,:,0]=np.zeros(Yoh.shape[1])\n",
        "    return Yoh"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "jQQwIVejAJm6",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "if (os.path.isfile(\"attention_decoder.zip\")==False):\n",
        "  r = requests.get(\"https://github.com/meethariprasad/phd/raw/master/assignments/NLP/Translation/attention_decoder.zip\")\n",
        "  z = zipfile.ZipFile(io.BytesIO(r.content))\n",
        "  z.extractall()\n",
        "  os.listdir()\n",
        "  del z,r"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "K-WXobmH__rC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "76d28178-59af-449c-db36-94b316553e6d"
      },
      "cell_type": "code",
      "source": [
        "#del model\n",
        "from random import randint\n",
        "from numpy import array\n",
        "from numpy import argmax\n",
        "from numpy import array_equal\n",
        "from keras.models import Sequential\n",
        "from keras.layers import LSTM\n",
        "from attention_decoder import AttentionDecoder\n",
        "\n",
        "\n",
        "\n",
        "# configure problem\n",
        "n_features = len(english_dictionary)\n",
        "input_features=len(hindi_dictionary)\n",
        "n_timesteps_in = X.shape[1]\n",
        "n_timesteps_out = Y.shape[1]\n",
        "LSTM_Unitsize=150\n",
        "input_embed_dimension=100\n",
        "print(n_timesteps_in,n_timesteps_out,input_features,n_features,LSTM_Unitsize)\n",
        "#Previous run: 20 20 8651 6768 150 model saved.\n",
        "#model available to load and predict ready. 60% Validation Accuracy under 40000 data with 600 Epoch run on GPU"
      ],
      "execution_count": 120,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "10 10 211 200 150\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "xXR_36fh_Y43",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#Trying nce loss from tensorflow.\n",
        "#nn.nce_loss\n",
        "#del model2\n",
        "#import keras.backend as K\n",
        "import tensorflow as tf\n",
        "\n",
        "def keras_nce_loss(tgt, pred):\n",
        "    return tf.nn.nce_loss(labels=tgt,inputs=pred,num_sampled=100)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Krvi6kA0Tuw5",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 656
        },
        "outputId": "d7887611-d4ea-4147-ebd0-355a11e854be"
      },
      "cell_type": "code",
      "source": [
        "#import gc\n",
        "#locals()\n",
        "# train LSTM\n",
        "#model.fit(Xoh, Yoh, epochs=1, verbose=2)\n",
        "# define model\n",
        "#del model2\n",
        "import gc\n",
        "gc.collect()\n",
        "from keras.layers import Dropout,Masking,Embedding\n",
        "from attention_decoder import AttentionDecoder\n",
        "\n",
        "if (os.path.isfile(\"main_model_weights_attn_new.h5\")==False):\n",
        "  model2 = Sequential()\n",
        "  model2.add(Embedding(input_features, input_embed_dimension, input_length=n_timesteps_in,mask_zero=True))\n",
        "  model2.add(Dropout(0.2))\n",
        "  model2.add(LSTM(LSTM_Unitsize,return_sequences=True,activation='relu'))\n",
        "  model2.add(Masking(mask_value=0.))\n",
        "  model2.add(AttentionDecoder(LSTM_Unitsize, n_features))\n",
        "  model2.compile(loss=keras_nce_loss, optimizer='adam', metrics=['acc'])\n",
        "  #model2.save(\"model2_compiled.hd5\")"
      ],
      "execution_count": 157,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-157-0d76d4053a42>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m   \u001b[0mmodel2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mMasking\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmask_value\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m   \u001b[0mmodel2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mAttentionDecoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mLSTM_Unitsize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_features\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m   \u001b[0mmodel2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkeras_nce_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'adam'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'acc'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m   \u001b[0;31m#model2.save(\"model2_compiled.hd5\")\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/models.py\u001b[0m in \u001b[0;36mcompile\u001b[0;34m(self, optimizer, loss, metrics, sample_weight_mode, **kwargs)\u001b[0m\n\u001b[1;32m    786\u001b[0m                            \u001b[0mmetrics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    787\u001b[0m                            \u001b[0msample_weight_mode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight_mode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 788\u001b[0;31m                            **kwargs)\n\u001b[0m\u001b[1;32m    789\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    790\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mcompile\u001b[0;34m(self, optimizer, loss, metrics, loss_weights, sample_weight_mode, **kwargs)\u001b[0m\n\u001b[1;32m    909\u001b[0m             \u001b[0mloss_weight\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_weights_list\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    910\u001b[0m             output_loss = weighted_loss(y_true, y_pred,\n\u001b[0;32m--> 911\u001b[0;31m                                         sample_weight, mask)\n\u001b[0m\u001b[1;32m    912\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    913\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics_tensors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mweighted\u001b[0;34m(y_true, y_pred, weights, mask)\u001b[0m\n\u001b[1;32m    434\u001b[0m         \"\"\"\n\u001b[1;32m    435\u001b[0m         \u001b[0;31m# score_array has ndim >= 2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 436\u001b[0;31m         \u001b[0mscore_array\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    437\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmask\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    438\u001b[0m             \u001b[0;31m# Cast the mask to floatX to avoid float64 upcasting in theano\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-155-ec20de882530>\u001b[0m in \u001b[0;36mkeras_nce_loss\u001b[0;34m(tgt, pred)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mkeras_nce_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtgt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnce_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtgt\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpred\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnum_sampled\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m: nce_loss() missing 3 required positional arguments: 'weights', 'biases', and 'num_classes'"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "CLDZUfxVL9bR",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#from IPython.display import SVG\n",
        "#from keras.utils.vis_utils import model_to_dot\n",
        "#import pydot\n",
        "\n",
        "#SVG(model_to_dot(model2).create(prog='dot', format='svg'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Sq-0tZPcEsvR",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 306
        },
        "outputId": "b5e5e719-e6b5-4345-e21f-9b688ec097b3"
      },
      "cell_type": "code",
      "source": [
        "#from IPython.display import SVG\n",
        "#from keras.utils.vis_utils import model_to_dot\n",
        "#import pydot\n",
        "\n",
        "#SVG(model_to_dot(model2).create(prog='dot', format='svg'))\n",
        "model2.summary()"
      ],
      "execution_count": 124,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_2 (Embedding)      (None, 10, 100)           21100     \n",
            "_________________________________________________________________\n",
            "dropout_2 (Dropout)          (None, 10, 100)           0         \n",
            "_________________________________________________________________\n",
            "lstm_2 (LSTM)                (None, 10, 150)           150600    \n",
            "_________________________________________________________________\n",
            "masking_2 (Masking)          (None, 10, 150)           0         \n",
            "_________________________________________________________________\n",
            "AttentionDecoder (AttentionD (None, 10, 200)           393450    \n",
            "=================================================================\n",
            "Total params: 565,150\n",
            "Trainable params: 565,150\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "mAEF6vy1IKau",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 272
        },
        "outputId": "9778b2ce-a632-4a2c-f735-a1d4dda483b3"
      },
      "cell_type": "code",
      "source": [
        "#Train & Validation Parameters\n",
        "sample_size=X.shape[0]\n",
        "\n",
        "# Or Adjust Samplesize if you want it to be small\n",
        "#sample_size=20000\n",
        "\n",
        "test_batch_size=100\n",
        "test_start_index=(sample_size-test_batch_size)\n",
        "test_end_index=sample_size\n",
        "test_generator_batch_size=20\n",
        "test_samples_per_epoc=test_batch_size\n",
        "###############################################################\n",
        "\n",
        "\n",
        "train_start_index=0\n",
        "train_end_index=test_start_index\n",
        "train_generator_batch_size=20\n",
        "train_samples_per_epoc=100\n",
        "###############################################################\n",
        "\n",
        "print(\"Data Size\",sample_size)\n",
        "print(\"######################################\")\n",
        "print(\"train_start_index:\",train_start_index)\n",
        "print(\"train_end_index:\",train_end_index)\n",
        "print(\"train_generator_batch_size\",train_generator_batch_size)\n",
        "print(\"train_samples_per_epoc\",train_samples_per_epoc)\n",
        "print(\"######################################\")\n",
        "\n",
        "print(\"test_start_index:\",test_start_index)\n",
        "print(\"test_end_index:\",test_end_index)\n",
        "print(\"test_generator_batch_size:\",test_generator_batch_size)\n",
        "print(\"test_samples_per_epoc\",test_samples_per_epoc)\n",
        "print(\"######################################\")\n",
        "\n",
        "print(\"Approximate Epcs Required to Cover Samples given train_samples_per_epoc(sample_size/train_samples_per_epoc)\",round(sample_size/train_samples_per_epoc))\n",
        "epoc=round(sample_size/train_samples_per_epoc)*40\n",
        "print(\"epoc:\",epoc)\n",
        "gc.collect()"
      ],
      "execution_count": 125,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Data Size 500\n",
            "######################################\n",
            "train_start_index: 0\n",
            "train_end_index: 400\n",
            "train_generator_batch_size 20\n",
            "train_samples_per_epoc 100\n",
            "######################################\n",
            "test_start_index: 400\n",
            "test_end_index: 500\n",
            "test_generator_batch_size: 20\n",
            "test_samples_per_epoc 100\n",
            "######################################\n",
            "Approximate Epcs Required to Cover Samples given train_samples_per_epoc(sample_size/train_samples_per_epoc) 5\n",
            "epoc: 200\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "522"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 125
        }
      ]
    },
    {
      "metadata": {
        "id": "nH7nfqXo3Q20",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#Train_X=X[train_start_index:train_end_index]\n",
        "#Train_Y=Y[train_start_index:train_end_index]\n",
        "#Test_X=X[test_start_index:test_end_index]\n",
        "#Test_Y=Y[test_start_index:test_end_index]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "xkdpct59YLoj",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#For Train:start_index=0,train_iteration_size=train_sample_size\n",
        "#batch_generator(start_index,X,Y,train_iteration_size,hindi_dictionary,english_dictionary,n_s)\n",
        "#For Test:start_index=train_sample_size,train_iteration_size=(samples_size-train_sample_size-1)\n",
        "#batch_generator(start_index,X,Y,train_iteration_size,hindi_dictionary,english_dictionary,n_s)\n",
        "#Keras new version has timeseries generator which pretty much does same as the code below.\n",
        "\n",
        "def batch_generator(label,train_start_index,train_end_index,X,Y,train_generator_batch_size,english_dictionary):\n",
        "  Xoh_batch = np.zeros((train_generator_batch_size,X.shape[1]))\n",
        "  Yoh_batch = np.zeros((train_generator_batch_size,Y.shape[1],len(english_dictionary)))\n",
        "  batch_size=1\n",
        "  #LOG_EVERY_N = 1000\n",
        "  print(label,\"start_index\",train_start_index)\n",
        "  start_index=train_start_index\n",
        "  \n",
        "  while True:\n",
        "    for ind in range(train_generator_batch_size):\n",
        "      if (start_index<train_end_index):\n",
        "        \n",
        "        #For Sequential Index Generation\n",
        "        index=start_index\n",
        "        #if(label==\"train\" and (index%1000 == 0)):\n",
        "        #  print(label,\"example index\",index)\n",
        "        #elif (label==\"test\" and (index == train_end_index-1)):\n",
        "        #  print(label,\"example index\",index)\n",
        "\n",
        "        Xoh_batch[ind]=np.reshape(X[index],(1,X.shape[1]))\n",
        "        Yoh_batch[ind]=get_single_onehot_array(Y,index,batch_size,english_dictionary)\n",
        "        start_index=start_index+1\n",
        "      else:\n",
        "        if (label==\"train\"):\n",
        "          print(label,\"start_index before reset\",start_index)\n",
        "        start_index=train_start_index\n",
        "    #print(Xoh_batch.shape,Yoh_batch.shape)\n",
        "    yield (Xoh_batch,Yoh_batch)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "0yco3YK69KxE",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "With the generator above, if we define train_generator_batch_size = 10 , that means it will take out 10 samples from features and labels to feed into each epoch until an epoch hits train_samples_per_epoc  sample limit. Then fit_generator() destroys the used data and move on repeating the same process in new epoch.\n"
      ]
    },
    {
      "metadata": {
        "id": "HEmbdyK58r_l",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "8b7b89c0-fd17-4c2b-8f1c-815177db3f12"
      },
      "cell_type": "code",
      "source": [
        "train_label=\"train\"\n",
        "test_label=\"test\"\n",
        "train_batch_generator=batch_generator(train_label,train_start_index,train_end_index,X,Y,train_generator_batch_size,english_dictionary)\n",
        "test_batch_generator=batch_generator(test_label,test_start_index,test_end_index,X,Y,test_generator_batch_size,english_dictionary)\n",
        "print(train_batch_generator,test_batch_generator)"
      ],
      "execution_count": 128,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<generator object batch_generator at 0x7f214a8faca8> <generator object batch_generator at 0x7f214a8ca360>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "dYrmYXoNppm5",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 7459
        },
        "outputId": "fce0e49c-d15a-4f0f-e803-2cf27cbd647a"
      },
      "cell_type": "code",
      "source": [
        "#Run the model.fit. If only best validation model needs to be saved, then change save_best_only=True in checkpoint.\n",
        "from keras.callbacks import ModelCheckpoint,EarlyStopping\n",
        "checkpoint = ModelCheckpoint('best_model_new.h5', monitor='val_acc', verbose=2,save_best_only=True, mode='auto',save_weights_only=False)\n",
        "early_stop=EarlyStopping(monitor='val_acc', min_delta=0.0001, patience=50,mode='auto')\n",
        "import pandas\n",
        "gc.collect()\n",
        "#pandas.DataFrame(model.fit(trainX, trainY, epochs=200, batch_size=100, validation_data=(testX, testY), callbacks=[checkpoint]).history).to_csv(\"history.csv\")\n",
        "#model.fit(trainX, trainY, epochs=200, batch_size=20, validation_data=(testX, testY), callbacks=[checkpoint])\n",
        "model2.fit_generator(train_batch_generator,steps_per_epoch=train_samples_per_epoc\n",
        "                     ,validation_data=test_batch_generator,validation_steps=test_samples_per_epoc\n",
        "                     ,epochs=epoc\n",
        "                     #,verbose=1\n",
        "                     ,callbacks=[checkpoint,early_stop]\n",
        "                     ,max_q_size=1\n",
        "                     #,workers=5\n",
        "                     ,initial_epoch=57\n",
        "                    )"
      ],
      "execution_count": 129,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 58/200\n",
            "train start_index 0\n",
            " 19/100 [====>.........................] - ETA: 12s - loss: 3.4131 - acc: 0.1113train start_index before reset 400\n",
            " 39/100 [==========>...................] - ETA: 7s - loss: 2.9520 - acc: 0.1247train start_index before reset 400\n",
            " 59/100 [================>.............] - ETA: 4s - loss: 2.6535 - acc: 0.1466train start_index before reset 400\n",
            " 79/100 [======================>.......] - ETA: 2s - loss: 2.4395 - acc: 0.1766train start_index before reset 400\n",
            " 99/100 [============================>.] - ETA: 0s - loss: 2.2648 - acc: 0.2058train start_index before reset 400\n",
            "test start_index 400\n",
            "Epoch 00057: val_acc improved from -inf to 0.30520, saving model to best_model_new.h5\n",
            "100/100 [==============================] - 15s - loss: 2.2660 - acc: 0.2060 - val_loss: 2.2496 - val_acc: 0.3052\n",
            "Epoch 59/200\n",
            " 19/100 [====>.........................] - ETA: 6s - loss: 1.2721 - acc: 0.4087train start_index before reset 400\n",
            " 39/100 [==========>...................] - ETA: 5s - loss: 1.2142 - acc: 0.4191train start_index before reset 400\n",
            " 59/100 [================>.............] - ETA: 3s - loss: 1.1536 - acc: 0.4330train start_index before reset 400\n",
            " 60/100 [=================>............] - ETA: 3s - loss: 1.1648 - acc: 0.4318"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            " 79/100 [======================>.......] - ETA: 1s - loss: 1.0682 - acc: 0.4515train start_index before reset 400\n",
            " 99/100 [============================>.] - ETA: 0s - loss: 0.9839 - acc: 0.4680train start_index before reset 400\n",
            "Epoch 00058: val_acc improved from 0.30520 to 0.44785, saving model to best_model_new.h5\n",
            "100/100 [==============================] - 13s - loss: 0.9857 - acc: 0.4677 - val_loss: 1.4590 - val_acc: 0.4479\n",
            "Epoch 60/200\n",
            " 19/100 [====>.........................] - ETA: 7s - loss: 0.4852 - acc: 0.5729train start_index before reset 400\n",
            " 39/100 [==========>...................] - ETA: 5s - loss: 0.4445 - acc: 0.5850train start_index before reset 400\n",
            " 59/100 [================>.............] - ETA: 3s - loss: 0.4071 - acc: 0.5944train start_index before reset 400\n",
            " 79/100 [======================>.......] - ETA: 1s - loss: 0.3791 - acc: 0.6017train start_index before reset 400\n",
            " 99/100 [============================>.] - ETA: 0s - loss: 0.3489 - acc: 0.6097train start_index before reset 400\n",
            "Epoch 00059: val_acc improved from 0.44785 to 0.56685, saving model to best_model_new.h5\n",
            "100/100 [==============================] - 14s - loss: 0.3479 - acc: 0.6104 - val_loss: 1.0934 - val_acc: 0.5668\n",
            "Epoch 61/200\n",
            " 13/100 [==>...........................] - ETA: 7s - loss: 0.1294 - acc: 0.6462"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            " 19/100 [====>.........................] - ETA: 7s - loss: 0.1859 - acc: 0.6434train start_index before reset 400\n",
            " 39/100 [==========>...................] - ETA: 5s - loss: 0.1747 - acc: 0.6490train start_index before reset 400\n",
            " 59/100 [================>.............] - ETA: 3s - loss: 0.1635 - acc: 0.6514train start_index before reset 400\n",
            " 79/100 [======================>.......] - ETA: 1s - loss: 0.1522 - acc: 0.6537train start_index before reset 400\n",
            " 99/100 [============================>.] - ETA: 0s - loss: 0.1621 - acc: 0.6545train start_index before reset 400\n",
            "Epoch 00060: val_acc improved from 0.56685 to 0.59790, saving model to best_model_new.h5\n",
            "100/100 [==============================] - 13s - loss: 0.1631 - acc: 0.6547 - val_loss: 1.0225 - val_acc: 0.5979\n",
            "Epoch 62/200\n",
            " 20/100 [=====>........................] - ETA: 7s - loss: 0.1227 - acc: 0.6585train start_index before reset 400\n",
            " 40/100 [===========>..................] - ETA: 5s - loss: 0.1129 - acc: 0.6603train start_index before reset 400\n",
            " 60/100 [=================>............] - ETA: 3s - loss: 0.1038 - acc: 0.6623train start_index before reset 400\n",
            " 67/100 [===================>..........] - ETA: 2s - loss: 0.0983 - acc: 0.6630"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            " 80/100 [=======================>......] - ETA: 1s - loss: 0.1138 - acc: 0.6626train start_index before reset 400\n",
            " 99/100 [============================>.] - ETA: 0s - loss: 0.1056 - acc: 0.6633Epoch 00061: val_acc improved from 0.59790 to 0.61755, saving model to best_model_new.h5\n",
            "100/100 [==============================] - 14s - loss: 0.1069 - acc: 0.6631 - val_loss: 0.9862 - val_acc: 0.6175\n",
            "Epoch 63/200\n",
            "train start_index before reset 400\n",
            " 20/100 [=====>........................] - ETA: 6s - loss: 0.0615 - acc: 0.6710train start_index before reset 400\n",
            " 40/100 [===========>..................] - ETA: 5s - loss: 0.0595 - acc: 0.6720train start_index before reset 400\n",
            " 60/100 [=================>............] - ETA: 3s - loss: 0.0561 - acc: 0.6725train start_index before reset 400\n",
            " 80/100 [=======================>......] - ETA: 1s - loss: 0.0799 - acc: 0.6708train start_index before reset 400\n",
            " 99/100 [============================>.] - ETA: 0s - loss: 0.0746 - acc: 0.6716Epoch 00062: val_acc improved from 0.61755 to 0.61835, saving model to best_model_new.h5\n",
            "100/100 [==============================] - 13s - loss: 0.0755 - acc: 0.6713 - val_loss: 0.9514 - val_acc: 0.6184\n",
            "Epoch 64/200\n",
            "train start_index before reset 400\n",
            " 14/100 [===>..........................] - ETA: 7s - loss: 0.0360 - acc: 0.6671"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            " 20/100 [=====>........................] - ETA: 6s - loss: 0.0515 - acc: 0.6737train start_index before reset 400\n",
            " 40/100 [===========>..................] - ETA: 5s - loss: 0.0455 - acc: 0.6752train start_index before reset 400\n",
            " 60/100 [=================>............] - ETA: 3s - loss: 0.0416 - acc: 0.6755train start_index before reset 400\n",
            " 80/100 [=======================>......] - ETA: 1s - loss: 0.0383 - acc: 0.6762train start_index before reset 400\n",
            " 99/100 [============================>.] - ETA: 0s - loss: 0.0349 - acc: 0.6771Epoch 00063: val_acc improved from 0.61835 to 0.63640, saving model to best_model_new.h5\n",
            "100/100 [==============================] - 13s - loss: 0.0356 - acc: 0.6767 - val_loss: 0.9324 - val_acc: 0.6364\n",
            "Epoch 65/200\n",
            "train start_index before reset 400\n",
            " 20/100 [=====>........................] - ETA: 6s - loss: 0.0456 - acc: 0.6780train start_index before reset 400\n",
            " 40/100 [===========>..................] - ETA: 5s - loss: 0.0326 - acc: 0.6796train start_index before reset 400\n",
            " 60/100 [=================>............] - ETA: 3s - loss: 0.0472 - acc: 0.6788train start_index before reset 400\n",
            " 68/100 [===================>..........] - ETA: 2s - loss: 0.0426 - acc: 0.6791"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            " 80/100 [=======================>......] - ETA: 1s - loss: 0.0440 - acc: 0.6787train start_index before reset 400\n",
            " 99/100 [============================>.] - ETA: 0s - loss: 0.0408 - acc: 0.6789Epoch 00064: val_acc improved from 0.63640 to 0.64425, saving model to best_model_new.h5\n",
            "100/100 [==============================] - 13s - loss: 0.0408 - acc: 0.6790 - val_loss: 0.9132 - val_acc: 0.6443\n",
            "Epoch 66/200\n",
            "train start_index before reset 400\n",
            " 21/100 [=====>........................] - ETA: 6s - loss: 0.0805 - acc: 0.6693train start_index before reset 400\n",
            " 41/100 [===========>..................] - ETA: 5s - loss: 0.0591 - acc: 0.6745train start_index before reset 400\n",
            " 61/100 [=================>............] - ETA: 3s - loss: 0.0478 - acc: 0.6760train start_index before reset 400\n",
            " 81/100 [=======================>......] - ETA: 1s - loss: 0.0415 - acc: 0.6770train start_index before reset 400\n",
            " 99/100 [============================>.] - ETA: 0s - loss: 0.0367 - acc: 0.6774Epoch 00065: val_acc improved from 0.64425 to 0.64425, saving model to best_model_new.h5\n",
            "100/100 [==============================] - 13s - loss: 0.0364 - acc: 0.6779 - val_loss: 0.8750 - val_acc: 0.6443\n",
            "Epoch 67/200\n",
            "  1/100 [..............................] - ETA: 9s - loss: 0.0577 - acc: 0.6750train start_index before reset 400\n",
            " 11/100 [==>...........................] - ETA: 7s - loss: 0.0121 - acc: 0.6768"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            " 21/100 [=====>........................] - ETA: 6s - loss: 0.0152 - acc: 0.6814train start_index before reset 400\n",
            " 41/100 [===========>..................] - ETA: 5s - loss: 0.0127 - acc: 0.6821train start_index before reset 400\n",
            " 61/100 [=================>............] - ETA: 3s - loss: 0.0112 - acc: 0.6825train start_index before reset 400\n",
            " 81/100 [=======================>......] - ETA: 1s - loss: 0.0101 - acc: 0.6827train start_index before reset 400\n",
            " 99/100 [============================>.] - ETA: 0s - loss: 0.0093 - acc: 0.6826Epoch 00066: val_acc improved from 0.64425 to 0.64660, saving model to best_model_new.h5\n",
            "100/100 [==============================] - 14s - loss: 0.0092 - acc: 0.6828 - val_loss: 0.9059 - val_acc: 0.6466\n",
            "Epoch 68/200\n",
            "  1/100 [..............................] - ETA: 9s - loss: 0.0148 - acc: 0.6850train start_index before reset 400\n",
            " 21/100 [=====>........................] - ETA: 6s - loss: 0.0059 - acc: 0.6833train start_index before reset 400\n",
            " 41/100 [===========>..................] - ETA: 5s - loss: 0.0054 - acc: 0.6834train start_index before reset 400\n",
            " 61/100 [=================>............] - ETA: 3s - loss: 0.0052 - acc: 0.6832train start_index before reset 400\n",
            " 64/100 [==================>...........] - ETA: 3s - loss: 0.0052 - acc: 0.6836"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            " 81/100 [=======================>......] - ETA: 1s - loss: 0.0050 - acc: 0.6832train start_index before reset 400\n",
            " 99/100 [============================>.] - ETA: 0s - loss: 0.0048 - acc: 0.6830Epoch 00067: val_acc did not improve\n",
            "100/100 [==============================] - 14s - loss: 0.0048 - acc: 0.6832 - val_loss: 0.9211 - val_acc: 0.6464\n",
            "Epoch 69/200\n",
            "  1/100 [..............................] - ETA: 8s - loss: 0.0104 - acc: 0.6850train start_index before reset 400\n",
            " 21/100 [=====>........................] - ETA: 6s - loss: 0.0040 - acc: 0.6833train start_index before reset 400\n",
            " 41/100 [===========>..................] - ETA: 5s - loss: 0.0038 - acc: 0.6835train start_index before reset 400\n",
            " 61/100 [=================>............] - ETA: 3s - loss: 0.0037 - acc: 0.6834train start_index before reset 400\n",
            " 81/100 [=======================>......] - ETA: 1s - loss: 0.0036 - acc: 0.6835train start_index before reset 400\n",
            " 99/100 [============================>.] - ETA: 0s - loss: 0.0035 - acc: 0.6830Epoch 00068: val_acc improved from 0.64660 to 0.64735, saving model to best_model_new.h5\n",
            "100/100 [==============================] - 14s - loss: 0.0035 - acc: 0.6833 - val_loss: 0.9232 - val_acc: 0.6474\n",
            "Epoch 70/200\n",
            "  1/100 [..............................] - ETA: 8s - loss: 0.0027 - acc: 0.6800train start_index before reset 400\n",
            " 10/100 [==>...........................] - ETA: 8s - loss: 0.0025 - acc: 0.6815"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            " 22/100 [=====>........................] - ETA: 6s - loss: 0.0031 - acc: 0.6855train start_index before reset 400\n",
            " 42/100 [===========>..................] - ETA: 5s - loss: 0.0030 - acc: 0.6844train start_index before reset 400\n",
            " 62/100 [=================>............] - ETA: 3s - loss: 0.0029 - acc: 0.6839train start_index before reset 400\n",
            " 82/100 [=======================>......] - ETA: 1s - loss: 0.0028 - acc: 0.6837train start_index before reset 400\n",
            " 99/100 [============================>.] - ETA: 0s - loss: 0.0027 - acc: 0.6830Epoch 00069: val_acc did not improve\n",
            "100/100 [==============================] - 14s - loss: 0.0027 - acc: 0.6830 - val_loss: 0.9390 - val_acc: 0.6463\n",
            "Epoch 71/200\n",
            "  2/100 [..............................] - ETA: 8s - loss: 0.0035 - acc: 0.7075train start_index before reset 400\n",
            " 22/100 [=====>........................] - ETA: 6s - loss: 0.0025 - acc: 0.6850train start_index before reset 400\n",
            " 42/100 [===========>..................] - ETA: 5s - loss: 0.0024 - acc: 0.6842train start_index before reset 400\n",
            " 62/100 [=================>............] - ETA: 3s - loss: 0.0023 - acc: 0.6839train start_index before reset 400\n",
            " 68/100 [===================>..........] - ETA: 2s - loss: 0.0022 - acc: 0.6843"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            " 82/100 [=======================>......] - ETA: 1s - loss: 0.0023 - acc: 0.6837train start_index before reset 400\n",
            " 99/100 [============================>.] - ETA: 0s - loss: 0.0022 - acc: 0.6833Epoch 00070: val_acc did not improve\n",
            "100/100 [==============================] - 14s - loss: 0.0022 - acc: 0.6835 - val_loss: 0.9595 - val_acc: 0.6461\n",
            "Epoch 72/200\n",
            "  2/100 [..............................] - ETA: 8s - loss: 0.0031 - acc: 0.6950train start_index before reset 400\n",
            " 22/100 [=====>........................] - ETA: 6s - loss: 0.0020 - acc: 0.6843train start_index before reset 400\n",
            " 42/100 [===========>..................] - ETA: 5s - loss: 0.0020 - acc: 0.6839train start_index before reset 400\n",
            " 62/100 [=================>............] - ETA: 3s - loss: 0.0019 - acc: 0.6835train start_index before reset 400\n",
            " 82/100 [=======================>......] - ETA: 1s - loss: 0.0019 - acc: 0.6835train start_index before reset 400\n",
            " 99/100 [============================>.] - ETA: 0s - loss: 0.0018 - acc: 0.6831Epoch 00071: val_acc did not improve\n",
            "100/100 [==============================] - 13s - loss: 0.0018 - acc: 0.6832 - val_loss: 0.9585 - val_acc: 0.6466\n",
            "Epoch 73/200\n",
            "  2/100 [..............................] - ETA: 8s - loss: 0.0028 - acc: 0.6950train start_index before reset 400\n",
            " 12/100 [==>...........................] - ETA: 7s - loss: 0.0014 - acc: 0.6833"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            " 22/100 [=====>........................] - ETA: 6s - loss: 0.0017 - acc: 0.6843train start_index before reset 400\n",
            " 42/100 [===========>..................] - ETA: 4s - loss: 0.0017 - acc: 0.6840train start_index before reset 400\n",
            " 62/100 [=================>............] - ETA: 3s - loss: 0.0118 - acc: 0.6831train start_index before reset 400\n",
            " 82/100 [=======================>......] - ETA: 1s - loss: 0.0093 - acc: 0.6832train start_index before reset 400\n",
            " 99/100 [============================>.] - ETA: 0s - loss: 0.0079 - acc: 0.6826Epoch 00072: val_acc did not improve\n",
            "100/100 [==============================] - 13s - loss: 0.0079 - acc: 0.6828 - val_loss: 0.9681 - val_acc: 0.6464\n",
            "Epoch 74/200\n",
            "  2/100 [..............................] - ETA: 8s - loss: 0.0015 - acc: 0.6950train start_index before reset 400\n",
            " 23/100 [=====>........................] - ETA: 6s - loss: 0.0015 - acc: 0.6865train start_index before reset 400\n",
            " 43/100 [===========>..................] - ETA: 4s - loss: 0.0015 - acc: 0.6850train start_index before reset 400\n",
            " 63/100 [=================>............] - ETA: 3s - loss: 0.0014 - acc: 0.6843train start_index before reset 400\n",
            " 66/100 [==================>...........] - ETA: 2s - loss: 0.0014 - acc: 0.6840"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            " 83/100 [=======================>......] - ETA: 1s - loss: 0.0420 - acc: 0.6806train start_index before reset 400\n",
            " 99/100 [============================>.] - ETA: 0s - loss: 0.0574 - acc: 0.6763Epoch 00073: val_acc did not improve\n",
            "100/100 [==============================] - 13s - loss: 0.0581 - acc: 0.6763 - val_loss: 1.0148 - val_acc: 0.6153\n",
            "Epoch 75/200\n",
            "  3/100 [..............................] - ETA: 8s - loss: 0.1011 - acc: 0.6700train start_index before reset 400\n",
            " 23/100 [=====>........................] - ETA: 6s - loss: 0.0655 - acc: 0.6680train start_index before reset 400\n",
            " 43/100 [===========>..................] - ETA: 4s - loss: 0.0518 - acc: 0.6706train start_index before reset 400\n",
            " 63/100 [=================>............] - ETA: 3s - loss: 0.0409 - acc: 0.6741train start_index before reset 400\n",
            " 83/100 [=======================>......] - ETA: 1s - loss: 0.0329 - acc: 0.6763train start_index before reset 400\n",
            " 99/100 [============================>.] - ETA: 0s - loss: 0.0283 - acc: 0.6769Epoch 00074: val_acc did not improve\n",
            "100/100 [==============================] - 13s - loss: 0.0280 - acc: 0.6771 - val_loss: 0.8425 - val_acc: 0.6473\n",
            "Epoch 76/200\n",
            "  3/100 [..............................] - ETA: 8s - loss: 0.0075 - acc: 0.6967train start_index before reset 400\n",
            " 12/100 [==>...........................] - ETA: 7s - loss: 0.0036 - acc: 0.6871"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            " 23/100 [=====>........................] - ETA: 6s - loss: 0.0049 - acc: 0.6848train start_index before reset 400\n",
            " 43/100 [===========>..................] - ETA: 4s - loss: 0.0040 - acc: 0.6842train start_index before reset 400\n",
            " 63/100 [=================>............] - ETA: 3s - loss: 0.0035 - acc: 0.6837train start_index before reset 400\n",
            " 83/100 [=======================>......] - ETA: 1s - loss: 0.0032 - acc: 0.6836train start_index before reset 400\n",
            " 99/100 [============================>.] - ETA: 0s - loss: 0.0030 - acc: 0.6831Epoch 00075: val_acc did not improve\n",
            "100/100 [==============================] - 13s - loss: 0.0030 - acc: 0.6832 - val_loss: 0.8833 - val_acc: 0.6473\n",
            "Epoch 77/200\n",
            "  3/100 [..............................] - ETA: 8s - loss: 0.0028 - acc: 0.6950train start_index before reset 400\n",
            " 23/100 [=====>........................] - ETA: 6s - loss: 0.0020 - acc: 0.6848train start_index before reset 400\n",
            " 43/100 [===========>..................] - ETA: 4s - loss: 0.0018 - acc: 0.6843train start_index before reset 400\n",
            " 63/100 [=================>............] - ETA: 3s - loss: 0.0018 - acc: 0.6840train start_index before reset 400\n",
            " 67/100 [===================>..........] - ETA: 2s - loss: 0.0018 - acc: 0.6840"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            " 83/100 [=======================>......] - ETA: 1s - loss: 0.0017 - acc: 0.6839train start_index before reset 400\n",
            " 99/100 [============================>.] - ETA: 0s - loss: 0.0017 - acc: 0.6829Epoch 00076: val_acc did not improve\n",
            "100/100 [==============================] - 13s - loss: 0.0017 - acc: 0.6832 - val_loss: 0.8949 - val_acc: 0.6461\n",
            "Epoch 78/200\n",
            "  3/100 [..............................] - ETA: 8s - loss: 0.0015 - acc: 0.6967train start_index before reset 400\n",
            " 24/100 [======>.......................] - ETA: 6s - loss: 0.0015 - acc: 0.6871train start_index before reset 400\n",
            " 44/100 [============>.................] - ETA: 4s - loss: 0.0014 - acc: 0.6853train start_index before reset 400\n",
            " 64/100 [==================>...........] - ETA: 3s - loss: 0.0013 - acc: 0.6845train start_index before reset 400\n",
            " 84/100 [========================>.....] - ETA: 1s - loss: 0.0300 - acc: 0.6817train start_index before reset 400\n",
            " 99/100 [============================>.] - ETA: 0s - loss: 0.0300 - acc: 0.6804Epoch 00077: val_acc improved from 0.64735 to 0.64740, saving model to best_model_new.h5\n",
            "100/100 [==============================] - 13s - loss: 0.0318 - acc: 0.6803 - val_loss: 0.8755 - val_acc: 0.6474\n",
            "Epoch 79/200\n",
            "  4/100 [>.............................] - ETA: 8s - loss: 0.0058 - acc: 0.7013train start_index before reset 400\n",
            " 13/100 [==>...........................] - ETA: 7s - loss: 0.1771 - acc: 0.6765"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            " 24/100 [======>.......................] - ETA: 6s - loss: 0.1048 - acc: 0.6779train start_index before reset 400\n",
            " 44/100 [============>.................] - ETA: 4s - loss: 0.0615 - acc: 0.6794train start_index before reset 400\n",
            " 64/100 [==================>...........] - ETA: 3s - loss: 0.0441 - acc: 0.6805train start_index before reset 400\n",
            " 84/100 [========================>.....] - ETA: 1s - loss: 0.0342 - acc: 0.6812train start_index before reset 400\n",
            " 99/100 [============================>.] - ETA: 0s - loss: 0.0292 - acc: 0.6802Epoch 00078: val_acc improved from 0.64740 to 0.64760, saving model to best_model_new.h5\n",
            "100/100 [==============================] - 13s - loss: 0.0290 - acc: 0.6810 - val_loss: 0.8477 - val_acc: 0.6476\n",
            "Epoch 80/200\n",
            "  4/100 [>.............................] - ETA: 8s - loss: 0.0027 - acc: 0.6975train start_index before reset 400\n",
            " 24/100 [======>.......................] - ETA: 6s - loss: 0.0776 - acc: 0.6798train start_index before reset 400\n",
            " 44/100 [============>.................] - ETA: 4s - loss: 0.0457 - acc: 0.6807train start_index before reset 400\n",
            " 64/100 [==================>...........] - ETA: 3s - loss: 0.0323 - acc: 0.6813train start_index before reset 400\n",
            " 65/100 [==================>...........] - ETA: 3s - loss: 0.0318 - acc: 0.6820"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            " 84/100 [========================>.....] - ETA: 1s - loss: 0.0250 - acc: 0.6818train start_index before reset 400\n",
            " 99/100 [============================>.] - ETA: 0s - loss: 0.0214 - acc: 0.6808Epoch 00079: val_acc did not improve\n",
            "100/100 [==============================] - 13s - loss: 0.0212 - acc: 0.6816 - val_loss: 0.8587 - val_acc: 0.6454\n",
            "Epoch 81/200\n",
            "  4/100 [>.............................] - ETA: 8s - loss: 0.0022 - acc: 0.6937train start_index before reset 400\n",
            " 24/100 [======>.......................] - ETA: 6s - loss: 0.0016 - acc: 0.6850train start_index before reset 400\n",
            " 44/100 [============>.................] - ETA: 4s - loss: 0.0015 - acc: 0.6844train start_index before reset 400\n",
            " 64/100 [==================>...........] - ETA: 3s - loss: 0.0014 - acc: 0.6841train start_index before reset 400\n",
            " 84/100 [========================>.....] - ETA: 1s - loss: 0.0013 - acc: 0.6839train start_index before reset 400\n",
            " 99/100 [============================>.] - ETA: 0s - loss: 0.0012 - acc: 0.6825Epoch 00080: val_acc did not improve\n",
            "100/100 [==============================] - 13s - loss: 0.0013 - acc: 0.6830 - val_loss: 0.8840 - val_acc: 0.6453\n",
            "Epoch 82/200\n",
            "  4/100 [>.............................] - ETA: 8s - loss: 0.0016 - acc: 0.7012train start_index before reset 400\n",
            " 12/100 [==>...........................] - ETA: 7s - loss: 0.2704 - acc: 0.6638"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            " 25/100 [======>.......................] - ETA: 6s - loss: 0.1361 - acc: 0.6756train start_index before reset 400\n",
            " 45/100 [============>.................] - ETA: 4s - loss: 0.0774 - acc: 0.6790train start_index before reset 400\n",
            " 65/100 [==================>...........] - ETA: 3s - loss: 0.0541 - acc: 0.6802train start_index before reset 400\n",
            " 85/100 [========================>.....] - ETA: 1s - loss: 0.0418 - acc: 0.6809train start_index before reset 400\n",
            " 99/100 [============================>.] - ETA: 0s - loss: 0.0360 - acc: 0.6798Epoch 00081: val_acc did not improve\n",
            "100/100 [==============================] - 13s - loss: 0.0357 - acc: 0.6800 - val_loss: 0.8767 - val_acc: 0.6450\n",
            "Epoch 83/200\n",
            "  5/100 [>.............................] - ETA: 8s - loss: 0.0020 - acc: 0.7050train start_index before reset 400\n",
            " 25/100 [======>.......................] - ETA: 6s - loss: 0.0014 - acc: 0.6872train start_index before reset 400\n",
            " 45/100 [============>.................] - ETA: 4s - loss: 0.0013 - acc: 0.6854train start_index before reset 400\n",
            " 65/100 [==================>...........] - ETA: 3s - loss: 0.0067 - acc: 0.6842train start_index before reset 400\n",
            " 67/100 [===================>..........] - ETA: 2s - loss: 0.0065 - acc: 0.6845"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            " 85/100 [========================>.....] - ETA: 1s - loss: 0.0054 - acc: 0.6839train start_index before reset 400\n",
            " 99/100 [============================>.] - ETA: 0s - loss: 0.0396 - acc: 0.6789Epoch 00082: val_acc did not improve\n",
            "100/100 [==============================] - 13s - loss: 0.0393 - acc: 0.6789 - val_loss: 0.8935 - val_acc: 0.6356\n",
            "Epoch 84/200\n",
            "  5/100 [>.............................] - ETA: 8s - loss: 0.0966 - acc: 0.6870train start_index before reset 400\n",
            " 25/100 [======>.......................] - ETA: 6s - loss: 0.0362 - acc: 0.6812train start_index before reset 400\n",
            " 45/100 [============>.................] - ETA: 4s - loss: 0.0262 - acc: 0.6809train start_index before reset 400\n",
            " 65/100 [==================>...........] - ETA: 3s - loss: 0.0201 - acc: 0.6811train start_index before reset 400\n",
            " 85/100 [========================>.....] - ETA: 1s - loss: 0.0462 - acc: 0.6795train start_index before reset 400\n",
            " 99/100 [============================>.] - ETA: 0s - loss: 0.0408 - acc: 0.6787Epoch 00083: val_acc did not improve\n",
            "100/100 [==============================] - 13s - loss: 0.0404 - acc: 0.6788 - val_loss: 0.8489 - val_acc: 0.6475\n",
            "Epoch 85/200\n",
            "  5/100 [>.............................] - ETA: 8s - loss: 0.0081 - acc: 0.7080train start_index before reset 400\n",
            " 14/100 [===>..........................] - ETA: 7s - loss: 0.0041 - acc: 0.6914"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            " 25/100 [======>.......................] - ETA: 6s - loss: 0.0033 - acc: 0.6882train start_index before reset 400\n",
            " 45/100 [============>.................] - ETA: 4s - loss: 0.0026 - acc: 0.6862train start_index before reset 400\n",
            " 65/100 [==================>...........] - ETA: 3s - loss: 0.0022 - acc: 0.6853train start_index before reset 400\n",
            " 85/100 [========================>.....] - ETA: 1s - loss: 0.0061 - acc: 0.6846train start_index before reset 400\n",
            " 99/100 [============================>.] - ETA: 0s - loss: 0.0055 - acc: 0.6831Epoch 00084: val_acc did not improve\n",
            "100/100 [==============================] - 13s - loss: 0.0054 - acc: 0.6832 - val_loss: 0.8738 - val_acc: 0.6454\n",
            "Epoch 86/200\n",
            "  5/100 [>.............................] - ETA: 8s - loss: 0.0018 - acc: 0.7060train start_index before reset 400\n",
            " 26/100 [======>.......................] - ETA: 6s - loss: 0.0432 - acc: 0.6867train start_index before reset 400\n",
            " 46/100 [============>.................] - ETA: 4s - loss: 0.0256 - acc: 0.6852train start_index before reset 400\n",
            " 66/100 [==================>...........] - ETA: 2s - loss: 0.0867 - acc: 0.6780train start_index before reset 400\n",
            " 69/100 [===================>..........] - ETA: 2s - loss: 0.0842 - acc: 0.6774"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            " 86/100 [========================>.....] - ETA: 1s - loss: 0.0749 - acc: 0.6773train start_index before reset 400\n",
            " 99/100 [============================>.] - ETA: 0s - loss: 0.0659 - acc: 0.6767Epoch 00085: val_acc did not improve\n",
            "100/100 [==============================] - 13s - loss: 0.0653 - acc: 0.6767 - val_loss: 0.8206 - val_acc: 0.6455\n",
            "Epoch 87/200\n",
            "  6/100 [>.............................] - ETA: 8s - loss: 0.0082 - acc: 0.7033train start_index before reset 400\n",
            " 26/100 [======>.......................] - ETA: 6s - loss: 0.0040 - acc: 0.6875train start_index before reset 400\n",
            " 46/100 [============>.................] - ETA: 4s - loss: 0.0436 - acc: 0.6823train start_index before reset 400\n",
            " 66/100 [==================>...........] - ETA: 2s - loss: 0.0402 - acc: 0.6808train start_index before reset 400\n",
            " 86/100 [========================>.....] - ETA: 1s - loss: 0.0338 - acc: 0.6806train start_index before reset 400\n",
            " 99/100 [============================>.] - ETA: 0s - loss: 0.0297 - acc: 0.6798Epoch 00086: val_acc did not improve\n",
            "100/100 [==============================] - 13s - loss: 0.0294 - acc: 0.6797 - val_loss: 0.8410 - val_acc: 0.6459\n",
            "Epoch 88/200\n",
            "  6/100 [>.............................] - ETA: 8s - loss: 0.0048 - acc: 0.7050train start_index before reset 400\n",
            " 13/100 [==>...........................] - ETA: 7s - loss: 0.0031 - acc: 0.6946"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            " 26/100 [======>.......................] - ETA: 6s - loss: 0.0028 - acc: 0.6883train start_index before reset 400\n",
            " 46/100 [============>.................] - ETA: 4s - loss: 0.0023 - acc: 0.6862train start_index before reset 400\n",
            " 66/100 [==================>...........] - ETA: 2s - loss: 0.0020 - acc: 0.6852train start_index before reset 400\n",
            " 86/100 [========================>.....] - ETA: 1s - loss: 0.0018 - acc: 0.6847train start_index before reset 400\n",
            " 99/100 [============================>.] - ETA: 0s - loss: 0.0017 - acc: 0.6835Epoch 00087: val_acc did not improve\n",
            "100/100 [==============================] - 13s - loss: 0.0017 - acc: 0.6833 - val_loss: 0.8569 - val_acc: 0.6466\n",
            "Epoch 89/200\n",
            "  6/100 [>.............................] - ETA: 8s - loss: 0.0019 - acc: 0.7033train start_index before reset 400\n",
            " 26/100 [======>.......................] - ETA: 6s - loss: 0.0013 - acc: 0.6879train start_index before reset 400\n",
            " 46/100 [============>.................] - ETA: 4s - loss: 0.0011 - acc: 0.6861train start_index before reset 400\n",
            " 66/100 [==================>...........] - ETA: 2s - loss: 0.0011 - acc: 0.6852train start_index before reset 400\n",
            " 71/100 [====================>.........] - ETA: 2s - loss: 0.0011 - acc: 0.6852"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            " 86/100 [========================>.....] - ETA: 1s - loss: 0.0010 - acc: 0.6848train start_index before reset 400\n",
            " 99/100 [============================>.] - ETA: 0s - loss: 9.7907e-04 - acc: 0.6832Epoch 00088: val_acc did not improve\n",
            "100/100 [==============================] - 13s - loss: 9.7678e-04 - acc: 0.6833 - val_loss: 0.8561 - val_acc: 0.6476\n",
            "Epoch 90/200\n",
            "  6/100 [>.............................] - ETA: 7s - loss: 0.0012 - acc: 0.7025train start_index before reset 400\n",
            " 27/100 [=======>......................] - ETA: 6s - loss: 9.5924e-04 - acc: 0.6894train start_index before reset 400\n",
            " 47/100 [=============>................] - ETA: 4s - loss: 8.7719e-04 - acc: 0.6868train start_index before reset 400\n",
            " 67/100 [===================>..........] - ETA: 2s - loss: 8.3382e-04 - acc: 0.6856train start_index before reset 400\n",
            " 87/100 [=========================>....] - ETA: 1s - loss: 8.0270e-04 - acc: 0.6851train start_index before reset 400\n",
            " 99/100 [============================>.] - ETA: 0s - loss: 7.6485e-04 - acc: 0.6834Epoch 00089: val_acc did not improve\n",
            "100/100 [==============================] - 13s - loss: 7.6249e-04 - acc: 0.6834 - val_loss: 0.8731 - val_acc: 0.6462\n",
            "Epoch 91/200\n",
            "  7/100 [=>............................] - ETA: 7s - loss: 0.0011 - acc: 0.7007train start_index before reset 400\n",
            " 13/100 [==>...........................] - ETA: 7s - loss: 9.3043e-04 - acc: 0.6938"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            " 27/100 [=======>......................] - ETA: 6s - loss: 8.3894e-04 - acc: 0.6874train start_index before reset 400\n",
            " 47/100 [=============>................] - ETA: 4s - loss: 7.7234e-04 - acc: 0.6856train start_index before reset 400\n",
            " 67/100 [===================>..........] - ETA: 2s - loss: 7.2955e-04 - acc: 0.6849train start_index before reset 400\n",
            " 87/100 [=========================>....] - ETA: 1s - loss: 7.0435e-04 - acc: 0.6845train start_index before reset 400\n",
            " 99/100 [============================>.] - ETA: 0s - loss: 6.7006e-04 - acc: 0.6834Epoch 00090: val_acc did not improve\n",
            "100/100 [==============================] - 13s - loss: 6.6831e-04 - acc: 0.6833 - val_loss: 0.8851 - val_acc: 0.6455\n",
            "Epoch 92/200\n",
            "  7/100 [=>............................] - ETA: 7s - loss: 9.0102e-04 - acc: 0.6993train start_index before reset 400\n",
            " 27/100 [=======>......................] - ETA: 6s - loss: 6.6066e-04 - acc: 0.6874train start_index before reset 400\n",
            " 47/100 [=============>................] - ETA: 4s - loss: 6.2132e-04 - acc: 0.6857train start_index before reset 400\n",
            " 67/100 [===================>..........] - ETA: 2s - loss: 5.9556e-04 - acc: 0.6849train start_index before reset 400\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            " 87/100 [=========================>....] - ETA: 1s - loss: 5.8291e-04 - acc: 0.6845train start_index before reset 400\n",
            " 99/100 [============================>.] - ETA: 0s - loss: 5.5530e-04 - acc: 0.6833Epoch 00091: val_acc did not improve\n",
            "100/100 [==============================] - 13s - loss: 5.5479e-04 - acc: 0.6834 - val_loss: 0.8922 - val_acc: 0.6450\n",
            "Epoch 93/200\n",
            "  7/100 [=>............................] - ETA: 8s - loss: 7.8020e-04 - acc: 0.6971train start_index before reset 400\n",
            " 27/100 [=======>......................] - ETA: 6s - loss: 5.7174e-04 - acc: 0.6869train start_index before reset 400\n",
            " 47/100 [=============>................] - ETA: 4s - loss: 5.2990e-04 - acc: 0.6855train start_index before reset 400\n",
            " 67/100 [===================>..........] - ETA: 2s - loss: 5.1203e-04 - acc: 0.6849train start_index before reset 400\n",
            " 87/100 [=========================>....] - ETA: 1s - loss: 5.0207e-04 - acc: 0.6845train start_index before reset 400\n",
            " 99/100 [============================>.] - ETA: 0s - loss: 4.7953e-04 - acc: 0.6833Epoch 00092: val_acc did not improve\n",
            "100/100 [==============================] - 13s - loss: 4.7846e-04 - acc: 0.6830 - val_loss: 0.8977 - val_acc: 0.6455\n",
            "Epoch 94/200\n",
            "  7/100 [=>............................] - ETA: 8s - loss: 6.2866e-04 - acc: 0.7007train start_index before reset 400\n",
            " 12/100 [==>...........................] - ETA: 7s - loss: 5.6627e-04 - acc: 0.6942"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            " 28/100 [=======>......................] - ETA: 6s - loss: 5.1487e-04 - acc: 0.6895train start_index before reset 400\n",
            " 48/100 [=============>................] - ETA: 4s - loss: 4.7677e-04 - acc: 0.6869train start_index before reset 400\n",
            " 68/100 [===================>..........] - ETA: 2s - loss: 4.5887e-04 - acc: 0.6857train start_index before reset 400\n",
            " 88/100 [=========================>....] - ETA: 1s - loss: 4.4460e-04 - acc: 0.6851train start_index before reset 400\n",
            " 99/100 [============================>.] - ETA: 0s - loss: 4.2519e-04 - acc: 0.6836Epoch 00093: val_acc did not improve\n",
            "100/100 [==============================] - 13s - loss: 4.2461e-04 - acc: 0.6835 - val_loss: 0.8915 - val_acc: 0.6467\n",
            "Epoch 95/200\n",
            "  8/100 [=>............................] - ETA: 8s - loss: 5.6500e-04 - acc: 0.6988train start_index before reset 400\n",
            " 28/100 [=======>......................] - ETA: 6s - loss: 0.0258 - acc: 0.6857train start_index before reset 400\n",
            " 48/100 [=============>................] - ETA: 4s - loss: 0.0153 - acc: 0.6847train start_index before reset 400\n",
            " 68/100 [===================>..........] - ETA: 2s - loss: 0.0109 - acc: 0.6843train start_index before reset 400\n",
            " 69/100 [===================>..........] - ETA: 2s - loss: 0.0108 - acc: 0.6850"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            " 88/100 [=========================>....] - ETA: 1s - loss: 0.0085 - acc: 0.6840train start_index before reset 400\n",
            " 99/100 [============================>.] - ETA: 0s - loss: 0.0076 - acc: 0.6830Epoch 00094: val_acc did not improve\n",
            "100/100 [==============================] - 13s - loss: 0.0075 - acc: 0.6830 - val_loss: 0.9050 - val_acc: 0.6461\n",
            "Epoch 96/200\n",
            "  8/100 [=>............................] - ETA: 7s - loss: 5.8005e-04 - acc: 0.6956train start_index before reset 400\n",
            " 28/100 [=======>......................] - ETA: 6s - loss: 4.3915e-04 - acc: 0.6868train start_index before reset 400\n",
            " 48/100 [=============>................] - ETA: 4s - loss: 4.1004e-04 - acc: 0.6854train start_index before reset 400\n",
            " 68/100 [===================>..........] - ETA: 2s - loss: 3.9692e-04 - acc: 0.6846train start_index before reset 400\n",
            " 88/100 [=========================>....] - ETA: 1s - loss: 3.8768e-04 - acc: 0.6843train start_index before reset 400\n",
            " 99/100 [============================>.] - ETA: 0s - loss: 3.7103e-04 - acc: 0.6834Epoch 00095: val_acc did not improve\n",
            "100/100 [==============================] - 13s - loss: 3.7004e-04 - acc: 0.6832 - val_loss: 0.9180 - val_acc: 0.6466\n",
            "Epoch 97/200\n",
            "  8/100 [=>............................] - ETA: 8s - loss: 5.0781e-04 - acc: 0.6956train start_index before reset 400\n",
            " 14/100 [===>..........................] - ETA: 7s - loss: 3.9993e-04 - acc: 0.6932"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            " 28/100 [=======>......................] - ETA: 6s - loss: 3.8074e-04 - acc: 0.6868train start_index before reset 400\n",
            " 48/100 [=============>................] - ETA: 4s - loss: 3.6028e-04 - acc: 0.6855train start_index before reset 400\n",
            " 68/100 [===================>..........] - ETA: 2s - loss: 3.4864e-04 - acc: 0.6849train start_index before reset 400\n",
            " 88/100 [=========================>....] - ETA: 1s - loss: 3.4190e-04 - acc: 0.6845train start_index before reset 400\n",
            " 99/100 [============================>.] - ETA: 0s - loss: 3.2816e-04 - acc: 0.6835Epoch 00096: val_acc did not improve\n",
            "100/100 [==============================] - 13s - loss: 3.2692e-04 - acc: 0.6833 - val_loss: 0.9201 - val_acc: 0.6460\n",
            "Epoch 98/200\n",
            "  8/100 [=>............................] - ETA: 7s - loss: 4.1079e-04 - acc: 0.6950train start_index before reset 400\n",
            " 29/100 [=======>......................] - ETA: 6s - loss: 3.5117e-04 - acc: 0.6883train start_index before reset 400\n",
            " 49/100 [=============>................] - ETA: 4s - loss: 3.2818e-04 - acc: 0.6862train start_index before reset 400\n",
            " 69/100 [===================>..........] - ETA: 2s - loss: 3.1650e-04 - acc: 0.6852"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "train start_index before reset 400\n",
            " 89/100 [=========================>....] - ETA: 0s - loss: 3.0979e-04 - acc: 0.6848train start_index before reset 400\n",
            " 99/100 [============================>.] - ETA: 0s - loss: 2.9748e-04 - acc: 0.6837Epoch 00097: val_acc did not improve\n",
            "100/100 [==============================] - 13s - loss: 2.9622e-04 - acc: 0.6833 - val_loss: 0.9279 - val_acc: 0.6462\n",
            "Epoch 99/200\n",
            "  9/100 [=>............................] - ETA: 7s - loss: 3.9203e-04 - acc: 0.6956train start_index before reset 400\n",
            " 29/100 [=======>......................] - ETA: 6s - loss: 3.0865e-04 - acc: 0.6867train start_index before reset 400\n",
            " 49/100 [=============>................] - ETA: 4s - loss: 2.9401e-04 - acc: 0.6853train start_index before reset 400\n",
            " 69/100 [===================>..........] - ETA: 2s - loss: 2.8521e-04 - acc: 0.6847train start_index before reset 400\n",
            " 89/100 [=========================>....] - ETA: 0s - loss: 2.7878e-04 - acc: 0.6844train start_index before reset 400\n",
            " 99/100 [============================>.] - ETA: 0s - loss: 2.6829e-04 - acc: 0.6839Epoch 00098: val_acc improved from 0.64760 to 0.64810, saving model to best_model_new.h5\n",
            "100/100 [==============================] - 13s - loss: 2.6689e-04 - acc: 0.6833 - val_loss: 0.9091 - val_acc: 0.6481\n",
            "Epoch 100/200\n",
            "  9/100 [=>............................] - ETA: 8s - loss: 3.5254e-04 - acc: 0.6944"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "train start_index before reset 400\n",
            " 29/100 [=======>......................] - ETA: 6s - loss: 2.8281e-04 - acc: 0.6867train start_index before reset 400\n",
            " 42/100 [===========>..................] - ETA: 5s - loss: 2.4991e-04 - acc: 0.6831"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-129-c3988e4e30be>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m                      \u001b[0;34m,\u001b[0m\u001b[0mmax_q_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m                      \u001b[0;31m#,workers=5\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m                      \u001b[0;34m,\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m57\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m                     )\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/legacy/interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     86\u001b[0m                 warnings.warn('Update your `' + object_name +\n\u001b[1;32m     87\u001b[0m                               '` call to the Keras 2 API: ' + signature, stacklevel=2)\n\u001b[0;32m---> 88\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_support_signature\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minspect\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetargspec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/models.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_q_size, workers, pickle_safe, initial_epoch)\u001b[0m\n\u001b[1;32m   1122\u001b[0m                                         \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1123\u001b[0m                                         \u001b[0mpickle_safe\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpickle_safe\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1124\u001b[0;31m                                         initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m   1125\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1126\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0minterfaces\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_generator_methods_support\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/legacy/interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     86\u001b[0m                 warnings.warn('Update your `' + object_name +\n\u001b[1;32m     87\u001b[0m                               '` call to the Keras 2 API: ' + signature, stacklevel=2)\n\u001b[0;32m---> 88\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_support_signature\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minspect\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetargspec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_q_size, workers, pickle_safe, initial_epoch)\u001b[0m\n\u001b[1;32m   1900\u001b[0m                     outs = self.train_on_batch(x, y,\n\u001b[1;32m   1901\u001b[0m                                                \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1902\u001b[0;31m                                                class_weight=class_weight)\n\u001b[0m\u001b[1;32m   1903\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1904\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(self, x, y, sample_weight, class_weight)\u001b[0m\n\u001b[1;32m   1640\u001b[0m             \u001b[0mins\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1641\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_train_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1642\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1643\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1644\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2267\u001b[0m         updated = session.run(self.outputs + [self.updates_op],\n\u001b[1;32m   2268\u001b[0m                               \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2269\u001b[0;31m                               **self.session_kwargs)\n\u001b[0m\u001b[1;32m   2270\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2271\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    903\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    904\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 905\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    906\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    907\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1138\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1139\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1140\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1141\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1142\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1319\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1320\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1321\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1322\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1323\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1325\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1326\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1327\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1328\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1329\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1310\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1311\u001b[0m       return self._call_tf_sessionrun(\n\u001b[0;32m-> 1312\u001b[0;31m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1313\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1314\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1418\u001b[0m         return tf_session.TF_Run(\n\u001b[1;32m   1419\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1420\u001b[0;31m             status, run_metadata)\n\u001b[0m\u001b[1;32m   1421\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1422\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_tf_sessionprun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "pVdDT1Lhd9yk",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "daa561ec-dcb7-4d13-8595-350112141fff"
      },
      "cell_type": "code",
      "source": [
        "from keras.models import model_from_json\n",
        "model_json = model2.to_json()\n",
        "with open(\"model2.json\", \"w\") as json_file:\n",
        "    json_file.write(model_json)\n",
        "# serialize weights to HDF5\n",
        "model2.save_weights(\"model2_weights.h5\")\n",
        "print(\"Saved model to disk\")\n",
        " \n",
        "# later...\n",
        " \n",
        "# load json and create model\n",
        "json_file = open('model2.json', 'r')\n",
        "loaded_model_json = json_file.read()\n",
        "json_file.close()\n",
        "loaded_model = model_from_json(loaded_model_json,custom_objects={'AttentionDecoder': AttentionDecoder(LSTM_Unitsize, n_features)})\n",
        "print(\"Loaded model from disk\")\n"
      ],
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Saved model to disk\n",
            "Loaded model from disk\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "xHFKRBwogu7S",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "a582d9bb-5287-4c7c-b0c2-facebd02e1d4"
      },
      "cell_type": "code",
      "source": [
        "# load weights into new model\n",
        "loaded_model.load_weights(\"model2_weights.h5\")\n",
        "print(\"Loaded weights from disk\")\n",
        " \n",
        "# Compile the model\n",
        "loaded_model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "print(\"Compile model\")"
      ],
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loaded weights from disk\n",
            "Compile model\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "Jq75QNpnXuNt",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Install the PyDrive wrapper & import libraries. This is to upload model & weights to your GDrive.\n",
        "# This only needs to be done once in a notebook.\n",
        "!pip install -U -q PyDrive\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "\n",
        "# Authenticate and create the PyDrive client.\n",
        "# This only needs to be done once in a notebook.\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "aXi-naqjcm9T",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "5f29f96f-d345-4508-f785-086024f74140"
      },
      "cell_type": "code",
      "source": [
        "# Create & upload a file.\n",
        "modelname=\"model2.json\"\n",
        "uploaded = drive.CreateFile({'title': modelname})\n",
        "uploaded.SetContentFile(modelname)\n",
        "uploaded.Upload()\n",
        "print('Uploaded file with ID {}'.format(uploaded.get('id')))"
      ],
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Uploaded file with ID 1onnsT2hFRDUfxnVi_oMoI99xVd20D6Ss\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "oYB4U0JpjMvd",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "2ce587cf-73dd-4086-b3cc-d95082c2be29"
      },
      "cell_type": "code",
      "source": [
        "# Create & upload a file.\n",
        "modelname=\"model2_weights.h5\"\n",
        "uploaded = drive.CreateFile({'title': modelname})\n",
        "uploaded.SetContentFile(modelname)\n",
        "uploaded.Upload()\n",
        "print('Uploaded file with ID {}'.format(uploaded.get('id')))\n"
      ],
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Uploaded file with ID 1oOQPd7_uUI7HXH6qMt-VhYc_f21deN6-\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "uO7ymZoomx7Q",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "6f1dd5f3-4923-4830-b11c-51e092cf0ade"
      },
      "cell_type": "code",
      "source": [
        "# Create & upload a file.\n",
        "if (os.path.isfile(\"best_model_new.h5\")==True):\n",
        "  uploaded = drive.CreateFile({'title': modelname})\n",
        "  uploaded.SetContentFile(modelname)\n",
        "  uploaded.Upload()\n",
        "  print('Uploaded file with ID {}'.format(uploaded.get('id')))"
      ],
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Uploaded file with ID 1POtvelqieUz92ot9fjAcUMfZsAhwcB7_\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "qcjgbzzOY84m",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from keras.models import load_model\n",
        "#If you need best validation accuracy model\n",
        "prediction_model = load_model('best_model_new.h5',custom_objects={'AttentionDecoder': AttentionDecoder(LSTM_Unitsize, n_features)})\n",
        "#If you need overall model which was loaded with json, weights & compiled earlier\n",
        "#prediction_model=loaded_model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "frWPpNryEqsk",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "305df817-22a3-4c11-ec4c-802446bbeea6"
      },
      "cell_type": "code",
      "source": [
        "#Test\n",
        "test_index=test_start_index+14\n",
        "test=np.reshape(X[test_index],(1,X.shape[1]))\n",
        "prediction=np.round(prediction_model.predict(test))\n",
        "predicted_argmax=np.argmax(prediction,axis=2)\n",
        "predicted_argmax=np.reshape(predicted_argmax,(predicted_argmax.shape[1],))\n",
        "Y[test_index],predicted_argmax"
      ],
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([2654, 2480,  465,  249, 2491, 5521, 1165,  826, 4759,  249, 3673,\n",
              "        3927, 4143, 4155, 2653, 6767,    0,    0,    0,    0]),\n",
              " array([2654, 2480,  465,  249, 2491, 5521, 1165,  826, 4759,  249, 3673,\n",
              "        3927, 4143, 4155, 2653, 6767, 6767,    0, 6767, 6767]))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 79
        }
      ]
    },
    {
      "metadata": {
        "id": "BgUlalCkqNZb",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1054
        },
        "outputId": "49a1fff4-6f24-4a4e-a375-e67ae26a2ee2"
      },
      "cell_type": "code",
      "source": [
        "#Predict for test sentence index which model has not seen yet.\n",
        "print(\"Pure Testacases are from following sentence numbers in X\",test_start_index)\n",
        "#Given encoding matrix of sentence & dictionary, get the sentence\n",
        "def return_sentences(X,Y,revere_dictionary_english,test_index,model2):\n",
        "  \n",
        "  def return_predicted_array(X,test_index,model2):\n",
        "    test=np.reshape(X[test_index],(1,X.shape[1]))\n",
        "    encoding_prediction=np.round(model2.predict(test))\n",
        "    predicted_argmax=np.argmax(encoding_prediction,axis=2)\n",
        "    predicted_argmax=np.reshape(predicted_argmax,(predicted_argmax.shape[1],))\n",
        "    return predicted_argmax\n",
        "  \n",
        "  encoding_prediction=return_predicted_array(X,test_index,prediction_model)\n",
        "  \n",
        "  encoding_actual=Y[test_index]\n",
        "  \n",
        "  def return_sentence_list(encoding,revere_dictionary_english,test_index):\n",
        "    #print(test_index)\n",
        "    sentence=list()\n",
        "    for key in encoding:\n",
        "      key=int(key)\n",
        "      #print(type(int(key)))\n",
        "      #print(revere_dictionary_english[key])\n",
        "      sentence.append(revere_dictionary_english[key])\n",
        "    return sentence\n",
        "\n",
        "  def concatenate_list_data(list):\n",
        "      result= ''\n",
        "      for element in list:\n",
        "          result += str(element)\n",
        "          result += str(\" \")\n",
        "      return result\n",
        "  actual_sentence=return_sentence_list(encoding_actual,revere_dictionary_english,test_index)\n",
        "  actual_sentence=concatenate_list_data(actual_sentence)\n",
        "  \n",
        "  predicted_sentence=return_sentence_list(encoding_prediction,revere_dictionary_english,test_index)\n",
        "  predicted_sentence=concatenate_list_data(predicted_sentence)\n",
        "  return(actual_sentence,predicted_sentence)\n",
        "\n",
        "#print(test_index)\n",
        "test_sentences=10\n",
        "for test_sentence_index in range(test_start_index,test_start_index+test_sentences):\n",
        "  Actual,Predicted=return_sentences(X,Y,revere_dictionary_english,test_sentence_index,prediction_model)\n",
        "  print(\"#############################\")\n",
        "  print(\"Actual Sentence is:\")\n",
        "  print(Actual)\n",
        "  print(\"Predicted Sentence is:\")\n",
        "  print(Predicted)\n",
        "  print(\"#############################\")"
      ],
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Pure Testacases are from following sentence numbers in X 54900\n",
            "#############################\n",
            "Actual Sentence is:\n",
            "administer the dpt vaccine to the child </s> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> \n",
            "Predicted Sentence is:\n",
            "administer the dpt vaccine to the child </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> \n",
            "#############################\n",
            "#############################\n",
            "Actual Sentence is:\n",
            "do not clean deep wounds yourself </s> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> \n",
            "Predicted Sentence is:\n",
            "do not clean deep wounds yourself </s> </s> </s> </s> </s> </s> <pad> <pad> </s> </s> </s> </s> </s> </s> \n",
            "#############################\n",
            "#############################\n",
            "Actual Sentence is:\n",
            "clean the mouth after meal </s> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> \n",
            "Predicted Sentence is:\n",
            "clean the mouth after meal </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> </s> \n",
            "#############################\n",
            "#############################\n",
            "Actual Sentence is:\n",
            "avoid heavy hard to digest tea coffee tobacco intoxicating things and exciting element </s> <pad> <pad> <pad> <pad> <pad> <pad> \n",
            "Predicted Sentence is:\n",
            "avoid heavy hard to digest tea coffee tobacco intoxicating things and exciting element </s> <pad> <pad> <pad> </s> </s> </s> \n",
            "#############################\n",
            "#############################\n",
            "Actual Sentence is:\n",
            "changes occurring in gum lips and cheek should be paid attention </s> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> \n",
            "Predicted Sentence is:\n",
            "changes occurring in gum lips and cheek should be paid attention </s> </s> food </s> <pad> </s> </s> </s> </s> \n",
            "#############################\n",
            "#############################\n",
            "Actual Sentence is:\n",
            "all the nerves traveling to and fro have to cross the middle line to go to the other side </s> \n",
            "Predicted Sentence is:\n",
            "all the nerves traveling to and fro have to cross the middle line to go to the other side </s> \n",
            "#############################\n",
            "#############################\n",
            "Actual Sentence is:\n",
            "the easy way of diagnosis of tuberculosis of the lungs is three times test of the phlegm </s> <pad> <pad> \n",
            "Predicted Sentence is:\n",
            "the easy way of diagnosis of tuberculosis of the lungs is three times test of the phlegm </s> </s> </s> \n",
            "#############################\n",
            "#############################\n",
            "Actual Sentence is:\n",
            "because in this there are nutrients like vitamin fiber and calcium which are not available in simple sugar </s> <pad> \n",
            "Predicted Sentence is:\n",
            "<pad> in <pad> <pad> <pad> <pad> <pad> <pad> fiber and which <pad> are <pad> <pad> <pad> <pad> <pad> </s> </s> \n",
            "#############################\n",
            "#############################\n",
            "Actual Sentence is:\n",
            "rch kits are being provided directly to the districts by the government of india </s> <pad> <pad> <pad> <pad> <pad> \n",
            "Predicted Sentence is:\n",
            "rch kits are being provided directly to the districts by the government of india </s> </s> </s> <pad> </s> </s> \n",
            "#############################\n",
            "#############################\n",
            "Actual Sentence is:\n",
            "such woman should use i very carefully who has the disease of a </s> <pad> <pad> <pad> <pad> <pad> <pad> \n",
            "Predicted Sentence is:\n",
            "such <pad> <pad> <pad> <pad> <pad> carefully <pad> <pad> the <pad> of <pad> </s> </s> </s> </s> </s> </s> </s> \n",
            "#############################\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "XCEpNyhxI8S-",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "[Click here to use google transliterate and copy the hindi sentence from there and paste in below cell when it asks for input.](https://www.google.co.in/inputtools/try/)"
      ]
    },
    {
      "metadata": {
        "id": "RstFo7hqHTxb",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "ac8564a1-338b-468f-d6fa-9b47bbff9060"
      },
      "cell_type": "code",
      "source": [
        "#Enter source language sentence from google transliterate https://www.google.co.in/inputtools/try/\n",
        "print(\"Enter Sentences less than 15 words. As of now that is what is set.\")\n",
        "user_sentence=input()\n",
        "user_sentence=user_sentence+eos\n",
        "print(type(user_sentence))"
      ],
      "execution_count": 85,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Enter Sentences less than 15 words. As of now that is what is set.\n",
            "आपका दांत सुन्दर है \n",
            "<class 'str'>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "6atLy0LnJitT",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "901e92c0-84c1-449d-b19e-403b3ede9f1e"
      },
      "cell_type": "code",
      "source": [
        "#Get Encoding\n",
        "words=user_sentence.split()\n",
        "user_encoding=[]\n",
        "for word in words:\n",
        "  try:\n",
        "    #print(hindi_dictionary[word])\n",
        "    user_encoding.append(hindi_dictionary[word])\n",
        "  except KeyError:\n",
        "    #print(hindi_dictionary['<unk>'])\n",
        "    user_encoding.append(hindi_dictionary['<unk>'])\n",
        "user_encoding\n",
        "#print(X.shape[1],len(user_encoding))\n",
        "if (X.shape[1]>len(user_encoding)):\n",
        "  padding_count=X.shape[1]-len(user_encoding)\n",
        "  for x in range(0,padding_count):\n",
        "    user_encoding.append(0)\n",
        "else:\n",
        "  user_encoding=user_encoding[0:X.shape[1]]\n",
        "user_encoding=np.array(user_encoding)\n",
        "user_encoding"
      ],
      "execution_count": 86,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ 555, 8649, 7981, 8512, 8650,    0,    0,    0,    0,    0,    0,\n",
              "          0,    0,    0,    0,    0,    0,    0,    0,    0])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 86
        }
      ]
    },
    {
      "metadata": {
        "id": "-OlZNNWupj48",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "9c4f56dc-4b25-40c9-9232-7c084076d494"
      },
      "cell_type": "code",
      "source": [
        "def return_predicted_array_user(user_encoding,model2):\n",
        "  test=np.reshape(user_encoding,(1,X.shape[1]))\n",
        "  encoding_prediction=np.round(model2.predict(test))\n",
        "  predicted_argmax=np.argmax(encoding_prediction,axis=2)\n",
        "  predicted_argmax=np.reshape(predicted_argmax,(predicted_argmax.shape[1],))\n",
        "  return predicted_argmax\n",
        "\n",
        "#from keras.models import load_model\n",
        "#bestmodel = load_model('complete_model_with_weigths.h5')\n",
        "\n",
        "predicted_user=return_predicted_array_user(user_encoding,model2)\n",
        "print(\"Predicted Array\",predicted_user)\n",
        "\n",
        "def return_sentence_list(encoding,revere_dictionary_english):\n",
        "  sentence=list()\n",
        "  for ind in encoding:\n",
        "    #print(ind)\n",
        "    predicted_word=revere_dictionary_english[ind]\n",
        "    #print(predicted_word)\n",
        "    sentence.append(revere_dictionary_english[ind])\n",
        "  return sentence\n",
        "  \n",
        "user_translation_list=return_sentence_list(predicted_user,revere_dictionary_english)\n",
        "\n",
        "def concatenate_list_data(list):\n",
        "  result= ''\n",
        "  for element in list:\n",
        "    result += str(element)\n",
        "    result += str(\" \")\n",
        "  return result\n",
        "\n",
        "predicted_sentence=concatenate_list_data(user_translation_list)\n",
        "print(\"Predicted Sentence\")\n",
        "print(\"###################################\")\n",
        "print(predicted_sentence)"
      ],
      "execution_count": 87,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Predicted Array [3091    0    0 4135    0 6767 6767 6767    0    0    0    0    0 6767\n",
            " 6767 6767 6767 6767 6767 6767]\n",
            "Predicted Sentence\n",
            "###################################\n",
            "its <pad> <pad> of <pad> </s> </s> </s> <pad> <pad> <pad> <pad> <pad> </s> </s> </s> </s> </s> </s> </s> \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "3ni-lR-FppnI",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "We can now see the results on new examples."
      ]
    },
    {
      "metadata": {
        "id": "S8ZzMgVhppnQ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## 5 References"
      ]
    },
    {
      "metadata": {
        "id": "SQmFPC6RppnR",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Neural Machine Translation by Jointly Learning to Align and Translate: Dzmitry Bahdanau, Kyunghyun Cho, Yoshua Bengio https://arxiv.org/pdf/1409.0473.pdf\n",
        "\n",
        "https://machinelearningmastery.com\n",
        "\n",
        "https://www.coursera.org/\n",
        "\n",
        "https://www.udemy.com/"
      ]
    },
    {
      "metadata": {
        "id": "Pkhih1osppnT",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Appendix"
      ]
    },
    {
      "metadata": {
        "id": "l_RxBwoYppnT",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "One thing we can do to improve the model is instead of one hot encodings of words of length vocabulary, get the word2vec vectors for each word with fixed length.\n",
        "\n",
        "Another thing that can be done is train only short sentences.\n",
        "\n",
        "In below section we will provide the functions to help to do the tasks."
      ]
    },
    {
      "metadata": {
        "id": "oPjSbyZFppnU",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#Converting input to word2vec.\n",
        "def sentences_to_word2vec_input_format(language_sentences_list):\n",
        "    word2vec_sentence_feed=list()\n",
        "    for sentence in language_sentences_list:\n",
        "        word2vec_sentence_feed.append(sentence.split())\n",
        "    return(word2vec_sentence_feed)\n",
        "english_sentences_w2v_format=sentences_to_word2vec_input_format(english_sentences_list)\n",
        "hindi_sentences_w2v_format=sentences_to_word2vec_input_format(hindi_sentences_list)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "55q44941ppnk",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from gensim.models import Word2Vec\n",
        "# train model\n",
        "english_model = Word2Vec(english_sentences_w2v_format, min_count=1)\n",
        "english_words_vocab = list(english_model.wv.vocab)\n",
        "hindi_model = Word2Vec(hindi_sentences_w2v_format, min_count=1)\n",
        "english_words_vocab = list(hindi_model.wv.vocab)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "k7PhtEkvppnn",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def sentences_to_w2vec(language_encoding,revere_dictionary_language,language_model):\n",
        "    import numpy as np\n",
        "    sentence_level_w2vec_list=[]\n",
        "    #arr = np.empty((2,), float)\n",
        "    number_of_sentences=language_encoding.shape[0]\n",
        "    for i in range(0,number_of_sentences):\n",
        "        language_list_padded=[]\n",
        "        #print (english_encoding[i])\n",
        "        for key in language_encoding[i]:\n",
        "            #print(revere_dictionary_english[key])\n",
        "            word=(revere_dictionary_language[key])\n",
        "            try:\n",
        "                #print(\"Found word Shape of word vector\",(english_model[word]).shape,arr.shape)\n",
        "                language_list_padded.append(language_model[word])\n",
        "            except KeyError:\n",
        "                unk='<unk>'\n",
        "                #print(\"not found! Assigning Unknown Vector\",  (english_model[unk]).shape)\n",
        "                language_list_padded.append(language_model[unk])\n",
        "        #print(np.array(language_list_padded))\n",
        "        sentence_level_w2vec_list.append((np.array(language_list_padded)))\n",
        "    sentence_level_w2vec=np.array(sentence_level_w2vec_list)\n",
        "    return(sentence_level_w2vec)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "bbsNCN0Oppnq",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "X=hindi_encoding\n",
        "Y=english_encoding\n",
        "#Y will remain the same.\n",
        "Yoh=np.array(list(map(lambda x: to_categorical(x, num_classes=len(english_dictionary)), Y)))\n",
        "print(\"X.shape:\", X.shape)\n",
        "print(\"Y.shape:\", Y.shape)\n",
        "print(\"Yoh.shape:\", Yoh.shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Lx__HSMlppnu",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#Run this if you want word2vec instead of One hot encoding\n",
        "#Naming it still as X0h and Yoh to avoid changes in too many places further.\n",
        "#Yoh \n",
        "Xoh=sentences_to_w2vec(hindi_encoding,revere_dictionary_hindi,hindi_model)\n",
        "print(\"Xoh.shape:\", Xoh.shape)\n",
        "print(\"Yoh.shape:\", Yoh.shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "a5xqchLzppn4",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "One might also like to get the sentences of only specific length from source as well as target, for example get all sentences which has maximum 5 words and in hindi maximum 8 words. Use below function and feed the length you need."
      ]
    },
    {
      "metadata": {
        "id": "ZIIhBSOwppn5",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#dataset=Ndarray with following dimentions (sentence_length, 2)\n",
        "#source_len is the length of language in dataset[0][1]\n",
        "#target_len is the length of language in dataset[0][0]\n",
        "def get_sentences_subset(dataset,source_len,target_len):\n",
        "    limited=dataset\n",
        "    indexes_list=[]\n",
        "    for indexes in range(0,limited.shape[0]):\n",
        "        #print(len(limited[i][0].split()),len(limited[i][1].split()))\n",
        "        eng_len=len(limited[indexes][0].split()) \n",
        "        hin_len=len(limited[indexes][1].split())\n",
        "        state1=(eng_len<=target_len)\n",
        "        state2=(hin_len<=source_len)\n",
        "        final=state2&state1\n",
        "        #print(eng_len,hin_len,final)\n",
        "        #print(state1,state2,final)\n",
        "        if (final):\n",
        "            indexes_list.append(indexes)\n",
        "    #print(indexes_list,type(indexes_list))\n",
        "    return(limited[indexes_list])"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}